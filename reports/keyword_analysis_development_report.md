# TSI-SOTA-AI Keyword Analysis Module Development Report

**Project:** Systematic Literature Review - Agentic AI in Supply Chain Management  
**Date:** June 5, 2025  
**Status:** BLOCKED - Critical Data Acquisition Issues  
**Report Version:** 1.1 (Updated with Latest Findings)

---

## Executive Summary

This report documents the current progress on developing a comprehensive keyword analysis module for the TSI-SOTA-AI project. **Critical data acquisition issues have been identified and confirmed that require immediate resolution before proceeding with keyword analysis implementation.**

**CRITICAL UPDATE (v1.1)**: Latest testing confirms the 99.5% duplication rate issue persists with only 1 unique publication (paper ID: `806208c8d27347eab578ecb2faff64012a7d67dc`) retrieved from 200 total API calls across 6 different query strategies.

## Project Objectives

### Primary Goals
- Develop comprehensive keyword analysis capabilities for academic publications
- Implement multiple extraction methods (API-based, NLP-based with TF-IDF/RAKE/YAKE)
- Create semantic analysis using BGE-M3 embeddings and clustering
- Build temporal trend analysis and keyword lifecycle tracking
- Generate interactive visualizations and dashboards

### Target Dataset
- **Focus**: Publications on "Agentic AI in Supply Chain Management"
- **Expected Volume**: ~165 publications for 2025 (based on Semantic Scholar UI)
- **Data Source**: Semantic Scholar API (primary focus per requirements)
- **Quality Requirements**: High-quality, deduplicated academic publications

---

## Current Status: üî¥ BLOCKED - Data Acquisition Issues

### ‚úÖ Completed Components

#### 1. Infrastructure Setup
- **Notebook Framework**: Comprehensive Jupyter notebook created (`agent_research_dynamics_analysis.ipynb`)
- **Configuration Management**: Integrated with existing config system (`slr_config.yaml`)
- **Development Workflow**: DataFrame-based development mode implemented (no CSV saving during development)
- **Directory Structure**: Proper organization with `/data/slr_raw/` for raw storage

#### 2. Data Acquisition Framework
- **API Integration**: Successfully connected to Semantic Scholar API client
- **Rate Limiting**: Implemented 5-second delays between API calls to respect rate limits
- **Error Handling**: Comprehensive error handling for API failures and rate limiting
- **Approach Tracking**: Comprehensive metadata tracking system implemented:
  - `approach_id`: Unique identifier for each fetch method
  - `query_used`: Exact query string used
  - `fetch_method`: Technical method employed
  - `fetch_timestamp`: Retrieval timestamp

#### 3. Query Strategy Implementation
- **Multi-Query Approach**: Implemented 6 different query variations:
  1. `method_1_direct_client`: "agent AND (scm OR "supply chain management" OR logistics)"
  2. `method_3a_simple`: "agent scm"
  3. `method_3b_quoted`: "supply chain" agent
  4. `method_3c_autonomous`: "autonomous agent logistics"
  5. `method_3d_agent_based`: "agent-based supply chain"
  6. `method_3e_multi_agent`: "multi-agent supply chain"

#### 4. Data Quality Analysis
- **Deduplication System**: Implemented comprehensive duplicate detection using paper ID
- **Quality Metrics**: Built-in analysis of data quality and coverage
- **Debug Capabilities**: Extensive logging and diagnostic information
- **Metadata Preservation**: Full tracking of data sources and acquisition methods

### ‚ùå Critical Issues Identified

#### 1. Severe Data Acquisition Failure
**Problem**: Massive discrepancy between expected and actual unique results

**Key Metrics**:
- **Expected**: ~165 publications (Semantic Scholar UI shows this count for 2025)
- **Actual**: **1 unique publication** after deduplication
- **Raw Fetch**: 200 records collected across 6 query methods
- **Duplication Rate**: **99.5%** (199 out of 200 records were duplicates)

#### 2. API Response Anomalies
**Symptoms**:
- Same publication (`paperId: 0397c2643eb6bb0e52df371b6f772ed21e3aeb80`) returned for multiple different queries
- High rate limiting (429 errors) despite 5-second delays between requests
- Inconsistent response patterns across semantically different query variations
- Potential API endpoint or parameter configuration issues

**Evidence**:
```
üîç Latest Test Results (June 5, 2025):
   Total API calls across 6 query methods: 200 records
   Unique publication after deduplication: 1 record
   Specific paper retrieved: "Enhancing supply chain resilience with multi-agent systems..."
   Paper ID: 806208c8d27347eab578ecb2faff64012a7d67dc
   Author: Md Zahidur Rahman Farazi
   Year: 2025, Citations: 0

üîç Query Distribution (ALL returning same paper):
   'agent scm': 50 publications
   'autonomous agent logistics': 50 publications  
   'agent-based supply chain': 50 publications
   'multi-agent supply chain': 50 publications

üîÑ Confirmed Duplication Rate: 99.5% (199/200 duplicates)
```

#### 3. Potential Root Causes Analysis

**Primary Hypotheses**:

1. **Year Filtering Issues**
   - 2025 filter may be too restrictive
   - API year parameter might not be working as expected
   - Semantic Scholar may have limited 2025 content

2. **API Query Parameter Problems**
   - Query syntax may not match Semantic Scholar's expected format
   - Field parameters might be incorrectly configured
   - Search vs. bulk endpoint mismatch

3. **Rate Limiting Side Effects**
   - 429 errors may be causing result set contamination
   - API may be returning cached/default results under rate limiting
   - Server-side caching returning same results for different queries

4. **Search Algorithm Issues**
   - API search algorithm may be defaulting to most relevant result
   - Query similarity leading to identical result sets
   - Insufficient result diversity in API responses

5. **Configuration Mismatch**
   - Our API configuration may differ from Semantic Scholar UI settings
   - Missing required parameters for diverse result retrieval
   - Endpoint version or method selection issues

---

## Work Status Breakdown

### üîÑ Work in Progress

#### 1. Data Acquisition (BLOCKED)
- **Status**: Critical blocking issue preventing progress
- **Issue**: Multiple query strategies yielding insufficient unique results
- **Mitigation**: Rate limiting protection in place but still experiencing issues
- **Next Steps**: Systematic debugging of API configuration and query parameters

#### 2. Keyword Analysis Framework (READY)
- **Status**: Infrastructure prepared and ready for implementation
- **Components Ready**:
  - API-based keyword extraction framework
  - NLP-based methods (TF-IDF, RAKE, YAKE) implementation structure
  - Frequency analysis and distribution statistics preparation

#### 3. Semantic Analysis Framework (READY)
- **Status**: Architecture designed and ready for data
- **Components Ready**:
  - BGE-M3 embedding generation pipeline
  - Clustering algorithms (K-means, DBSCAN, hierarchical)
  - Dimensionality reduction (UMAP, t-SNE) configuration

### ‚è≥ Pending Implementation (Blocked by Data Issues)

#### 1. Immediate Priority: Debug Data Acquisition
**Critical Actions Required**:

1. **API Endpoint Investigation**
   - Test different Semantic Scholar API endpoints
   - Verify search vs. bulk download methods
   - Compare API documentation with actual behavior

2. **Query Optimization**
   - Experiment with different query syntax variations
   - Test query complexity levels (simple vs. complex boolean queries)
   - Validate field-specific search parameters

3. **Year Range Testing**
   - Try broader date ranges (2024-2025, 2023-2025)
   - Test without year filtering
   - Compare results with Semantic Scholar UI manually

4. **Rate Limiting Strategy Enhancement**
   - Implement exponential backoff algorithms
   - Consider Semantic Scholar API key acquisition
   - Test different delay intervals (10s, 15s, 30s)

5. **Alternative Data Validation**
   - Cross-reference with other academic databases (OpenAlex, ArXiv)
   - Validate Semantic Scholar UI results manually
   - Compare query performance across different platforms

#### 2. Core Analysis Components (Ready for Implementation)

**Keyword Extraction Suite**:
- Multi-method keyword extraction implementation
- Frequency analysis and statistical profiling
- Keyword quality scoring and filtering

**Semantic Analysis Pipeline**:
- BGE-M3 embedding generation for semantic similarity
- Multi-level clustering (keyword, document, temporal)
- Semantic drift detection and evolution tracking

**Temporal Analysis Framework**:
- Keyword trend detection and lifecycle analysis
- Emergence and decline pattern identification
- Cross-temporal semantic relationship mapping

**Visualization and Dashboard Suite**:
- Static visualization generation (matplotlib, seaborn)
- Interactive dashboard creation (Plotly, Dash)
- Network visualization for keyword relationships

**Export and Integration System**:
- Multi-format export (JSON, CSV, HTML)
- Dashboard publishing and sharing
- Integration with existing TSI-SOTA-AI workflow

#### 3. Quality Assurance and Testing
- **Performance Testing**: Large-scale dataset processing validation
- **Accuracy Testing**: Keyword extraction quality assessment
- **Integration Testing**: End-to-end workflow validation
- **Documentation**: Comprehensive user guides and API documentation

---

## Technical Architecture

### Current Implementation Stack
- **Data Source**: Semantic Scholar API (primary)
- **Processing**: Python with pandas, numpy
- **NLP**: scikit-learn, RAKE-NLTK, YAKE
- **Embeddings**: BGE-M3 model via transformers
- **Clustering**: scikit-learn algorithms
- **Visualization**: Plotly, matplotlib, seaborn
- **Storage**: JSON, CSV, in-memory DataFrames

### Technical Debt and Considerations

#### 1. Rate Limiting Strategy
- **Current**: Fixed 5-second delays
- **Issues**: Insufficient for consistent access
- **Recommendations**: 
  - Implement exponential backoff
  - Acquire Semantic Scholar API key
  - Consider request queuing system

#### 2. Data Validation and Quality
- **Current**: Basic duplicate detection
- **Needed**: 
  - Robust API response validation
  - Data quality scoring system
  - Fallback strategies for failed requests

#### 3. Scalability Concerns
- **Current**: Single-threaded, synchronous processing
- **Issues**: May not scale for larger datasets
- **Recommendations**:
  - Implement batch processing
  - Add memory optimization for large-scale analysis
  - Consider async processing for API calls

#### 4. Error Handling and Resilience
- **Current**: Basic try-catch blocks
- **Needed**:
  - Comprehensive error taxonomy
  - Automatic retry mechanisms
  - Graceful degradation strategies

---

## Risk Assessment

### üî¥ High Risk
- **Data Acquisition Failure**: Current blocking issue preventing all downstream progress
- **API Reliability**: Inconsistent Semantic Scholar API behavior affecting project timeline
- **Data Quality**: Single unique record insufficient for meaningful analysis

### üü° Medium Risk  
- **Rate Limiting**: May require significant delays or API key acquisition
- **Scalability**: Current approach may not handle production-scale datasets efficiently
- **Integration Complexity**: Multiple analysis components need careful coordination

### üü¢ Low Risk
- **Analysis Implementation**: Framework is well-designed and ready for rapid implementation
- **Visualization**: Well-established libraries and proven patterns available
- **Export Functionality**: Straightforward implementation once data issues resolved

---

## Success Metrics and Milestones

### Phase 1: Data Acquisition Resolution (Critical)
- **Target**: Achieve consistent retrieval of 100+ unique publications
- **Timeline**: 2-3 days
- **Success Criteria**: 
  - Duplication rate < 20%
  - Consistent API response behavior
  - Reproducible query results

### Phase 2: Core Analysis Implementation (Dependent on Phase 1)
- **Target**: Complete keyword extraction and semantic analysis
- **Timeline**: 1 week after Phase 1
- **Success Criteria**:
  - Multi-method keyword extraction working
  - Semantic clustering producing meaningful results
  - Basic visualizations generated

### Phase 3: Advanced Features and Integration
- **Target**: Full feature implementation and testing
- **Timeline**: 2 weeks after Phase 2
- **Success Criteria**:
  - Interactive dashboard functional
  - Export capabilities working
  - Integration with existing TSI-SOTA-AI workflow

---

## Recommendations and Next Steps

### Immediate Actions (Next 24-48 Hours)
1. **Systematic API Debugging**
   - Create minimal test scripts for different API configurations
   - Document exact API responses for manual analysis
   - Compare API behavior with Semantic Scholar UI manually

2. **Alternative Query Strategies**
   - Test completely different query formulations
   - Experiment with different time ranges and filters
   - Try API endpoints beyond basic search

3. **Data Validation Protocol**
   - Implement comprehensive API response logging
   - Create manual validation dataset from UI
   - Establish baseline for expected vs. actual results

### Short-term Goals (Next Week)
1. **Resolve Data Acquisition**: Achieve stable, diverse data retrieval
2. **Rapid Analysis Implementation**: Deploy prepared analysis frameworks
3. **Initial Results Generation**: Create first working analysis pipeline
4. **Documentation Update**: Document successful strategies and configurations

### Medium-term Objectives (Next Month)
1. **Full Feature Deployment**: Complete all planned analysis capabilities
2. **Performance Optimization**: Optimize for production-scale usage
3. **Integration Testing**: Comprehensive testing with real datasets
4. **User Documentation**: Complete user guides and tutorials

---

## Resource Requirements

### Technical Resources
- **API Access**: Semantic Scholar API key (recommended)
- **Compute**: Current resources sufficient for development
- **Storage**: Minimal additional storage needed

### Time Investment
- **Critical Path**: Data acquisition debugging (2-3 days)
- **Implementation**: Analysis framework deployment (1 week)
- **Testing and Polish**: Quality assurance and documentation (1 week)

### External Dependencies
- **Semantic Scholar API**: Critical dependency requiring resolution
- **NLP Libraries**: Already available and tested
- **Visualization Tools**: Standard libraries, no issues expected

---

## Conclusion

The keyword analysis module has established a solid foundation with comprehensive infrastructure and well-designed analysis frameworks. The architecture is sound and ready for rapid implementation once data acquisition issues are resolved.

**Critical Blocker**: The current data acquisition problem appears to be related to Semantic Scholar API configuration or behavior rather than fundamental implementation issues. The 99.5% duplication rate (200 records ‚Üí 1 unique) indicates a systematic problem that must be debugged before proceeding.

**Strategic Assessment**: Despite the current blocking issue, the project is well-positioned for rapid progress once data acquisition is resolved. The modular architecture and prepared frameworks will enable quick implementation of all planned features.

**Immediate Priority**: Focus exclusively on debugging and resolving data acquisition issues. All other development should be paused until consistent, diverse data retrieval is achieved.

---

## Appendix

### File Structure
```
/workspaces/tsi-sota-ai/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ agent_research_dynamics_analysis.ipynb  # Main development notebook
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ agent_scm_publications_combined.csv      # Current output (1 record)
‚îÇ   ‚îî‚îÄ‚îÄ slr_raw/                                 # Raw data storage
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ slr_config.yaml                          # Configuration management
‚îî‚îÄ‚îÄ reports/
    ‚îî‚îÄ‚îÄ keyword_analysis_development_report.md   # This report
```

### Key Dependencies
```
- semanticscholar>=0.3.0
- pandas>=1.5.0
- numpy>=1.21.0
- scikit-learn>=1.0.0
- plotly>=5.0.0
- transformers>=4.20.0
- torch>=1.12.0
```

---

**Report Prepared By**: Development Team  
**Review Status**: Pending Technical Review  
**Next Update**: Upon resolution of data acquisition issues  
**Distribution**: Project stakeholders, technical team

---

*This report will be updated as progress is made on resolving the identified issues and advancing the implementation.*
