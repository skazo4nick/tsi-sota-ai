{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup and Configuration ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Optional\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: article_downloader.ipynb (first cell)\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"app\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and config from utils.py\n",
    "from utils import config, create_data_directories, logger\n",
    "from pdf_downloader import download_pdf\n",
    "from html_parser import parse_article_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directories if they don't exist\n",
    "create_data_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --- 2. Load Input Data ---\n",
    "references_dir = \"/Users/max/Documents/Code/tsi-sota-ai/data/references\"\n",
    "csv_pattern = os.path.join(references_dir, \"*.csv\")\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "if not csv_files:\n",
    "    logger.error(f\"No CSV files found in the directory: {references_dir}\")\n",
    "    raise FileNotFoundError(f\"No CSV files found in: {references_dir}\")\n",
    "\n",
    "data_frames = []\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file).dropna(subset=['doi'])\n",
    "        logger.info(f\"Successfully loaded {csv_file}. Shape: {df.shape}\")\n",
    "        data_frames.append(df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV file {csv_file}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Combine all dataframes into one\n",
    "articles_df = pd.concat(data_frames, ignore_index=True)\n",
    "logger.info(f\"Combined articles data shape: {articles_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>short_journal</th>\n",
       "      <th>volume</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>issue</th>\n",
       "      <th>page</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-25</td>\n",
       "      <td>Predictive big data analytics for supply chain...</td>\n",
       "      <td>10.1186/s40537-020-00329-2</td>\n",
       "      <td>[{'author_name': 'Seyedeh Mahya Seyedan', 'aut...</td>\n",
       "      <td>Journal of Big Data</td>\n",
       "      <td>J Big Data</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>Springer Science and Business Media LLC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Big data analytics (BDA) in supply chain manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>Overcoming Barriers in Supply Chain Analytics—...</td>\n",
       "      <td>10.3390/logistics4010005</td>\n",
       "      <td>[{'author_name': 'Tino T. Herden', 'author_slu...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>While supply chain analytics shows promise reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>Decision support system for handling control d...</td>\n",
       "      <td>10.1186/s40537-022-00653-9</td>\n",
       "      <td>[{'author_name': 'Dimah Alahmadi', 'author_slu...</td>\n",
       "      <td>Journal of Big Data</td>\n",
       "      <td>J Big Data</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>Springer Science and Business Media LLC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abstract\\n                Background\\n        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-08</td>\n",
       "      <td>A Systematic Investigation of the Integration ...</td>\n",
       "      <td>10.3390/logistics5030062</td>\n",
       "      <td>[{'author_name': 'Meike Schroeder', 'author_sl...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>62</td>\n",
       "      <td>The main objective of the paper is to analyze ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>Disorders, Vulnerabilities and Resilience in t...</td>\n",
       "      <td>10.3390/logistics5030048</td>\n",
       "      <td>[{'author_name': 'Catarina Ferreira', 'author_...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48</td>\n",
       "      <td>The economic and social environment caused by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2020-07-25  Predictive big data analytics for supply chain...   \n",
       "1  2020-02-26  Overcoming Barriers in Supply Chain Analytics—...   \n",
       "2  2022-11-30  Decision support system for handling control d...   \n",
       "3  2021-09-08  A Systematic Investigation of the Integration ...   \n",
       "4  2021-07-05  Disorders, Vulnerabilities and Resilience in t...   \n",
       "\n",
       "                          doi  \\\n",
       "0  10.1186/s40537-020-00329-2   \n",
       "1    10.3390/logistics4010005   \n",
       "2  10.1186/s40537-022-00653-9   \n",
       "3    10.3390/logistics5030062   \n",
       "4    10.3390/logistics5030048   \n",
       "\n",
       "                                             authors              journal  \\\n",
       "0  [{'author_name': 'Seyedeh Mahya Seyedan', 'aut...  Journal of Big Data   \n",
       "1  [{'author_name': 'Tino T. Herden', 'author_slu...            Logistics   \n",
       "2  [{'author_name': 'Dimah Alahmadi', 'author_slu...  Journal of Big Data   \n",
       "3  [{'author_name': 'Meike Schroeder', 'author_sl...            Logistics   \n",
       "4  [{'author_name': 'Catarina Ferreira', 'author_...            Logistics   \n",
       "\n",
       "  short_journal  volume  year                                publisher  issue  \\\n",
       "0    J Big Data     7.0  2020  Springer Science and Business Media LLC    1.0   \n",
       "1     Logistics     4.0  2020                                  MDPI AG    1.0   \n",
       "2    J Big Data     9.0  2022  Springer Science and Business Media LLC    1.0   \n",
       "3     Logistics     5.0  2021                                  MDPI AG    3.0   \n",
       "4     Logistics     5.0  2021                                  MDPI AG    3.0   \n",
       "\n",
       "  page                                           abstract  \n",
       "0  NaN  Big data analytics (BDA) in supply chain manag...  \n",
       "1    5  While supply chain analytics shows promise reg...  \n",
       "2  NaN  Abstract\\n                Background\\n        ...  \n",
       "3   62  The main objective of the paper is to analyze ...  \n",
       "4   48  The economic and social environment caused by ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import numpy as np  # Import numpy to handle embeddings as arrays\n",
    "\n",
    "def generate_embeddings(text: str) -> Optional[list]:\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using the Gemini API (text-embedding-004 model).\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to encode.\n",
    "\n",
    "    Returns:\n",
    "        Optional[list]: A list representing the embedding vector, or None if there was an error.\n",
    "    \"\"\"\n",
    "    if not config.google_api_key:\n",
    "        logger.warning(\"GOOGLE_API_KEY is not set in .env or config.yaml. Gemini embeddings will not be generated.\")\n",
    "        return None\n",
    "\n",
    "    genai.configure(api_key=config.google_api_key) # Configure Gemini API with key from config\n",
    "    model = genai.GenerativeModel(\"models/text-embedding-004\") # Specify the embedding model\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Generating Gemini embedding for text (first 50 chars): %s...\", text[:50])\n",
    "        response = model.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=text\n",
    "        )\n",
    "        embedding_vector = response['embedding'] # Extract embedding vector from response\n",
    "        logger.info(\"Gemini embedding generated successfully.\")\n",
    "        return embedding_vector # Return the embedding as a list\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Gemini embedding: {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Article Processing and Dataframe Population ---\n",
    "def fetch_article_content(doi: str, title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches article content (Markdown and PDF) using DOI and title.\n",
    "    \"\"\"\n",
    "    article_data = {\n",
    "        'doi': doi,\n",
    "        'title': title,\n",
    "        'full_text_markdown': None,\n",
    "        'pdf_filepath': None,\n",
    "        'retrieval_method': None,\n",
    "        'download_success': False,\n",
    "        'cluster_id': None # Will be added later if clustering is enabled\n",
    "    }\n",
    "    article_url = f\"https://doi.org/{doi}\"\n",
    "\n",
    "    # Use parse_article_html and pass the configuration option to choose between BeautifulSoup and Jina Reader API\n",
    "    parsed_content = parse_article_html(article_url, use_jina_reader_api_config=config.use_jina_reader_api_config)\n",
    "    if parsed_content:\n",
    "        article_data['full_text_markdown'] = parsed_content['content']\n",
    "        # Update retrieval_method to reflect which parser was actually used\n",
    "        article_data['retrieval_method'] = parsed_content['metadata'].get('parser', 'BeautifulSoup+html2text') # Get parser info from metadata\n",
    "        logger.info(f\"Successfully parsed article HTML for DOI: {doi} using {article_data['retrieval_method']}\") # Log actual parser used\n",
    "    else:\n",
    "        article_data['retrieval_method'] = 'HTML Parsing Failed'\n",
    "        logger.warning(f\"HTML parsing failed for DOI: {doi}\")\n",
    "\n",
    "    if config.use_clustering_in_pipeline: # Only generate embeddings if clustering is enabled in pipeline\n",
    "        embeddings = generate_embeddings(article_data['full_text_markdown'] or '') # Generate embeddings for Markdown content or empty string\n",
    "        article_data['abstract_embedding'] = embeddings # Add embedding to article data\n",
    "\n",
    "    if config.use_clustering_in_pipeline: # Only download PDFs if clustering is enabled in pipeline\n",
    "        cluster_id = article_data.get('cluster_id', 0)  # Default cluster ID if not assigned yet\n",
    "        pdf_path = download_pdf(doi, cluster_id) # Download PDF and get path\n",
    "        if pdf_path:\n",
    "            article_data['pdf_filepath'] = pdf_path\n",
    "            article_data['download_success'] = True\n",
    "            logger.info(f\"PDF download successful for DOI: {doi}, saved to: {pdf_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"PDF download failed for DOI: {doi}\")\n",
    "    else: # If no clustering, still try to download PDF to a default cluster (cluster 0) - or you can skip PDF download if no clustering\n",
    "        cluster_id = 0 # Default cluster ID for non-clustered PDFs\n",
    "        pdf_path = download_pdf(doi, cluster_id) # Download PDF to default cluster\n",
    "        if pdf_path:\n",
    "            article_data['pdf_filepath'] = pdf_path\n",
    "            article_data['download_success'] = True\n",
    "            logger.info(f\"PDF download successful for DOI: {doi}, saved to default cluster: {pdf_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"PDF download failed for DOI: {doi} (default cluster)\")\n",
    "\n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_data = []  # List to store processed article data\n",
    "for index, row in tqdm(articles_df.iterrows(), total=len(articles_df), desc=\"Processing articles\"):\n",
    "    doi = row['doi']\n",
    "    title = row['title']\n",
    "    try:\n",
    "        processed_article_data = fetch_article_content(doi, title)\n",
    "        output_data.append({**row.to_dict(), **processed_article_data})  # Merge original row data with processed data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing article with DOI: {doi}. Error: {e}\", exc_info=True)\n",
    "        output_data.append({**row.to_dict(), 'error': str(e)})  # Append error info\n",
    "    # Pause 1 second between requests to respect rate limiting\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_articles_df = pd.DataFrame(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Clustering (Conditional - if enabled in config) ---\n",
    "if config.use_clustering_in_pipeline:\n",
    "    logger.info(\"Clustering pipeline enabled. Starting clustering...\")\n",
    "    # Placeholder for embedding retrieval from DataFrame (if you implemented embedding generation)\n",
    "    embeddings_array = np.array([row['abstract_embedding'] for index, row in processed_articles_df.iterrows() if row['abstract_embedding'] is not None]) # Example - adjust based on your embedding column name\n",
    "    if embeddings_array.size > 0: # Proceed only if embeddings were generated\n",
    "        logger.info(f\"Embeddings array shape for clustering: {embeddings_array.shape}\")\n",
    "        n_clusters_optimal = config.n_clusters # Use configured number of clusters\n",
    "        kmeans = KMeans(n_clusters=n_clusters_optimal, random_state=42, n_init=10) # Explicitly set n_init\n",
    "        clusters = kmeans.fit_predict(embeddings_array)\n",
    "        processed_articles_df['cluster_id'] = -1 # Default to -1 (unassigned)\n",
    "        valid_embedding_indices = [index for index, row in processed_articles_df.iterrows() if row['abstract_embedding'] is not None] # Get indices of rows with embeddings\n",
    "        for i, index in enumerate(valid_embedding_indices):\n",
    "            processed_articles_df.at[index, 'cluster_id'] = clusters[i] # Assign cluster IDs based on original indices\n",
    "        logger.info(f\"K-Means clustering completed with {n_clusters_optimal} clusters.\")\n",
    "    else:\n",
    "        logger.warning(\"No embeddings found for clustering. Skipping clustering step.\")\n",
    "else:\n",
    "    logger.info(\"Clustering pipeline disabled in config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Organize Clusters (PDFs) and Save Results ---\n",
    "if config.use_clustering_in_pipeline:\n",
    "    cluster_counts = processed_articles_df['cluster_id'].value_counts().sort_index()\n",
    "    print(\"Cluster Distribution:\")\n",
    "    print(cluster_counts)\n",
    "\n",
    "    for cluster_id in processed_articles_df['cluster_id'].unique():\n",
    "        if cluster_id != -1: # Skip unassigned cluster (-1)\n",
    "            cluster_dir = config.get_cluster_dir(cluster_id)\n",
    "            os.makedirs(cluster_dir, exist_ok=True) # Ensure cluster directories exist\n",
    "            cluster_df = processed_articles_df[processed_articles_df['cluster_id'] == cluster_id]\n",
    "            logger.info(f\"Cluster {cluster_id}: {len(cluster_df)} articles. Sample titles: {cluster_df['title'].head(3).tolist()}\")\n",
    "else:\n",
    "    logger.info(\"Cluster organization (PDFs) skipped as clustering is disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = 'processed_articles_fulltext.json' # Choose output filename\n",
    "processed_articles_df.to_json(output_json_filepath, orient='records', lines=True)\n",
    "logger.info(f\"Processed data saved to: {output_json_filepath}\")\n",
    "print(f\"Processed data saved to: {output_json_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Basic Analysis and Summary (Optional) ---\n",
    "print(\"\\nRetrieval Method Distribution:\")\n",
    "print(processed_articles_df['retrieval_method'].value_counts())\n",
    "print(\"\\nDownload Success Rate:\")\n",
    "print(processed_articles_df['download_success'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Visualization (Optional - Clustering Results if enabled) ---\n",
    "if config.use_clustering_in_pipeline and embeddings_array.size > 0:\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE # Import here, only if needed\n",
    "        tsne = TSNE(n_components=2, random_state=42, n_iter=300, perplexity=30) # Example TSNE parameters\n",
    "        tsne_results = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1], hue=processed_articles_df.loc[valid_embedding_indices, 'cluster_id'], palette='viridis', legend='full') # Use valid indices for hue\n",
    "        plt.title('Article Clusters Visualized with t-SNE')\n",
    "        plt.xlabel('TSNE Dimension 1')\n",
    "        plt.ylabel('TSNE Dimension 2')\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        logger.warning(\"t-SNE visualization requires scikit-learn and matplotlib. Please install them to visualize clusters.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during t-SNE visualization: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.info(\"t-SNE visualization skipped as clustering is disabled or no embeddings available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
