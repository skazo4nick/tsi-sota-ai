{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup and Configuration ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Optional\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: article_downloader.ipynb (first cell)\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"app\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and config from utils.py\n",
    "from utils import config, create_data_directories, logger\n",
    "from pdf_downloader import download_pdf\n",
    "from html_parser import parse_article_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directories if they don't exist\n",
    "create_data_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load Input Data ---\n",
    "input_csv_filepath = 'articles.csv' # Replace with your CSV path\n",
    "try:\n",
    "    articles_df = pd.read_csv(input_csv_filepath).dropna(subset=['doi'])\n",
    "    logger.info(f\"Successfully loaded articles data from: {input_csv_filepath}. Shape: {articles_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Input CSV file not found: {input_csv_filepath}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading input CSV file: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Embedding Generation (Placeholder - To be implemented) ---\n",
    "def generate_embeddings(text: str) -> Optional[list]:\n",
    "    \"\"\"\n",
    "    Placeholder function for generating embeddings for article content.\n",
    "    Replace with your actual embedding generation code (e.g., using OpenAI, Gemini).\n",
    "    For now, returns None.\n",
    "    \"\"\"\n",
    "    logger.warning(\"Embedding generation is a placeholder - returning None.\")\n",
    "    return None # Replace with actual embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Article Processing and Dataframe Population ---\n",
    "def fetch_article_content(doi: str, title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches article content (Markdown and PDF) using DOI and title.\n",
    "    \"\"\"\n",
    "    article_data = {\n",
    "        'doi': doi,\n",
    "        'title': title,\n",
    "        'full_text_markdown': None,\n",
    "        'pdf_filepath': None,\n",
    "        'retrieval_method': None,\n",
    "        'download_success': False,\n",
    "        'cluster_id': None # Will be added later if clustering is enabled\n",
    "    }\n",
    "    article_url = f\"https://doi.org/{doi}\"\n",
    "\n",
    "    parsed_content = parse_article_html(article_url) # Use BeautifulSoup parser initially\n",
    "    if parsed_content:\n",
    "        article_data['full_text_markdown'] = parsed_content['content']\n",
    "        article_data['retrieval_method'] = parsed_content['metadata'].get('parser', 'BeautifulSoup+html2text') # Capture parser info\n",
    "        logger.info(f\"Successfully parsed article HTML for DOI: {doi} using {article_data['retrieval_method']}\")\n",
    "    else:\n",
    "        article_data['retrieval_method'] = 'HTML Parsing Failed'\n",
    "        logger.warning(f\"HTML parsing failed for DOI: {doi}\")\n",
    "\n",
    "    if config.use_clustering_in_pipeline: # Only generate embeddings if clustering is enabled in pipeline\n",
    "        embeddings = generate_embeddings(article_data['full_text_markdown'] or '') # Generate embeddings for Markdown content or empty string\n",
    "        article_data['abstract_embedding'] = embeddings # Add embedding to article data\n",
    "\n",
    "    if config.use_clustering_in_pipeline: # Only download PDFs if clustering is enabled in pipeline\n",
    "        cluster_id = article_data.get('cluster_id', 0)  # Default cluster ID if not assigned yet\n",
    "        pdf_path = download_pdf(doi, cluster_id) # Download PDF and get path\n",
    "        if pdf_path:\n",
    "            article_data['pdf_filepath'] = pdf_path\n",
    "            article_data['download_success'] = True\n",
    "            logger.info(f\"PDF download successful for DOI: {doi}, saved to: {pdf_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"PDF download failed for DOI: {doi}\")\n",
    "    else: # If no clustering, still try to download PDF to a default cluster (cluster 0) - or you can skip PDF download if no clustering\n",
    "        cluster_id = 0 # Default cluster ID for non-clustered PDFs\n",
    "        pdf_path = download_pdf(doi, cluster_id) # Download PDF to default cluster\n",
    "        if pdf_path:\n",
    "            article_data['pdf_filepath'] = pdf_path\n",
    "            article_data['download_success'] = True\n",
    "            logger.info(f\"PDF download successful for DOI: {doi}, saved to default cluster: {pdf_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"PDF download failed for DOI: {doi} (default cluster)\")\n",
    "\n",
    "\n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [] # List to store processed article data\n",
    "for index, row in tqdm(articles_df.iterrows(), total=len(articles_df), desc=\"Processing articles\"):\n",
    "    doi = row['doi']\n",
    "    title = row['title']\n",
    "    try:\n",
    "        processed_article_data = fetch_article_content(doi, title)\n",
    "        output_data.append({**row.to_dict(), **processed_article_data}) # Merge original row data with processed data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing article with DOI: {doi}. Error: {e}\", exc_info=True)\n",
    "        output_data.append({**row.to_dict(), 'error': str(e)}) # Append error info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_articles_df = pd.DataFrame(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Clustering (Conditional - if enabled in config) ---\n",
    "if config.use_clustering_in_pipeline:\n",
    "    logger.info(\"Clustering pipeline enabled. Starting clustering...\")\n",
    "    # Placeholder for embedding retrieval from DataFrame (if you implemented embedding generation)\n",
    "    embeddings_array = np.array([row['abstract_embedding'] for index, row in processed_articles_df.iterrows() if row['abstract_embedding'] is not None]) # Example - adjust based on your embedding column name\n",
    "    if embeddings_array.size > 0: # Proceed only if embeddings were generated\n",
    "        logger.info(f\"Embeddings array shape for clustering: {embeddings_array.shape}\")\n",
    "        n_clusters_optimal = config.n_clusters # Use configured number of clusters\n",
    "        kmeans = KMeans(n_clusters=n_clusters_optimal, random_state=42, n_init=10) # Explicitly set n_init\n",
    "        clusters = kmeans.fit_predict(embeddings_array)\n",
    "        processed_articles_df['cluster_id'] = -1 # Default to -1 (unassigned)\n",
    "        valid_embedding_indices = [index for index, row in processed_articles_df.iterrows() if row['abstract_embedding'] is not None] # Get indices of rows with embeddings\n",
    "        for i, index in enumerate(valid_embedding_indices):\n",
    "            processed_articles_df.at[index, 'cluster_id'] = clusters[i] # Assign cluster IDs based on original indices\n",
    "        logger.info(f\"K-Means clustering completed with {n_clusters_optimal} clusters.\")\n",
    "    else:\n",
    "        logger.warning(\"No embeddings found for clustering. Skipping clustering step.\")\n",
    "else:\n",
    "    logger.info(\"Clustering pipeline disabled in config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Organize Clusters (PDFs) and Save Results ---\n",
    "if config.use_clustering_in_pipeline:\n",
    "    cluster_counts = processed_articles_df['cluster_id'].value_counts().sort_index()\n",
    "    print(\"Cluster Distribution:\")\n",
    "    print(cluster_counts)\n",
    "\n",
    "    for cluster_id in processed_articles_df['cluster_id'].unique():\n",
    "        if cluster_id != -1: # Skip unassigned cluster (-1)\n",
    "            cluster_dir = config.get_cluster_dir(cluster_id)\n",
    "            os.makedirs(cluster_dir, exist_ok=True) # Ensure cluster directories exist\n",
    "            cluster_df = processed_articles_df[processed_articles_df['cluster_id'] == cluster_id]\n",
    "            logger.info(f\"Cluster {cluster_id}: {len(cluster_df)} articles. Sample titles: {cluster_df['title'].head(3).tolist()}\")\n",
    "else:\n",
    "    logger.info(\"Cluster organization (PDFs) skipped as clustering is disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filepath = 'processed_articles_fulltext.json' # Choose output filename\n",
    "processed_articles_df.to_json(output_json_filepath, orient='records', lines=True)\n",
    "logger.info(f\"Processed data saved to: {output_json_filepath}\")\n",
    "print(f\"Processed data saved to: {output_json_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Basic Analysis and Summary (Optional) ---\n",
    "print(\"\\nRetrieval Method Distribution:\")\n",
    "print(processed_articles_df['retrieval_method'].value_counts())\n",
    "print(\"\\nDownload Success Rate:\")\n",
    "print(processed_articles_df['download_success'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Visualization (Optional - Clustering Results if enabled) ---\n",
    "if config.use_clustering_in_pipeline and embeddings_array.size > 0:\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE # Import here, only if needed\n",
    "        tsne = TSNE(n_components=2, random_state=42, n_iter=300, perplexity=30) # Example TSNE parameters\n",
    "        tsne_results = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1], hue=processed_articles_df.loc[valid_embedding_indices, 'cluster_id'], palette='viridis', legend='full') # Use valid indices for hue\n",
    "        plt.title('Article Clusters Visualized with t-SNE')\n",
    "        plt.xlabel('TSNE Dimension 1')\n",
    "        plt.ylabel('TSNE Dimension 2')\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        logger.warning(\"t-SNE visualization requires scikit-learn and matplotlib. Please install them to visualize clusters.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during t-SNE visualization: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.info(\"t-SNE visualization skipped as clustering is disabled or no embeddings available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
