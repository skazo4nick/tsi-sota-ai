{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Keywords Analysis\n",
    "\n",
    "## 5.1 Task Setting and Description\n",
    "\n",
    "**Objective**: Extract and analyze keywords from the list of publications using Python and public APIs.\n",
    "\n",
    "**Tools**: \n",
    "- Python\n",
    "- DOI lookup APIs (e.g., CrossRef API)\n",
    "- Natural Language Processing libraries (e.g., NLTK, spaCy)\n",
    "- Pandas for data handling\n",
    "- Visualization libraries (e.g., matplotlib, seaborn, wordcloud, networkx)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Plan for Code Implementation\n",
    "\n",
    "### 1. Data Retrieval\n",
    "\n",
    "**Import Necessary Libraries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading numpy-2.1.3-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.3 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Load DOIs from CSV Files**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>short_journal</th>\n",
       "      <th>volume</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>issue</th>\n",
       "      <th>page</th>\n",
       "      <th>abstract</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>Transformative Procurement Trends: Integrating...</td>\n",
       "      <td>10.3390/logistics7030063</td>\n",
       "      <td>[{'author_name': 'Areej Althabatah', 'author_s...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63</td>\n",
       "      <td>Background: the advent of Industry 4.0 (I4.0) ...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-07</td>\n",
       "      <td>Exploring Progress with Supply Chain Risk Mana...</td>\n",
       "      <td>10.3390/logistics5040070</td>\n",
       "      <td>[{'author_name': 'Remko van Hoek', 'author_slu...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70</td>\n",
       "      <td>Background: In response to calls for actionabl...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Exploring Applications and Practical Examples ...</td>\n",
       "      <td>10.3390/logistics7040091</td>\n",
       "      <td>[{'author_name': 'João Reis', 'author_slug': '...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>91</td>\n",
       "      <td>Background: Material Requirements Planning (MR...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>Sustainable Innovations in the Food Industry t...</td>\n",
       "      <td>10.3390/logistics5040066</td>\n",
       "      <td>[{'author_name': 'Saurabh Sharma', 'author_slu...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66</td>\n",
       "      <td>The agri-food sector is an endless source of e...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>Artificial Intelligence (AI): Multidisciplinar...</td>\n",
       "      <td>10.1016/j.ijinfomgt.2019.08.002</td>\n",
       "      <td>[{'author_name': 'Yogesh K. Dwivedi', 'author_...</td>\n",
       "      <td>International Journal of Information Management</td>\n",
       "      <td>International Journal of Information Management</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Elsevier BV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101994</td>\n",
       "      <td>As far back as the industrial revolution, sign...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2023-09-13  Transformative Procurement Trends: Integrating...   \n",
       "1  2021-10-07  Exploring Progress with Supply Chain Risk Mana...   \n",
       "2  2023-12-01  Exploring Applications and Practical Examples ...   \n",
       "3  2021-09-27  Sustainable Innovations in the Food Industry t...   \n",
       "4  2021-04-01  Artificial Intelligence (AI): Multidisciplinar...   \n",
       "\n",
       "                               doi  \\\n",
       "0         10.3390/logistics7030063   \n",
       "1         10.3390/logistics5040070   \n",
       "2         10.3390/logistics7040091   \n",
       "3         10.3390/logistics5040066   \n",
       "4  10.1016/j.ijinfomgt.2019.08.002   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'author_name': 'Areej Althabatah', 'author_s...   \n",
       "1  [{'author_name': 'Remko van Hoek', 'author_slu...   \n",
       "2  [{'author_name': 'João Reis', 'author_slug': '...   \n",
       "3  [{'author_name': 'Saurabh Sharma', 'author_slu...   \n",
       "4  [{'author_name': 'Yogesh K. Dwivedi', 'author_...   \n",
       "\n",
       "                                           journal  \\\n",
       "0                                        Logistics   \n",
       "1                                        Logistics   \n",
       "2                                        Logistics   \n",
       "3                                        Logistics   \n",
       "4  International Journal of Information Management   \n",
       "\n",
       "                                     short_journal  volume  year    publisher  \\\n",
       "0                                        Logistics     7.0  2023      MDPI AG   \n",
       "1                                        Logistics     5.0  2021      MDPI AG   \n",
       "2                                        Logistics     7.0  2023      MDPI AG   \n",
       "3                                        Logistics     5.0  2021      MDPI AG   \n",
       "4  International Journal of Information Management    57.0  2021  Elsevier BV   \n",
       "\n",
       "   issue    page                                           abstract  \\\n",
       "0    3.0      63  Background: the advent of Industry 4.0 (I4.0) ...   \n",
       "1    4.0      70  Background: In response to calls for actionabl...   \n",
       "2    4.0      91  Background: Material Requirements Planning (MR...   \n",
       "3    4.0      66  The agri-food sector is an endless source of e...   \n",
       "4    NaN  101994  As far back as the industrial revolution, sign...   \n",
       "\n",
       "                                         source_file  \n",
       "0  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "1  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "2  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "3  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "4  1.2.2.1 LR - The Specialist Shortage and its I...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load DOIs from CSV files in data/references folder\n",
    "import pandas as pd\n",
    "\n",
    "# List of file paths\n",
    "file_paths = [\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.1 LR - The Specialist Shortage and its Impact.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.2 LR - AI Applications in SCM Decision Support.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.3 LR - Human-AI Collaboration in SCM.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.4 LR - Challenges and Limitations of LLMs in SCM.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.5 LR - Decision-Making Processes.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.6 LR - Agents.csv'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through file paths, read each CSV, and add a column with the source file name\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['source_file'] = file_path.split('/')[-1]  # Extract the file name from the path\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "references_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "references_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593 entries, 0 to 592\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           593 non-null    object \n",
      " 1   title          593 non-null    object \n",
      " 2   doi            593 non-null    object \n",
      " 3   authors        593 non-null    object \n",
      " 4   journal        593 non-null    object \n",
      " 5   short_journal  563 non-null    object \n",
      " 6   volume         589 non-null    float64\n",
      " 7   year           593 non-null    int64  \n",
      " 8   publisher      593 non-null    object \n",
      " 9   issue          403 non-null    float64\n",
      " 10  page           415 non-null    object \n",
      " 11  abstract       587 non-null    object \n",
      " 12  source_file    593 non-null    object \n",
      "dtypes: float64(2), int64(1), object(10)\n",
      "memory usage: 60.4+ KB\n"
     ]
    }
   ],
   "source": [
    "references_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the combined DataFrame to a JSON file\n",
    "references_df.to_json('/Users/max/Documents/Code/tsi-sota-ai/data/references_raw.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Fetch Publication Metadata Using CrossRef API**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crossrefapi\n",
      "  Downloading crossrefapi-1.6.0-py3-none-any.whl.metadata (538 bytes)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from crossrefapi) (2.32.3)\n",
      "Collecting urllib3==1.26.16 (from crossrefapi)\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (2024.8.30)\n",
      "Downloading crossrefapi-1.6.0-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, crossrefapi\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed crossrefapi-1.6.0 urllib3-1.26.16\n"
     ]
    }
   ],
   "source": [
    "!pip install crossrefapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Works\n",
    "\n",
    "# Initialize the Works client\n",
    "works = Works()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fetch metadata for a given DOI\n",
    "def fetch_metadata(doi):\n",
    "    try:\n",
    "        # Fetch metadata for the given DOI\n",
    "        metadata = works.doi(doi)\n",
    "        return {\n",
    "            'DOI': metadata.get('DOI'),\n",
    "            'title': metadata.get('title', [None])[0],\n",
    "            'authors': [author.get('given', '') + ' ' + author.get('family', '') for author in metadata.get('author', [])],\n",
    "            'abstract': metadata.get('abstract'),\n",
    "            'keywords': metadata.get('subject', [])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching metadata for DOI {doi}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOI': '10.1590/0102-311x00133115', 'title': 'Congenital Zika virus syndrome', 'authors': ['Viroj Wiwanitki'], 'abstract': None, 'keywords': []}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "doi = '10.1590/0102-311x00133115'\n",
    "metadata = fetch_metadata(doi)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (4.67.0)\n"
     ]
    }
   ],
   "source": [
    "# Loop through DOIs and collect metadata\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata: 100%|██████████| 593/593 [14:36<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "# Iterate through each DOI in the references DataFrame with a 1-second delay\n",
    "for doi in tqdm(references_df['doi'], desc='Fetching metadata'):\n",
    "    metadata = fetch_metadata(doi)\n",
    "    if metadata:\n",
    "        metadata_list.append(metadata)\n",
    "    time.sleep(1)  # Add a 1-second delay between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.3390/logistics7030063</td>\n",
       "      <td>Transformative Procurement Trends: Integrating...</td>\n",
       "      <td>[Areej Althabatah, Mohammed Yaqot, Brenno Mene...</td>\n",
       "      <td>&lt;jats:p&gt;Background: the advent of Industry 4.0...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.3390/logistics5040070</td>\n",
       "      <td>Exploring Progress with Supply Chain Risk Mana...</td>\n",
       "      <td>[Remko van Hoek]</td>\n",
       "      <td>&lt;jats:p&gt;Background: In response to calls for a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.3390/logistics7040091</td>\n",
       "      <td>Exploring Applications and Practical Examples ...</td>\n",
       "      <td>[João Reis]</td>\n",
       "      <td>&lt;jats:p&gt;Background: Material Requirements Plan...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.3390/logistics5040066</td>\n",
       "      <td>Sustainable Innovations in the Food Industry t...</td>\n",
       "      <td>[Saurabh Sharma, Vijay Kumar Gahlawat, Kumar R...</td>\n",
       "      <td>&lt;jats:p&gt;The agri-food sector is an endless sou...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.ijinfomgt.2019.08.002</td>\n",
       "      <td>Artificial Intelligence (AI): Multidisciplinar...</td>\n",
       "      <td>[Yogesh K. Dwivedi, Laurie Hughes, Elvira Isma...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               DOI  \\\n",
       "0         10.3390/logistics7030063   \n",
       "1         10.3390/logistics5040070   \n",
       "2         10.3390/logistics7040091   \n",
       "3         10.3390/logistics5040066   \n",
       "4  10.1016/j.ijinfomgt.2019.08.002   \n",
       "\n",
       "                                               title  \\\n",
       "0  Transformative Procurement Trends: Integrating...   \n",
       "1  Exploring Progress with Supply Chain Risk Mana...   \n",
       "2  Exploring Applications and Practical Examples ...   \n",
       "3  Sustainable Innovations in the Food Industry t...   \n",
       "4  Artificial Intelligence (AI): Multidisciplinar...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Areej Althabatah, Mohammed Yaqot, Brenno Mene...   \n",
       "1                                   [Remko van Hoek]   \n",
       "2                                        [João Reis]   \n",
       "3  [Saurabh Sharma, Vijay Kumar Gahlawat, Kumar R...   \n",
       "4  [Yogesh K. Dwivedi, Laurie Hughes, Elvira Isma...   \n",
       "\n",
       "                                            abstract keywords  \n",
       "0  <jats:p>Background: the advent of Industry 4.0...       []  \n",
       "1  <jats:p>Background: In response to calls for a...       []  \n",
       "2  <jats:p>Background: Material Requirements Plan...       []  \n",
       "3  <jats:p>The agri-food sector is an endless sou...       []  \n",
       "4                                               None       []  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the metadata list\n",
    "references_metadata_crossref_df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Display the first few rows of the metadata DataFrame\n",
    "references_metadata_crossref_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593 entries, 0 to 592\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   DOI       593 non-null    object\n",
      " 1   title     593 non-null    object\n",
      " 2   authors   593 non-null    object\n",
      " 3   abstract  538 non-null    object\n",
      " 4   keywords  593 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 23.3+ KB\n"
     ]
    }
   ],
   "source": [
    "references_metadata_crossref_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Crossref requests returned us empty lists for keywords, so let's try alternative approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Fetch Publication Metadata Using Semantic Scholar API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Function to fetch metadata using Semantic Scholar API with enhanced error handling and rate limiting\n",
    "def fetch_semantic_scholar_metadata(doi, max_retries=3, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Fetch metadata for a given DOI using the Semantic Scholar API.\n",
    "\n",
    "    Args:\n",
    "        doi (str): The DOI of the publication.\n",
    "        max_retries (int): Maximum number of retries for failed requests.\n",
    "        backoff_factor (int): Factor by which the wait time increases after each retry.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary containing metadata if successful, else None.\n",
    "    \"\"\"\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}'\n",
    "    params = {\n",
    "        'fields': 'title,authors,abstract,fieldsOfStudy'\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'YourProjectName/1.0 (your.email@example.com)'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return {\n",
    "                    'DOI': doi,\n",
    "                    'title': data.get('title'),\n",
    "                    'authors': [author['name'] for author in data.get('authors', [])],\n",
    "                    'abstract': data.get('abstract'),\n",
    "                    'keywords': data.get('fieldsOfStudy', [])\n",
    "                }\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limit exceeded\n",
    "                wait_time = backoff_factor ** attempt\n",
    "                print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for DOI {doi}. Status code: {response.status_code}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            print(f\"Error fetching data for DOI {doi}: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    print(f\"Failed to fetch data for DOI {doi} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOI': '10.1038/s41586-020-2649-2', 'title': 'Array programming with NumPy', 'authors': ['Charles R. Harris', 'K. Millman', 'S. Walt', 'R. Gommers', 'Pauli Virtanen', 'D. Cournapeau', 'Eric Wieser', 'Julian Taylor', 'Sebastian Berg', 'Nathaniel J. Smith', 'Robert Kern', 'Matti Picus', 'Stephan Hoyer', 'M. Kerkwijk', 'M. Brett', 'A. Haldane', \"Jaime Fern'andez del R'io\", 'Marcy Wiebe', 'Pearu Peterson', \"Pierre G'erard-Marchant\", 'Kevin Sheppard', 'Tyler Reddy', 'Warren Weckesser', 'Hameer Abbasi', 'C. Gohlke', 'T. Oliphant'], 'abstract': None, 'keywords': ['Computer Science', 'Mathematics', 'Medicine']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "doi = '10.1038/s41586-020-2649-2'\n",
    "metadata = fetch_semantic_scholar_metadata(doi)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:   4%|▍         | 23/593 [00:35<14:59,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.1287/mnsc.2019.3499. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:   9%|▊         | 51/593 [01:18<13:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.1287/mnsc.2019.3511. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  13%|█▎        | 75/593 [01:55<12:51,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.1287/mnsc.2023.4739. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  23%|██▎       | 134/593 [03:25<11:53,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.3390/smartcities6060142. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  25%|██▍       | 147/593 [03:45<11:21,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.3390/make5010017. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  25%|██▌       | 151/593 [03:51<11:23,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.3390/make5010017. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  30%|██▉       | 175/593 [04:30<10:46,  1.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[1;32m     12\u001b[0m         metadata_list\u001b[38;5;241m.\u001b[39mappend(metadata)\n\u001b[0;32m---> 13\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add a 1-second delay between requests to respect API rate limits\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the metadata list\u001b[39;00m\n\u001b[1;32m     16\u001b[0m references_metadata_semanticscholar_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(metadata_list)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize the list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "# Iterate through each DOI in the references DataFrame with a 1-second delay\n",
    "for doi in tqdm(references_df['doi'], desc='Fetching metadata'):\n",
    "    metadata = fetch_semantic_scholar_metadata(doi)\n",
    "    if metadata:\n",
    "        metadata_list.append(metadata)\n",
    "    time.sleep(1)  # Add a 1-second delay between requests to respect API rate limits\n",
    "\n",
    "# Create a DataFrame from the metadata list\n",
    "references_metadata_semanticscholar_df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Export the metadata DataFrame to a JSON file\n",
    "references_metadata_semanticscholar_df.to_json(\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references_metadata_semanticscholar.json',\n",
    "    orient='records',\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "# Display the first few rows of the metadata DataFrame\n",
    "references_metadata_semanticscholar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting 404 error, so let's wait for an API Key provisioning by Semantic Scholar. Meanwhile let's use third approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Extract Keywords from Abstracts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Use keyword extraction algorithms on available abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from nltk) (4.67.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/max/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/max/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/max/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords from text\n",
    "def extract_keywords(text, num_keywords=10):\n",
    "    \"\"\"\n",
    "    Extracts the top `num_keywords` keywords from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to extract keywords from.\n",
    "        num_keywords (int): The number of top keywords to extract.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted keywords.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter out non-alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(filtered_tokens)\n",
    "    \n",
    "    # Extract the most common keywords\n",
    "    return [word for word, count in word_counts.most_common(num_keywords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords: 100%|██████████| 593/593 [00:00<00:00, 1528.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>extracted_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.3390/logistics7030063</td>\n",
       "      <td>Transformative Procurement Trends: Integrating...</td>\n",
       "      <td>[technologies, procurement, supply, chain, man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.3390/logistics5040070</td>\n",
       "      <td>Exploring Progress with Supply Chain Risk Mana...</td>\n",
       "      <td>[pandemic, risks, risk, year, first, supply, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.3390/logistics7040091</td>\n",
       "      <td>Exploring Applications and Practical Examples ...</td>\n",
       "      <td>[mrp, python, management, article, erp, system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.3390/logistics5040066</td>\n",
       "      <td>Sustainable Innovations in the Food Industry t...</td>\n",
       "      <td>[ai, artificial, big, data, food, intelligent,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.ijinfomgt.2019.08.002</td>\n",
       "      <td>Artificial Intelligence (AI): Multidisciplinar...</td>\n",
       "      <td>[ai, significant, industrial, potential, new, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doi  \\\n",
       "0         10.3390/logistics7030063   \n",
       "1         10.3390/logistics5040070   \n",
       "2         10.3390/logistics7040091   \n",
       "3         10.3390/logistics5040066   \n",
       "4  10.1016/j.ijinfomgt.2019.08.002   \n",
       "\n",
       "                                               title  \\\n",
       "0  Transformative Procurement Trends: Integrating...   \n",
       "1  Exploring Progress with Supply Chain Risk Mana...   \n",
       "2  Exploring Applications and Practical Examples ...   \n",
       "3  Sustainable Innovations in the Food Industry t...   \n",
       "4  Artificial Intelligence (AI): Multidisciplinar...   \n",
       "\n",
       "                                  extracted_keywords  \n",
       "0  [technologies, procurement, supply, chain, man...  \n",
       "1  [pandemic, risks, risk, year, first, supply, m...  \n",
       "2  [mrp, python, management, article, erp, system...  \n",
       "3  [ai, artificial, big, data, food, intelligent,...  \n",
       "4  [ai, significant, industrial, potential, new, ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the extract_keywords function to the 'abstract' column\n",
    "tqdm.pandas(desc=\"Extracting keywords\")\n",
    "references_df['extracted_keywords'] = references_df['abstract'].progress_apply(\n",
    "    lambda x: extract_keywords(x) if pd.notnull(x) else []\n",
    ")\n",
    "\n",
    "# Display the first few rows with the extracted keywords\n",
    "references_df[['doi', 'title', 'extracted_keywords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our extracted_keywords, but those are far from being perfect. So let's use LLM to correct the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: tqdm in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (4.67.0)\n",
      "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.153.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting pydantic (from google-generativeai)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic->google-generativeai)\n",
      "  Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Downloading grpcio-1.68.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Using cached google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "Downloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "Downloading google_api_python_client-2.153.0-py2.py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.68.0-cp311-cp311-macosx_10_9_universal2.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.68.0-py3-none-any.whl (14 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: uritemplate, python-dotenv, pyparsing, pydantic-core, pyasn1, protobuf, grpcio, cachetools, annotated-types, rsa, pydantic, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.0 google-ai-generativelanguage-0.6.10 google-api-core-2.23.0 google-api-python-client-2.153.0 google-auth-2.36.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.3 googleapis-common-protos-1.66.0 grpcio-1.68.0 grpcio-status-1.68.0 httplib2-0.22.0 proto-plus-1.25.0 protobuf-5.28.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.9.2 pydantic-core-2.23.4 pyparsing-3.2.0 python-dotenv-1.0.1 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install google-generativeai python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Gemini API with the API key from .env\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini model\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to revise keywords using Gemini API\n",
    "def revise_keywords(title, abstract, extracted_keywords, max_retries=3, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Revises extracted keywords by analyzing the title and abstract using Gemini API.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the publication.\n",
    "        abstract (str): The abstract of the publication.\n",
    "        extracted_keywords (list): The initially extracted keywords.\n",
    "        max_retries (int): Maximum number of retries for failed API requests.\n",
    "        backoff_factor (int): Factor by which the wait time increases after each retry.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of revised keywords.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Given the following publication details, please provide a revised list of 6 relevant keywords.\\n\\n\"\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Abstract: {abstract}\\n\"\n",
    "        f\"Extracted Keywords: {', '.join(extracted_keywords)}\\n\\n\"\n",
    "        f\"Revised Keywords:\"\n",
    "    )\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            revised_keywords = response.text.strip().split(', ')\n",
    "            # Ensure only unique keywords and limit to 6\n",
    "            revised_keywords = list(dict.fromkeys(revised_keywords))[:6]\n",
    "            return revised_keywords\n",
    "        except Exception as e:\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            print(f\"Error revising keywords for DOI {doi}: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    print(f\"Failed to revise keywords for DOI {doi} after {max_retries} attempts.\")\n",
    "    return extracted_keywords  # Fallback to original keywords if revision fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your references DataFrame (assuming it's already loaded as references_df)\n",
    "# If not, load it using pd.read_csv or other relevant methods\n",
    "# references_df = pd.read_csv('path_to_your_references_df.csv')\n",
    "\n",
    "# Initialize the new column for revised keywords\n",
    "references_df['revised_keywords'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593 entries, 0 to 592\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                593 non-null    object \n",
      " 1   title               593 non-null    object \n",
      " 2   doi                 593 non-null    object \n",
      " 3   authors             593 non-null    object \n",
      " 4   journal             593 non-null    object \n",
      " 5   short_journal       563 non-null    object \n",
      " 6   volume              589 non-null    float64\n",
      " 7   year                593 non-null    int64  \n",
      " 8   publisher           593 non-null    object \n",
      " 9   issue               403 non-null    float64\n",
      " 10  page                415 non-null    object \n",
      " 11  abstract            587 non-null    object \n",
      " 12  source_file         593 non-null    object \n",
      " 13  extracted_keywords  593 non-null    object \n",
      " 14  revised_keywords    0 non-null      object \n",
      "dtypes: float64(2), int64(1), object(12)\n",
      "memory usage: 69.6+ KB\n"
     ]
    }
   ],
   "source": [
    "references_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Revising keywords: 100%|██████████| 593/593 [1:03:42<00:00,  6.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each row in the DataFrame and revise keywords\n",
    "for index, row in tqdm(references_df.iterrows(), total=references_df.shape[0], desc=\"Revising keywords\"):\n",
    "    doi = row['doi']\n",
    "    title = row['title']\n",
    "    abstract = row['abstract'] if pd.notnull(row['abstract']) else \"\"\n",
    "    extracted_keywords = row['extracted_keywords'] if isinstance(row['extracted_keywords'], list) else []\n",
    "    \n",
    "    revised = revise_keywords(title, abstract, extracted_keywords)\n",
    "    references_df.at[index, 'revised_keywords'] = revised\n",
    "    \n",
    "    time.sleep(1)  # Add a 1-second delay to respect API rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>extracted_keywords</th>\n",
       "      <th>revised_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.3390/logistics7030063</td>\n",
       "      <td>Transformative Procurement Trends: Integrating...</td>\n",
       "      <td>[technologies, procurement, supply, chain, man...</td>\n",
       "      <td>[1. Industry 4.0\\n2. Procurement 4.0\\n3. Suppl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.3390/logistics5040070</td>\n",
       "      <td>Exploring Progress with Supply Chain Risk Mana...</td>\n",
       "      <td>[pandemic, risks, risk, year, first, supply, m...</td>\n",
       "      <td>[1. Supply Chain Risk Management (SCRM)\\n2. CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.3390/logistics7040091</td>\n",
       "      <td>Exploring Applications and Practical Examples ...</td>\n",
       "      <td>[mrp, python, management, article, erp, system...</td>\n",
       "      <td>[1. Material Requirements Planning (MRP)\\n2. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.3390/logistics5040066</td>\n",
       "      <td>Sustainable Innovations in the Food Industry t...</td>\n",
       "      <td>[ai, artificial, big, data, food, intelligent,...</td>\n",
       "      <td>[1. AI in Food Industry\\n2. Big Data Analytics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.ijinfomgt.2019.08.002</td>\n",
       "      <td>Artificial Intelligence (AI): Multidisciplinar...</td>\n",
       "      <td>[ai, significant, industrial, potential, new, ...</td>\n",
       "      <td>[Artificial Intelligence (AI),  Technological ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doi  \\\n",
       "0         10.3390/logistics7030063   \n",
       "1         10.3390/logistics5040070   \n",
       "2         10.3390/logistics7040091   \n",
       "3         10.3390/logistics5040066   \n",
       "4  10.1016/j.ijinfomgt.2019.08.002   \n",
       "\n",
       "                                               title  \\\n",
       "0  Transformative Procurement Trends: Integrating...   \n",
       "1  Exploring Progress with Supply Chain Risk Mana...   \n",
       "2  Exploring Applications and Practical Examples ...   \n",
       "3  Sustainable Innovations in the Food Industry t...   \n",
       "4  Artificial Intelligence (AI): Multidisciplinar...   \n",
       "\n",
       "                                  extracted_keywords  \\\n",
       "0  [technologies, procurement, supply, chain, man...   \n",
       "1  [pandemic, risks, risk, year, first, supply, m...   \n",
       "2  [mrp, python, management, article, erp, system...   \n",
       "3  [ai, artificial, big, data, food, intelligent,...   \n",
       "4  [ai, significant, industrial, potential, new, ...   \n",
       "\n",
       "                                    revised_keywords  \n",
       "0  [1. Industry 4.0\\n2. Procurement 4.0\\n3. Suppl...  \n",
       "1  [1. Supply Chain Risk Management (SCRM)\\n2. CO...  \n",
       "2  [1. Material Requirements Planning (MRP)\\n2. P...  \n",
       "3  [1. AI in Food Industry\\n2. Big Data Analytics...  \n",
       "4  [Artificial Intelligence (AI),  Technological ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows with the revised keywords\n",
    "references_df[['doi', 'title', 'extracted_keywords', 'revised_keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV or JSON file\n",
    "# references_df.to_csv('path_to_save_revised_keywords.csv', index=False)\n",
    "references_df.to_json('/Users/max/Documents/Code/tsi-sota-ai/dah geminita/references_with_keywords_gemini.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Cleaning\n",
    "\n",
    "**Import NLP Libraries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data files if necessary\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Normalize Text**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "# Remove stopwords\n",
    "# Perform stemming or lemmatization\n",
    "def clean_text(text):\n",
    "    # Tokenize text\n",
    "    # Remove stopwords\n",
    "    # Lemmatize words\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Keyword Extraction\n",
    "\n",
    "**Using TF-IDF or RAKE Algorithm**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "# Extract top keywords based on TF-IDF scores\n",
    "\n",
    "# Option 2: RAKE Algorithm\n",
    "# from rake_nltk import Rake\n",
    "\n",
    "# Initialize RAKE\n",
    "# rake_nltk_var = Rake()\n",
    "\n",
    "# Extract keywords for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Visualization\n",
    "\n",
    "**Generate Word Clouds or Bar Charts**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate a word cloud of keywords\n",
    "wordcloud = WordCloud(background_color='white').generate_from_frequencies(keyword_frequencies)\n",
    "\n",
    "# Display the generated image\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Infographics Generation\n",
    "\n",
    "**Create Semantic Graphs of Keyword Relationships**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build a graph where nodes are keywords and edges represent co-occurrence\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on keyword relationships\n",
    "\n",
    "# Draw the network graph\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_size=50, font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Use Visualization Libraries**\n",
    "\n",
    "- Utilize `networkx` for building and visualizing network graphs.\n",
    "- Use `matplotlib` or `seaborn` for enhanced visualization aesthetics.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: Replace placeholder code with actual implementations. Ensure all necessary data files are correctly referenced, and dependencies are installed in your environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
