{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Keywords Analysis\n",
    "\n",
    "## 5.1 Task Setting and Description\n",
    "\n",
    "**Objective**: Extract and analyze keywords from the list of publications using Python and public APIs.\n",
    "\n",
    "**Tools**: \n",
    "- Python\n",
    "- DOI lookup APIs (e.g., CrossRef API)\n",
    "- Natural Language Processing libraries (e.g., NLTK, spaCy)\n",
    "- Pandas for data handling\n",
    "- Visualization libraries (e.g., matplotlib, seaborn, wordcloud, networkx)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Plan for Code Implementation\n",
    "\n",
    "### 1. Data Retrieval\n",
    "\n",
    "**Import Necessary Libraries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading numpy-2.1.3-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.3 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Load DOIs from CSV Files**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>short_journal</th>\n",
       "      <th>volume</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>issue</th>\n",
       "      <th>page</th>\n",
       "      <th>abstract</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>Transformative Procurement Trends: Integrating...</td>\n",
       "      <td>10.3390/logistics7030063</td>\n",
       "      <td>[{'author_name': 'Areej Althabatah', 'author_s...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63</td>\n",
       "      <td>Background: the advent of Industry 4.0 (I4.0) ...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-07</td>\n",
       "      <td>Exploring Progress with Supply Chain Risk Mana...</td>\n",
       "      <td>10.3390/logistics5040070</td>\n",
       "      <td>[{'author_name': 'Remko van Hoek', 'author_slu...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70</td>\n",
       "      <td>Background: In response to calls for actionabl...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Exploring Applications and Practical Examples ...</td>\n",
       "      <td>10.3390/logistics7040091</td>\n",
       "      <td>[{'author_name': 'João Reis', 'author_slug': '...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>91</td>\n",
       "      <td>Background: Material Requirements Planning (MR...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>Sustainable Innovations in the Food Industry t...</td>\n",
       "      <td>10.3390/logistics5040066</td>\n",
       "      <td>[{'author_name': 'Saurabh Sharma', 'author_slu...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66</td>\n",
       "      <td>The agri-food sector is an endless source of e...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>Artificial Intelligence (AI): Multidisciplinar...</td>\n",
       "      <td>10.1016/j.ijinfomgt.2019.08.002</td>\n",
       "      <td>[{'author_name': 'Yogesh K. Dwivedi', 'author_...</td>\n",
       "      <td>International Journal of Information Management</td>\n",
       "      <td>International Journal of Information Management</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Elsevier BV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101994</td>\n",
       "      <td>As far back as the industrial revolution, sign...</td>\n",
       "      <td>1.2.2.1 LR - The Specialist Shortage and its I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2023-09-13  Transformative Procurement Trends: Integrating...   \n",
       "1  2021-10-07  Exploring Progress with Supply Chain Risk Mana...   \n",
       "2  2023-12-01  Exploring Applications and Practical Examples ...   \n",
       "3  2021-09-27  Sustainable Innovations in the Food Industry t...   \n",
       "4  2021-04-01  Artificial Intelligence (AI): Multidisciplinar...   \n",
       "\n",
       "                               doi  \\\n",
       "0         10.3390/logistics7030063   \n",
       "1         10.3390/logistics5040070   \n",
       "2         10.3390/logistics7040091   \n",
       "3         10.3390/logistics5040066   \n",
       "4  10.1016/j.ijinfomgt.2019.08.002   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'author_name': 'Areej Althabatah', 'author_s...   \n",
       "1  [{'author_name': 'Remko van Hoek', 'author_slu...   \n",
       "2  [{'author_name': 'João Reis', 'author_slug': '...   \n",
       "3  [{'author_name': 'Saurabh Sharma', 'author_slu...   \n",
       "4  [{'author_name': 'Yogesh K. Dwivedi', 'author_...   \n",
       "\n",
       "                                           journal  \\\n",
       "0                                        Logistics   \n",
       "1                                        Logistics   \n",
       "2                                        Logistics   \n",
       "3                                        Logistics   \n",
       "4  International Journal of Information Management   \n",
       "\n",
       "                                     short_journal  volume  year    publisher  \\\n",
       "0                                        Logistics     7.0  2023      MDPI AG   \n",
       "1                                        Logistics     5.0  2021      MDPI AG   \n",
       "2                                        Logistics     7.0  2023      MDPI AG   \n",
       "3                                        Logistics     5.0  2021      MDPI AG   \n",
       "4  International Journal of Information Management    57.0  2021  Elsevier BV   \n",
       "\n",
       "   issue    page                                           abstract  \\\n",
       "0    3.0      63  Background: the advent of Industry 4.0 (I4.0) ...   \n",
       "1    4.0      70  Background: In response to calls for actionabl...   \n",
       "2    4.0      91  Background: Material Requirements Planning (MR...   \n",
       "3    4.0      66  The agri-food sector is an endless source of e...   \n",
       "4    NaN  101994  As far back as the industrial revolution, sign...   \n",
       "\n",
       "                                         source_file  \n",
       "0  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "1  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "2  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "3  1.2.2.1 LR - The Specialist Shortage and its I...  \n",
       "4  1.2.2.1 LR - The Specialist Shortage and its I...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load DOIs from CSV files in data/references folder\n",
    "import pandas as pd\n",
    "\n",
    "# List of file paths\n",
    "file_paths = [\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.1 LR - The Specialist Shortage and its Impact.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.2 LR - AI Applications in SCM Decision Support.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.3 LR - Human-AI Collaboration in SCM.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.4 LR - Challenges and Limitations of LLMs in SCM.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.5 LR - Decision-Making Processes.csv',\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references/1.2.2.6 LR - Agents.csv'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through file paths, read each CSV, and add a column with the source file name\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['source_file'] = file_path.split('/')[-1]  # Extract the file name from the path\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "references_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "references_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593 entries, 0 to 592\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           593 non-null    object \n",
      " 1   title          593 non-null    object \n",
      " 2   doi            593 non-null    object \n",
      " 3   authors        593 non-null    object \n",
      " 4   journal        593 non-null    object \n",
      " 5   short_journal  563 non-null    object \n",
      " 6   volume         589 non-null    float64\n",
      " 7   year           593 non-null    int64  \n",
      " 8   publisher      593 non-null    object \n",
      " 9   issue          403 non-null    float64\n",
      " 10  page           415 non-null    object \n",
      " 11  abstract       587 non-null    object \n",
      " 12  source_file    593 non-null    object \n",
      "dtypes: float64(2), int64(1), object(10)\n",
      "memory usage: 60.4+ KB\n"
     ]
    }
   ],
   "source": [
    "references_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the combined DataFrame to a JSON file\n",
    "references_df.to_json('/Users/max/Documents/Code/tsi-sota-ai/data/references_raw.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Fetch Publication Metadata Using CrossRef API**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crossrefapi\n",
      "  Downloading crossrefapi-1.6.0-py3-none-any.whl.metadata (538 bytes)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from crossrefapi) (2.32.3)\n",
      "Collecting urllib3==1.26.16 (from crossrefapi)\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->crossrefapi) (2024.8.30)\n",
      "Downloading crossrefapi-1.6.0-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, crossrefapi\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed crossrefapi-1.6.0 urllib3-1.26.16\n"
     ]
    }
   ],
   "source": [
    "!pip install crossrefapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Works\n",
    "\n",
    "# Initialize the Works client\n",
    "works = Works()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fetch metadata for a given DOI\n",
    "def fetch_metadata(doi):\n",
    "    try:\n",
    "        # Fetch metadata for the given DOI\n",
    "        metadata = works.doi(doi)\n",
    "        return {\n",
    "            'DOI': metadata.get('DOI'),\n",
    "            'title': metadata.get('title', [None])[0],\n",
    "            'authors': [author.get('given', '') + ' ' + author.get('family', '') for author in metadata.get('author', [])],\n",
    "            'abstract': metadata.get('abstract'),\n",
    "            'keywords': metadata.get('subject', [])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching metadata for DOI {doi}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOI': '10.1590/0102-311x00133115', 'title': 'Congenital Zika virus syndrome', 'authors': ['Viroj Wiwanitki'], 'abstract': None, 'keywords': []}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "doi = '10.1590/0102-311x00133115'\n",
    "metadata = fetch_metadata(doi)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (4.67.0)\n"
     ]
    }
   ],
   "source": [
    "# Loop through DOIs and collect metadata\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata: 100%|██████████| 593/593 [14:36<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "# Iterate through each DOI in the references DataFrame with a 1-second delay\n",
    "for doi in tqdm(references_df['doi'], desc='Fetching metadata'):\n",
    "    metadata = fetch_metadata(doi)\n",
    "    if metadata:\n",
    "        metadata_list.append(metadata)\n",
    "    time.sleep(1)  # Add a 1-second delay between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.3390/logistics7030063</td>\n",
       "      <td>Transformative Procurement Trends: Integrating...</td>\n",
       "      <td>[Areej Althabatah, Mohammed Yaqot, Brenno Mene...</td>\n",
       "      <td>&lt;jats:p&gt;Background: the advent of Industry 4.0...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.3390/logistics5040070</td>\n",
       "      <td>Exploring Progress with Supply Chain Risk Mana...</td>\n",
       "      <td>[Remko van Hoek]</td>\n",
       "      <td>&lt;jats:p&gt;Background: In response to calls for a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.3390/logistics7040091</td>\n",
       "      <td>Exploring Applications and Practical Examples ...</td>\n",
       "      <td>[João Reis]</td>\n",
       "      <td>&lt;jats:p&gt;Background: Material Requirements Plan...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.3390/logistics5040066</td>\n",
       "      <td>Sustainable Innovations in the Food Industry t...</td>\n",
       "      <td>[Saurabh Sharma, Vijay Kumar Gahlawat, Kumar R...</td>\n",
       "      <td>&lt;jats:p&gt;The agri-food sector is an endless sou...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1016/j.ijinfomgt.2019.08.002</td>\n",
       "      <td>Artificial Intelligence (AI): Multidisciplinar...</td>\n",
       "      <td>[Yogesh K. Dwivedi, Laurie Hughes, Elvira Isma...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               DOI  \\\n",
       "0         10.3390/logistics7030063   \n",
       "1         10.3390/logistics5040070   \n",
       "2         10.3390/logistics7040091   \n",
       "3         10.3390/logistics5040066   \n",
       "4  10.1016/j.ijinfomgt.2019.08.002   \n",
       "\n",
       "                                               title  \\\n",
       "0  Transformative Procurement Trends: Integrating...   \n",
       "1  Exploring Progress with Supply Chain Risk Mana...   \n",
       "2  Exploring Applications and Practical Examples ...   \n",
       "3  Sustainable Innovations in the Food Industry t...   \n",
       "4  Artificial Intelligence (AI): Multidisciplinar...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Areej Althabatah, Mohammed Yaqot, Brenno Mene...   \n",
       "1                                   [Remko van Hoek]   \n",
       "2                                        [João Reis]   \n",
       "3  [Saurabh Sharma, Vijay Kumar Gahlawat, Kumar R...   \n",
       "4  [Yogesh K. Dwivedi, Laurie Hughes, Elvira Isma...   \n",
       "\n",
       "                                            abstract keywords  \n",
       "0  <jats:p>Background: the advent of Industry 4.0...       []  \n",
       "1  <jats:p>Background: In response to calls for a...       []  \n",
       "2  <jats:p>Background: Material Requirements Plan...       []  \n",
       "3  <jats:p>The agri-food sector is an endless sou...       []  \n",
       "4                                               None       []  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the metadata list\n",
    "references_metadata_crossref_df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Display the first few rows of the metadata DataFrame\n",
    "references_metadata_crossref_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593 entries, 0 to 592\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   DOI       593 non-null    object\n",
      " 1   title     593 non-null    object\n",
      " 2   authors   593 non-null    object\n",
      " 3   abstract  538 non-null    object\n",
      " 4   keywords  593 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 23.3+ KB\n"
     ]
    }
   ],
   "source": [
    "references_metadata_crossref_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Crossref requests returned us empty lists for keywords, so let's try alternative approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Fetch Publication Metadata Using Semantic Scholar API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Function to fetch metadata using Semantic Scholar API with enhanced error handling and rate limiting\n",
    "def fetch_semantic_scholar_metadata(doi, max_retries=3, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Fetch metadata for a given DOI using the Semantic Scholar API.\n",
    "\n",
    "    Args:\n",
    "        doi (str): The DOI of the publication.\n",
    "        max_retries (int): Maximum number of retries for failed requests.\n",
    "        backoff_factor (int): Factor by which the wait time increases after each retry.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary containing metadata if successful, else None.\n",
    "    \"\"\"\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}'\n",
    "    params = {\n",
    "        'fields': 'title,authors,abstract,fieldsOfStudy'\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'YourProjectName/1.0 (your.email@example.com)'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return {\n",
    "                    'DOI': doi,\n",
    "                    'title': data.get('title'),\n",
    "                    'authors': [author['name'] for author in data.get('authors', [])],\n",
    "                    'abstract': data.get('abstract'),\n",
    "                    'keywords': data.get('fieldsOfStudy', [])\n",
    "                }\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limit exceeded\n",
    "                wait_time = backoff_factor ** attempt\n",
    "                print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for DOI {doi}. Status code: {response.status_code}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            wait_time = backoff_factor ** attempt\n",
    "            print(f\"Error fetching data for DOI {doi}: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    print(f\"Failed to fetch data for DOI {doi} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOI': '10.1038/s41586-020-2649-2', 'title': 'Array programming with NumPy', 'authors': ['Charles R. Harris', 'K. Millman', 'S. Walt', 'R. Gommers', 'Pauli Virtanen', 'D. Cournapeau', 'Eric Wieser', 'Julian Taylor', 'Sebastian Berg', 'Nathaniel J. Smith', 'Robert Kern', 'Matti Picus', 'Stephan Hoyer', 'M. Kerkwijk', 'M. Brett', 'A. Haldane', \"Jaime Fern'andez del R'io\", 'Marcy Wiebe', 'Pearu Peterson', \"Pierre G'erard-Marchant\", 'Kevin Sheppard', 'Tyler Reddy', 'Warren Weckesser', 'Hameer Abbasi', 'C. Gohlke', 'T. Oliphant'], 'abstract': None, 'keywords': ['Computer Science', 'Mathematics', 'Medicine']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "doi = '10.1038/s41586-020-2649-2'\n",
    "metadata = fetch_semantic_scholar_metadata(doi)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:   4%|▍         | 23/593 [00:35<14:59,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.1287/mnsc.2019.3499. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:   9%|▊         | 51/593 [01:18<13:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.1287/mnsc.2019.3511. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  13%|█▎        | 75/593 [01:55<12:51,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.1287/mnsc.2023.4739. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  23%|██▎       | 134/593 [03:25<11:53,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.3390/smartcities6060142. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  25%|██▍       | 147/593 [03:45<11:21,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.3390/make5010017. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  25%|██▌       | 151/593 [03:51<11:23,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for DOI 10.3390/make5010017. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata:  30%|██▉       | 175/593 [04:30<10:46,  1.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[1;32m     12\u001b[0m         metadata_list\u001b[38;5;241m.\u001b[39mappend(metadata)\n\u001b[0;32m---> 13\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add a 1-second delay between requests to respect API rate limits\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the metadata list\u001b[39;00m\n\u001b[1;32m     16\u001b[0m references_metadata_semanticscholar_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(metadata_list)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize the list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "# Iterate through each DOI in the references DataFrame with a 1-second delay\n",
    "for doi in tqdm(references_df['doi'], desc='Fetching metadata'):\n",
    "    metadata = fetch_semantic_scholar_metadata(doi)\n",
    "    if metadata:\n",
    "        metadata_list.append(metadata)\n",
    "    time.sleep(1)  # Add a 1-second delay between requests to respect API rate limits\n",
    "\n",
    "# Create a DataFrame from the metadata list\n",
    "references_metadata_semanticscholar_df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Export the metadata DataFrame to a JSON file\n",
    "references_metadata_semanticscholar_df.to_json(\n",
    "    '/Users/max/Documents/Code/tsi-sota-ai/data/references_metadata_semanticscholar.json',\n",
    "    orient='records',\n",
    "    lines=True\n",
    ")\n",
    "\n",
    "# Display the first few rows of the metadata DataFrame\n",
    "references_metadata_semanticscholar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting 404 error, so let's wait for an API Key provisioning by Semantic Scholar. Meanwhile let's use third approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Extract Keywords from Abstracts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Use keyword extraction algorithms on available abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /Users/max/miniconda3/envs/tsi/lib/python3.11/site-packages (from nltk) (4.67.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/max/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords from text\n",
    "def extract_keywords(text, num_keywords=10):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    word_counts = Counter(filtered_tokens)\n",
    "    return [word for word, count in word_counts.most_common(num_keywords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Cleaning\n",
    "\n",
    "**Import NLP Libraries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data files if necessary\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Normalize Text**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "# Remove stopwords\n",
    "# Perform stemming or lemmatization\n",
    "def clean_text(text):\n",
    "    # Tokenize text\n",
    "    # Remove stopwords\n",
    "    # Lemmatize words\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Keyword Extraction\n",
    "\n",
    "**Using TF-IDF or RAKE Algorithm**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "# Extract top keywords based on TF-IDF scores\n",
    "\n",
    "# Option 2: RAKE Algorithm\n",
    "# from rake_nltk import Rake\n",
    "\n",
    "# Initialize RAKE\n",
    "# rake_nltk_var = Rake()\n",
    "\n",
    "# Extract keywords for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Visualization\n",
    "\n",
    "**Generate Word Clouds or Bar Charts**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate a word cloud of keywords\n",
    "wordcloud = WordCloud(background_color='white').generate_from_frequencies(keyword_frequencies)\n",
    "\n",
    "# Display the generated image\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Infographics Generation\n",
    "\n",
    "**Create Semantic Graphs of Keyword Relationships**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build a graph where nodes are keywords and edges represent co-occurrence\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges based on keyword relationships\n",
    "\n",
    "# Draw the network graph\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_size=50, font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Use Visualization Libraries**\n",
    "\n",
    "- Utilize `networkx` for building and visualizing network graphs.\n",
    "- Use `matplotlib` or `seaborn` for enhanced visualization aesthetics.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: Replace placeholder code with actual implementations. Ensure all necessary data files are correctly referenced, and dependencies are installed in your environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
