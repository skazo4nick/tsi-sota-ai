{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference List Analysis\n",
    "\n",
    "This notebook analyzes all the articles references from files stored in our B2 bucket. We load the files directly from B2 using the native B2 SDK (b2sdk) into memory, combine them into a single DataFrame, and produce summary statistics such as:\n",
    "\n",
    "- Total number of articles\n",
    "- Distribution by publication year\n",
    "- List of unique journals\n",
    "- Additional descriptive statistics\n",
    "\n",
    "This analysis will help us quickly get an overview of the articles we plan to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error copying file: [Errno 2] No such file or directory: 'app/.env'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Update: The source .env file is located in the \"app\" folder,\n",
    "# and the target location is now the \"notebooks\" folder.\n",
    "source_env_file = os.path.join('app', '.env')\n",
    "dest_env_file = os.path.join('notebooks', '.env')\n",
    "\n",
    "try:\n",
    "    shutil.copy(source_env_file, dest_env_file)\n",
    "    print(\"Successfully copied .env file from the app folder to the notebooks folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error copying file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "from b2sdk.v2 import InMemoryAccountInfo, B2Api\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from repo root\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Get B2 credentials from environment\n",
    "application_key = os.getenv(\"B2_APPLICATION_KEY\")\n",
    "application_key_id = os.getenv(\"B2_APPLICATION_KEY_ID\")\n",
    "bucket_name = os.getenv(\"B2_BUCKET_NAME\")\n",
    "\n",
    "if not all([application_key, application_key_id, bucket_name]):\n",
    "    raise Exception(\"One or more required B2 credentials are missing from the environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in B2 bucket under references/xls/\n",
      "references/xls/1.2.2.1 LR - The Specialist Shortage and its Impact.csv\n",
      "references/xls/1.2.2.2 LR - AI Applications in SCM Decision Support.csv\n",
      "references/xls/1.2.2.3 LR - Human-AI Collaboration in SCM.csv\n",
      "references/xls/1.2.2.4 LR - Challenges and Limitations of LLMs in SCM.csv\n",
      "references/xls/1.2.2.5 LR - Decision-Making Processes.csv\n",
      "references/xls/1.2.2.6 LR - Agents.csv\n"
     ]
    }
   ],
   "source": [
    "# Set up B2 connection\n",
    "info = InMemoryAccountInfo()\n",
    "b2_api = B2Api(info)\n",
    "b2_api.authorize_account(\"production\", application_key_id, application_key)\n",
    "\n",
    "# Get bucket object\n",
    "bucket = b2_api.get_bucket_by_name(bucket_name)\n",
    "\n",
    "# List all Excel files in the references/xls directory\n",
    "prefix = 'references/xls/'\n",
    "files_info = [\n",
    "    file_version_info\n",
    "    for file_version_info, _ in bucket.ls(prefix)\n",
    "    if file_version_info.file_name.endswith(('.csv'))\n",
    "]\n",
    "\n",
    "print(\"Files in B2 bucket under\", prefix)\n",
    "for file_info in files_info:\n",
    "    print(file_info.file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DownloadedFile' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m downloaded_file \u001b[38;5;241m=\u001b[39m bucket\u001b[38;5;241m.\u001b[39mdownload_file_by_name(filename)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Read directly from the downloaded file (no .open() needed)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m file_data \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO(\u001b[43mdownloaded_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# Corrected - call .read() as a method\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Read CSV into DataFrame\u001b[39;00m\n\u001b[1;32m     28\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DownloadedFile' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Dictionary with exact filenames\n",
    "file_mapping = {\n",
    "    'references/xls/1.2.2.1 LR - The Specialist Shortage and its Impact.csv': '1_specialists_df',\n",
    "    'references/xls/1.2.2.2 LR - AI Applications in SCM Decision Support.csv': '2_aiscm_df',\n",
    "    'references/xls/1.2.2.3 LR - Human-AI Collaboration in SCM.csv': '3_humanai_df',\n",
    "    'references/xls/1.2.2.4 LR - Challenges and Limitations of LLMs in SCM.csv': '4_challenges_df',\n",
    "    'references/xls/1.2.2.5 LR - Decision-Making Processes.csv': '5_decision_df',\n",
    "    'references/xls/1.2.2.6 LR - Agents.csv': '6_agents_df'\n",
    "}\n",
    "\n",
    "# Initialize dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Download and read each file\n",
    "for file_info in files_info:\n",
    "    filename = file_info.file_name\n",
    "    if filename in file_mapping:\n",
    "        # Download file content\n",
    "        downloaded_file = bucket.download_file_by_name(filename)\n",
    "        \n",
    "        # Read directly from the downloaded file (no .open() needed)\n",
    "        file_data = io.StringIO(downloaded_file.read().decode('utf-8')) # Corrected - call .read() as a method\n",
    "        \n",
    "        # Read CSV into DataFrame\n",
    "        df = pd.read_csv(file_data)\n",
    "        df_name = file_mapping[filename]\n",
    "        dataframes[df_name] = df\n",
    "        print(f\"Loaded {filename} into {df_name} with shape {df.shape}\")\n",
    "    else:\n",
    "        print(f\"Skipping unmapped file: {filename}\")\n",
    "\n",
    "# Assign DataFrames to individual variables\n",
    "locals().update(dataframes)\n",
    "\n",
    "# Print basic info about each DataFrame\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{name} info:\")\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "Below are the first few rows of the combined references DataFrame to inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of articles\n",
    "total_articles = references_df.shape[0]\n",
    "print(f\"Total number of articles: {total_articles}\")\n",
    "\n",
    "# Distribution by publication year (assuming 'year' column exists)\n",
    "if 'year' in references_df.columns:\n",
    "    year_distribution = references_df['year'].value_counts().sort_index()\n",
    "    print(\"\\nPublication Year Distribution:\")\n",
    "    print(year_distribution)\n",
    "else:\n",
    "    print(\"The column 'year' is not found in the data.\")\n",
    "\n",
    "# List of unique journals (assuming 'journal' column exists)\n",
    "if 'journal' in references_df.columns:\n",
    "    unique_journals = references_df['journal'].unique()\n",
    "    print(f\"\\nUnique journals ({len(unique_journals)}):\")\n",
    "    print(unique_journals)\n",
    "else:\n",
    "    print(\"The column 'journal' is not found in the data.\")\n",
    "\n",
    "# Publisher distribution if 'publisher' column exists\n",
    "if 'publisher' in references_df.columns:\n",
    "    publisher_distribution = references_df['publisher'].value_counts()\n",
    "    print(\"\\nPublisher Distribution (top 10):\")\n",
    "    print(publisher_distribution.head(10))\n",
    "else:\n",
    "    print(\"The column 'publisher' is not found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOI Analysis and Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing DOIs\n",
    "missing_dois = references_df['doi'].isna().sum()\n",
    "print(f\"Number of entries with missing DOIs: {missing_dois}\")\n",
    "\n",
    "# Check DOI patterns\n",
    "if not missing_dois == len(references_df):\n",
    "    print(\"\\nSample of DOI patterns:\")\n",
    "    print(references_df['doi'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Analysis\n",
    "\n",
    "Analyzing abstracts can help us understand the content distribution and identify potential data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'abstract' in references_df.columns:\n",
    "    # Calculate abstract lengths\n",
    "    references_df['abstract_length'] = references_df['abstract'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"Abstract Statistics:\")\n",
    "    print(f\"Mean length: {references_df['abstract_length'].mean():.2f} characters\")\n",
    "    print(f\"Median length: {references_df['abstract_length'].median():.0f} characters\")\n",
    "    print(f\"Shortest abstract: {references_df['abstract_length'].min()} characters\")\n",
    "    print(f\"Longest abstract: {references_df['abstract_length'].max()} characters\")\n",
    "    \n",
    "    # Check for missing abstracts\n",
    "    missing_abstracts = references_df['abstract'].isna().sum()\n",
    "    print(f\"\\nNumber of entries with missing abstracts: {missing_abstracts}\")\n",
    "else:\n",
    "    print(\"The column 'abstract' is not found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Save the processed DataFrame for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON for easier reading\n",
    "output_json = os.path.join('data', 'references_analysis.json')\n",
    "references_df.to_json(output_json, orient='records', indent=2)\n",
    "print(f\"Saved processed data to: {output_json}\")\n",
    "\n",
    "# Save basic statistics to a separate file\n",
    "stats_dict = {\n",
    "    'total_articles': total_articles,\n",
    "    'unique_journals': len(references_df['journal'].unique()) if 'journal' in references_df.columns else 0,\n",
    "    'year_range': f\"{references_df['year'].min()}-{references_df['year'].max()}\" if 'year' in references_df.columns else 'N/A',\n",
    "    'missing_dois': missing_dois if 'doi' in references_df.columns else 'N/A',\n",
    "    'missing_abstracts': missing_abstracts if 'abstract' in references_df.columns else 'N/A'\n",
    "}\n",
    "\n",
    "stats_json = os.path.join('data', 'references_stats.json')\n",
    "with open(stats_json, 'w') as f:\n",
    "    json.dump(stats_dict, f, indent=2)\n",
    "print(f\"Saved statistics to: {stats_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
