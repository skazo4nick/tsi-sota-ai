{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference List Analysis\n",
    "\n",
    "This notebook analyzes all the articles references from our CSV files located in the `data/references` folder. We load the files, combine them into a single DataFrame, and produce summary statistics such as:\n",
    "\n",
    "- Total number of articles\n",
    "- Distribution by publication year\n",
    "- List of unique journals\n",
    "- Additional descriptive statistics\n",
    "\n",
    "This analysis will help us quickly get an overview of the articles we plan to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in: /workspaces/tsi-sota-ai/data/references\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Adjust path to be relative to the repository root\n",
    "# Going up one directory from notebooks/ to reach the project root\n",
    "references_dir = os.path.join('..', 'data', 'references')\n",
    "\n",
    "# Print absolute path to help debug\n",
    "print(f\"Looking for files in: {os.path.abspath(references_dir)}\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(references_dir):\n",
    "    print(f\"Directory does not exist: {references_dir}\")\n",
    "    # Create if needed\n",
    "    # os.makedirs(references_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in: /workspaces/tsi-sota-ai/data/references\n",
      "Loaded 1.2.2.1 LR - The Specialist Shortage and its Impact.csv into 1_specialists_df with shape (94, 12)\n",
      "Loaded 1.2.2.2 LR - AI Applications in SCM Decision Support.csv into 2_aiscm_df with shape (69, 12)\n",
      "Loaded 1.2.2.3 LR - Human-AI Collaboration in SCM.csv into 3_humanai_df with shape (97, 12)\n",
      "Loaded 1.2.2.4 LR - Challenges and Limitations of LLMs in SCM.csv into 4_challenges_df with shape (54, 12)\n",
      "Loaded 1.2.2.5 LR - Decision-Making Processes.csv into 5_decision_df with shape (110, 12)\n",
      "Loaded 1.2.2.6 LR - Agents.csv into 6_agents_df with shape (169, 12)\n",
      "\n",
      "1_specialists_df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94 entries, 0 to 93\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           94 non-null     object \n",
      " 1   title          94 non-null     object \n",
      " 2   doi            94 non-null     object \n",
      " 3   authors        94 non-null     object \n",
      " 4   journal        94 non-null     object \n",
      " 5   short_journal  87 non-null     object \n",
      " 6   volume         94 non-null     int64  \n",
      " 7   year           94 non-null     int64  \n",
      " 8   publisher      94 non-null     object \n",
      " 9   issue          84 non-null     float64\n",
      " 10  page           88 non-null     object \n",
      " 11  abstract       93 non-null     object \n",
      "dtypes: float64(1), int64(2), object(9)\n",
      "memory usage: 8.9+ KB\n",
      "None\n",
      "\n",
      "2_aiscm_df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69 entries, 0 to 68\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           69 non-null     object \n",
      " 1   title          69 non-null     object \n",
      " 2   doi            69 non-null     object \n",
      " 3   authors        69 non-null     object \n",
      " 4   journal        69 non-null     object \n",
      " 5   short_journal  68 non-null     object \n",
      " 6   volume         68 non-null     float64\n",
      " 7   year           69 non-null     int64  \n",
      " 8   publisher      69 non-null     object \n",
      " 9   issue          58 non-null     float64\n",
      " 10  page           54 non-null     object \n",
      " 11  abstract       69 non-null     object \n",
      "dtypes: float64(2), int64(1), object(9)\n",
      "memory usage: 6.6+ KB\n",
      "None\n",
      "\n",
      "3_humanai_df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           97 non-null     object \n",
      " 1   title          97 non-null     object \n",
      " 2   doi            97 non-null     object \n",
      " 3   authors        97 non-null     object \n",
      " 4   journal        97 non-null     object \n",
      " 5   short_journal  97 non-null     object \n",
      " 6   volume         97 non-null     int64  \n",
      " 7   year           97 non-null     int64  \n",
      " 8   publisher      97 non-null     object \n",
      " 9   issue          42 non-null     float64\n",
      " 10  page           44 non-null     object \n",
      " 11  abstract       96 non-null     object \n",
      "dtypes: float64(1), int64(2), object(9)\n",
      "memory usage: 9.2+ KB\n",
      "None\n",
      "\n",
      "4_challenges_df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54 entries, 0 to 53\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           54 non-null     object \n",
      " 1   title          54 non-null     object \n",
      " 2   doi            54 non-null     object \n",
      " 3   authors        54 non-null     object \n",
      " 4   journal        54 non-null     object \n",
      " 5   short_journal  51 non-null     object \n",
      " 6   volume         54 non-null     int64  \n",
      " 7   year           54 non-null     int64  \n",
      " 8   publisher      54 non-null     object \n",
      " 9   issue          36 non-null     float64\n",
      " 10  page           37 non-null     object \n",
      " 11  abstract       54 non-null     object \n",
      "dtypes: float64(1), int64(2), object(9)\n",
      "memory usage: 5.2+ KB\n",
      "None\n",
      "\n",
      "5_decision_df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110 entries, 0 to 109\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           110 non-null    object \n",
      " 1   title          110 non-null    object \n",
      " 2   doi            110 non-null    object \n",
      " 3   authors        110 non-null    object \n",
      " 4   journal        110 non-null    object \n",
      " 5   short_journal  105 non-null    object \n",
      " 6   volume         109 non-null    float64\n",
      " 7   year           110 non-null    int64  \n",
      " 8   publisher      110 non-null    object \n",
      " 9   issue          78 non-null     float64\n",
      " 10  page           75 non-null     object \n",
      " 11  abstract       108 non-null    object \n",
      "dtypes: float64(2), int64(1), object(9)\n",
      "memory usage: 10.4+ KB\n",
      "None\n",
      "\n",
      "6_agents_df info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 169 entries, 0 to 168\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           169 non-null    object \n",
      " 1   title          169 non-null    object \n",
      " 2   doi            169 non-null    object \n",
      " 3   authors        169 non-null    object \n",
      " 4   journal        169 non-null    object \n",
      " 5   short_journal  155 non-null    object \n",
      " 6   volume         167 non-null    float64\n",
      " 7   year           169 non-null    int64  \n",
      " 8   publisher      169 non-null    object \n",
      " 9   issue          105 non-null    float64\n",
      " 10  page           117 non-null    object \n",
      " 11  abstract       167 non-null    object \n",
      "dtypes: float64(2), int64(1), object(9)\n",
      "memory usage: 16.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Use absolute path or proper relative path\n",
    "# Option 1: Absolute path\n",
    "references_dir = '/workspaces/tsi-sota-ai/data/references'\n",
    "\n",
    "# Option 2: Relative path (going up one directory from notebooks)\n",
    "# references_dir = os.path.join('..', 'data', 'references')\n",
    "\n",
    "print(f\"Looking for files in: {references_dir}\")\n",
    "\n",
    "# Dictionary with exact filenames and their corresponding DataFrame names\n",
    "file_mapping = {\n",
    "    '1.2.2.1 LR - The Specialist Shortage and its Impact.csv': '1_specialists_df',\n",
    "    '1.2.2.2 LR - AI Applications in SCM Decision Support.csv': '2_aiscm_df',\n",
    "    '1.2.2.3 LR - Human-AI Collaboration in SCM.csv': '3_humanai_df',\n",
    "    '1.2.2.4 LR - Challenges and Limitations of LLMs in SCM.csv': '4_challenges_df',\n",
    "    '1.2.2.5 LR - Decision-Making Processes.csv': '5_decision_df',\n",
    "    '1.2.2.6 LR - Agents.csv': '6_agents_df'\n",
    "}\n",
    "\n",
    "# Initialize dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Read each CSV file\n",
    "for filename, df_name in file_mapping.items():\n",
    "    file_path = os.path.join(references_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes[df_name] = df\n",
    "        print(f\"Loaded {filename} into {df_name} with shape {df.shape}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Assign DataFrames to individual variables\n",
    "locals().update(dataframes)\n",
    "\n",
    "# Print basic info about each DataFrame\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{name} info:\")\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine DataFrames\n",
    "\n",
    "Combine all individual DataFrames into one for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined references data shape: (593, 12)\n"
     ]
    }
   ],
   "source": [
    "# Combine all DataFrames into one\n",
    "references_df = pd.concat(dataframes.values(), ignore_index=True)\n",
    "print(f\"Combined references data shape: {references_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "Below are the first few rows of the combined references DataFrame to inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>short_journal</th>\n",
       "      <th>volume</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>issue</th>\n",
       "      <th>page</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-13</td>\n",
       "      <td>Transformative Procurement Trends: Integrating...</td>\n",
       "      <td>10.3390/logistics7030063</td>\n",
       "      <td>[{'author_name': 'Areej Althabatah', 'author_s...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63</td>\n",
       "      <td>Background: the advent of Industry 4.0 (I4.0) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-07</td>\n",
       "      <td>Exploring Progress with Supply Chain Risk Mana...</td>\n",
       "      <td>10.3390/logistics5040070</td>\n",
       "      <td>[{'author_name': 'Remko van Hoek', 'author_slu...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70</td>\n",
       "      <td>Background: In response to calls for actionabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Exploring Applications and Practical Examples ...</td>\n",
       "      <td>10.3390/logistics7040091</td>\n",
       "      <td>[{'author_name': 'João Reis', 'author_slug': '...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>91</td>\n",
       "      <td>Background: Material Requirements Planning (MR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>Sustainable Innovations in the Food Industry t...</td>\n",
       "      <td>10.3390/logistics5040066</td>\n",
       "      <td>[{'author_name': 'Saurabh Sharma', 'author_slu...</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>MDPI AG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66</td>\n",
       "      <td>The agri-food sector is an endless source of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>Artificial Intelligence (AI): Multidisciplinar...</td>\n",
       "      <td>10.1016/j.ijinfomgt.2019.08.002</td>\n",
       "      <td>[{'author_name': 'Yogesh K. Dwivedi', 'author_...</td>\n",
       "      <td>International Journal of Information Management</td>\n",
       "      <td>International Journal of Information Management</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Elsevier BV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101994</td>\n",
       "      <td>As far back as the industrial revolution, sign...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2023-09-13  Transformative Procurement Trends: Integrating...   \n",
       "1  2021-10-07  Exploring Progress with Supply Chain Risk Mana...   \n",
       "2  2023-12-01  Exploring Applications and Practical Examples ...   \n",
       "3  2021-09-27  Sustainable Innovations in the Food Industry t...   \n",
       "4  2021-04-01  Artificial Intelligence (AI): Multidisciplinar...   \n",
       "\n",
       "                               doi  \\\n",
       "0         10.3390/logistics7030063   \n",
       "1         10.3390/logistics5040070   \n",
       "2         10.3390/logistics7040091   \n",
       "3         10.3390/logistics5040066   \n",
       "4  10.1016/j.ijinfomgt.2019.08.002   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'author_name': 'Areej Althabatah', 'author_s...   \n",
       "1  [{'author_name': 'Remko van Hoek', 'author_slu...   \n",
       "2  [{'author_name': 'João Reis', 'author_slug': '...   \n",
       "3  [{'author_name': 'Saurabh Sharma', 'author_slu...   \n",
       "4  [{'author_name': 'Yogesh K. Dwivedi', 'author_...   \n",
       "\n",
       "                                           journal  \\\n",
       "0                                        Logistics   \n",
       "1                                        Logistics   \n",
       "2                                        Logistics   \n",
       "3                                        Logistics   \n",
       "4  International Journal of Information Management   \n",
       "\n",
       "                                     short_journal  volume  year    publisher  \\\n",
       "0                                        Logistics     7.0  2023      MDPI AG   \n",
       "1                                        Logistics     5.0  2021      MDPI AG   \n",
       "2                                        Logistics     7.0  2023      MDPI AG   \n",
       "3                                        Logistics     5.0  2021      MDPI AG   \n",
       "4  International Journal of Information Management    57.0  2021  Elsevier BV   \n",
       "\n",
       "   issue    page                                           abstract  \n",
       "0    3.0      63  Background: the advent of Industry 4.0 (I4.0) ...  \n",
       "1    4.0      70  Background: In response to calls for actionabl...  \n",
       "2    4.0      91  Background: Material Requirements Planning (MR...  \n",
       "3    4.0      66  The agri-food sector is an endless source of e...  \n",
       "4    NaN  101994  As far back as the industrial revolution, sign...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles: 593\n",
      "\n",
      "Publication Year Distribution:\n",
      "year\n",
      "2008      1\n",
      "2010      3\n",
      "2011      2\n",
      "2012      1\n",
      "2013      4\n",
      "2014      5\n",
      "2015      2\n",
      "2016      3\n",
      "2017      6\n",
      "2018     24\n",
      "2019     22\n",
      "2020     58\n",
      "2021    113\n",
      "2022    121\n",
      "2023    182\n",
      "2024     46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique journals (23):\n",
      "['Logistics' 'International Journal of Information Management'\n",
      " 'Transport and Telecommunication Journal'\n",
      " 'International Journal of Information Systems and Project Management'\n",
      " 'Applied System Innovation' 'Sustainable Operations and Computers'\n",
      " 'Smart Cities' 'Management Science' 'Big Data and Cognitive Computing'\n",
      " 'Iet Collaborative Intelligent Manufacturing'\n",
      " 'Frontiers in Artificial Intelligence' 'Science'\n",
      " 'Frontiers in Robotics and Ai' 'Journal of Big Data'\n",
      " 'Machine Learning and Knowledge Extraction'\n",
      " 'Journal of Artificial Intelligence Research'\n",
      " 'Nature Machine Intelligence'\n",
      " 'Transportation Research Interdisciplinary Perspectives'\n",
      " 'Transactions of the Association for Computational Linguistics'\n",
      " 'Journal of Artificial Intelligence and Soft Computing Research'\n",
      " 'Network Neuroscience' 'Neuromorphic Computing and Engineering'\n",
      " 'Caai Transactions on Intelligence Technology']\n",
      "\n",
      "Publisher Distribution (top 10):\n",
      "publisher\n",
      "MDPI AG                                                                    278\n",
      "Frontiers Media SA                                                         154\n",
      "Institute for Operations Research and the Management Sciences (INFORMS)     32\n",
      "Springer Science and Business Media LLC                                     27\n",
      "Walter de Gruyter GmbH                                                      25\n",
      "University of Minho                                                         24\n",
      "AI Access Foundation                                                        22\n",
      "Institution of Engineering and Technology (IET)                             17\n",
      "Elsevier BV                                                                  7\n",
      "MIT Press                                                                    4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total number of articles\n",
    "total_articles = references_df.shape[0]\n",
    "print(f\"Total number of articles: {total_articles}\")\n",
    "\n",
    "# Distribution by publication year (assuming 'year' column exists)\n",
    "if 'year' in references_df.columns:\n",
    "    year_distribution = references_df['year'].value_counts().sort_index()\n",
    "    print(\"\\nPublication Year Distribution:\")\n",
    "    print(year_distribution)\n",
    "else:\n",
    "    print(\"The column 'year' is not found in the data.\")\n",
    "\n",
    "# List of unique journals (assuming 'journal' column exists)\n",
    "if 'journal' in references_df.columns:\n",
    "    unique_journals = references_df['journal'].unique()\n",
    "    print(f\"\\nUnique journals ({len(unique_journals)}):\")\n",
    "    print(unique_journals)\n",
    "else:\n",
    "    print(\"The column 'journal' is not found in the data.\")\n",
    "\n",
    "# Publisher distribution if 'publisher' column exists\n",
    "if 'publisher' in references_df.columns:\n",
    "    publisher_distribution = references_df['publisher'].value_counts()\n",
    "    print(\"\\nPublisher Distribution (top 10):\")\n",
    "    print(publisher_distribution.head(10))\n",
    "else:\n",
    "    print(\"The column 'publisher' is not found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOI Analysis and Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with missing DOIs: 0\n",
      "\n",
      "Sample of DOI patterns:\n",
      "doi\n",
      "10.3390/logistics7030063      15\n",
      "10.3390/logistics6030048      11\n",
      "10.1186/s40537-020-00329-2     9\n",
      "10.3390/logistics7010001       7\n",
      "10.3389/frai.2023.1264372      7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing DOIs\n",
    "missing_dois = references_df['doi'].isna().sum()\n",
    "print(f\"Number of entries with missing DOIs: {missing_dois}\")\n",
    "\n",
    "# Check DOI patterns\n",
    "if not missing_dois == len(references_df):\n",
    "    print(\"\\nSample of DOI patterns:\")\n",
    "    print(references_df['doi'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Analysis\n",
    "\n",
    "Analyzing abstracts can help us understand the content distribution and identify potential data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract Statistics:\n",
      "Mean length: 1437.88 characters\n",
      "Median length: 1396 characters\n",
      "Shortest abstract: 0 characters\n",
      "Longest abstract: 3071 characters\n",
      "\n",
      "Number of entries with missing abstracts: 6\n"
     ]
    }
   ],
   "source": [
    "if 'abstract' in references_df.columns:\n",
    "    # Calculate abstract lengths\n",
    "    references_df['abstract_length'] = references_df['abstract'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"Abstract Statistics:\")\n",
    "    print(f\"Mean length: {references_df['abstract_length'].mean():.2f} characters\")\n",
    "    print(f\"Median length: {references_df['abstract_length'].median():.0f} characters\")\n",
    "    print(f\"Shortest abstract: {references_df['abstract_length'].min()} characters\")\n",
    "    print(f\"Longest abstract: {references_df['abstract_length'].max()} characters\")\n",
    "    \n",
    "    # Check for missing abstracts\n",
    "    missing_abstracts = references_df['abstract'].isna().sum()\n",
    "    print(f\"\\nNumber of entries with missing abstracts: {missing_abstracts}\")\n",
    "else:\n",
    "    print(\"The column 'abstract' is not found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Save the processed DataFrame for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data directory: /workspaces/tsi-sota-ai/data\n",
      "Saved processed data to: /workspaces/tsi-sota-ai/data/references_analysis.json\n",
      "Saved statistics to: /workspaces/tsi-sota-ai/data/references_stats.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create proper paths relative to project root\n",
    "# Option 1: Using absolute path\n",
    "data_dir = '/workspaces/tsi-sota-ai/data'\n",
    "\n",
    "# Option 2: Using relative path\n",
    "# data_dir = os.path.join('..', 'data')  # Go up one level from notebooks/\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(f\"Using data directory: {data_dir}\")\n",
    "\n",
    "# Save to JSON using the correct path\n",
    "output_json = os.path.join(data_dir, 'references_analysis.json')\n",
    "references_df.to_json(output_json, orient='records', indent=2)\n",
    "print(f\"Saved processed data to: {output_json}\")\n",
    "\n",
    "# Save basic statistics to a separate file\n",
    "stats_dict = {\n",
    "    'total_articles': int(total_articles),\n",
    "    'unique_journals': int(len(references_df['journal'].unique())) if 'journal' in references_df.columns else 0,\n",
    "    'year_range': f\"{references_df['year'].min()}-{references_df['year'].max()}\" if 'year' in references_df.columns else 'N/A',\n",
    "    'missing_dois': int(missing_dois) if 'doi' in references_df.columns else 'N/A',\n",
    "    'missing_abstracts': int(missing_abstracts) if 'abstract' in references_df.columns else 'N/A'\n",
    "}\n",
    "\n",
    "stats_json = os.path.join(data_dir, 'references_stats.json')\n",
    "with open(stats_json, 'w') as f:\n",
    "    json.dump(stats_dict, f, indent=2)\n",
    "print(f\"Saved statistics to: {stats_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded analysis to B2: processed_data/references_analysis_2025-02-27_211112.json\n",
      "Uploaded statistics to B2: processed_data/references_stats_2025-02-27_211112.json\n",
      "\n",
      "Files successfully uploaded to B2 bucket with timestamps\n"
     ]
    }
   ],
   "source": [
    "# Upload JSON files to B2 bucket with timestamps\n",
    "import datetime\n",
    "\n",
    "# Generate timestamp in format YYYY-MM-DD_HHMMSS\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "# Construct filenames with timestamps\n",
    "timestamped_analysis_filename = f\"references_analysis_{timestamp}.json\"\n",
    "timestamped_stats_filename = f\"references_stats_{timestamp}.json\"\n",
    "\n",
    "# Define the upload directory in B2\n",
    "b2_upload_dir = \"processed_data/\"\n",
    "\n",
    "try:\n",
    "    # Upload analysis JSON\n",
    "    with open(output_json, 'rb') as f:\n",
    "        analysis_data = f.read()\n",
    "        bucket.upload_bytes(\n",
    "            analysis_data,\n",
    "            f\"{b2_upload_dir}{timestamped_analysis_filename}\",\n",
    "            content_type=\"application/json\"\n",
    "        )\n",
    "    print(f\"Uploaded analysis to B2: {b2_upload_dir}{timestamped_analysis_filename}\")\n",
    "    \n",
    "    # Upload stats JSON\n",
    "    with open(stats_json, 'rb') as f:\n",
    "        stats_data = f.read()\n",
    "        bucket.upload_bytes(\n",
    "            stats_data,\n",
    "            f\"{b2_upload_dir}{timestamped_stats_filename}\",\n",
    "            content_type=\"application/json\"\n",
    "        )\n",
    "    print(f\"Uploaded statistics to B2: {b2_upload_dir}{timestamped_stats_filename}\")\n",
    "    \n",
    "    print(\"\\nFiles successfully uploaded to B2 bucket with timestamps\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to B2: {e}\")\n",
    "    print(\"Check that the B2 bucket is properly authenticated and accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in 'processed_data/' folder in B2 bucket:\n",
      "--------------------------------------------------------------------------------\n",
      "Type of processed_files: <class 'list'>\n",
      "Found 3 files:\n",
      "File Name                                          |       Size | Upload Date         \n",
      "-------------------------------------------------------------------------------------\n",
      "Type of item in processed_files: <class 'tuple'>\n",
      "Type of file_info: <class 'b2sdk.v2.file_version.FileVersion'>\n",
      "processed_data/.bzEmpty                             | 1740690657219\n",
      "Type of item in processed_files: <class 'tuple'>\n",
      "Type of file_info: <class 'b2sdk.v2.file_version.FileVersion'>\n",
      "processed_data/references_analysis_2025-02-27_211112.json  | 1740690672786\n",
      "Type of item in processed_files: <class 'tuple'>\n",
      "Type of file_info: <class 'b2sdk.v2.file_version.FileVersion'>\n",
      "processed_data/references_stats_2025-02-27_211112.json  | 1740690673355\n"
     ]
    }
   ],
   "source": [
    "# Check B2 bucket to confirm files were uploaded\n",
    "print(\"Listing files in 'processed_data/' folder in B2 bucket:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# List files with the processed_data prefix\n",
    "processed_files_iterator = bucket.ls(\"processed_data/\") # Get the iterator\n",
    "processed_files = list(processed_files_iterator) # Convert to list to inspect\n",
    "\n",
    "print(f\"Type of processed_files: {type(processed_files)}\") # ADD THIS LINE\n",
    "\n",
    "if not processed_files:\n",
    "    print(\"No files found in processed_data/ folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(processed_files)} files:\")\n",
    "\n",
    "    # Create a list to check if our recent uploads are found\n",
    "    recent_uploads = [\n",
    "        f\"processed_data/{timestamped_analysis_filename}\",\n",
    "        f\"processed_data/{timestamped_stats_filename}\"\n",
    "    ]\n",
    "\n",
    "    found_uploads = []\n",
    "\n",
    "    # Display file info in a table format\n",
    "    print(f\"{'File Name':<50} | {'Size':>10} | {'Upload Date':<20}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for item in processed_files: # Changed loop variable name to 'item'\n",
    "        print(f\"Type of item in processed_files: {type(item)}\") # ADD THIS LINE\n",
    "        file_info, _ = item # Keep tuple unpacking\n",
    "        print(f\"Type of file_info: {type(file_info)}\") # ADD THIS LINE\n",
    "        print(f\"{file_info.file_name:<50}  | {file_info.upload_timestamp}\") # Keep this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in 'processed_data/' folder in B2 bucket:\n",
      "--------------------------------------------------------------------------------\n",
      "Found 3 files:\n",
      "File Name                                                    |       Size | Upload Date         \n",
      "-----------------------------------------------------------------------------------------------\n",
      "processed_data/.bzEmpty                                      |      0.0 KB | 2025-02-27 21:10:57\n",
      "processed_data/references_analysis_2025-02-27_211112.json    |   1350.8 KB | 2025-02-27 21:11:12\n",
      "processed_data/references_stats_2025-02-27_211112.json       |     0.12 KB | 2025-02-27 21:11:13\n",
      "\n",
      "Recently uploaded files verification:\n",
      "✅ processed_data/references_analysis_2025-02-27_211112.json successfully found\n",
      "✅ processed_data/references_stats_2025-02-27_211112.json successfully found\n"
     ]
    }
   ],
   "source": [
    "# Check B2 bucket to confirm files were uploaded\n",
    "import datetime\n",
    "\n",
    "print(\"Listing files in 'processed_data/' folder in B2 bucket:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# List files with the processed_data prefix\n",
    "processed_files = list(bucket.ls(\"processed_data/\"))\n",
    "\n",
    "if not processed_files:\n",
    "    print(\"No files found in processed_data/ folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(processed_files)} files:\")\n",
    "\n",
    "    # Create a list to check if our recent uploads are found\n",
    "    recent_uploads = [\n",
    "        f\"processed_data/{timestamped_analysis_filename}\",\n",
    "        f\"processed_data/{timestamped_stats_filename}\"\n",
    "    ]\n",
    "\n",
    "    found_uploads = []\n",
    "\n",
    "    # Display file info in a table format\n",
    "    print(f\"{'File Name':<60} | {'Size':>10} | {'Upload Date':<20}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    for file_info, _ in processed_files:\n",
    "        # Format the timestamp as readable date\n",
    "        upload_date = datetime.datetime.fromtimestamp(\n",
    "            file_info.upload_timestamp / 1000\n",
    "        ).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Get size in KB\n",
    "        size_kb = round(file_info.size / 1024, 2)\n",
    "        \n",
    "        print(f\"{file_info.file_name:<60} | {size_kb:>8} KB | {upload_date}\")\n",
    "        \n",
    "        # Check if this file matches our recent uploads\n",
    "        if file_info.file_name in recent_uploads:\n",
    "            found_uploads.append(file_info.file_name)\n",
    "\n",
    "    # Show confirmation of recent uploads\n",
    "    print(\"\\nRecently uploaded files verification:\")\n",
    "    for expected_file in recent_uploads:\n",
    "        if expected_file in found_uploads:\n",
    "            print(f\"✅ {expected_file} successfully found\")\n",
    "        else:\n",
    "            print(f\"❌ {expected_file} NOT FOUND\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
