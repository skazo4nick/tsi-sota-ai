{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comprehensive Publication Analysis\n",
                "## OpenAlex Data: Supply Chain, Agency & Temporal Keyword Evolution\n",
                "\n",
                "**Complete Workflow:**\n",
                "1. Load and explore OpenAlex data structure\n",
                "2. Filter for supply chain relevance\n",
                "3. Identify agency-related articles\n",
                "4. **Temporal Analysis**: Track 'agent' term evolution and AI/LLM connections\n",
                "5. Generate insights and visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import json\n",
                "from pathlib import Path\n",
                "from collections import Counter, defaultdict\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "import plotly.express as px\n",
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from wordcloud import WordCloud\n",
                "\n",
                "# Setup\n",
                "plt.style.use('default')\n",
                "plt.rcParams['figure.figsize'] = (12, 8)\n",
                "data_dir = Path('../data')\n",
                "\n",
                "print(\"=== ENVIRONMENT SETUP ===\")\n",
                "print(f\"Working directory: {Path.cwd()}\")\n",
                "print(f\"Data directory: {data_dir}\")\n",
                "print(f\"Available data files:\")\n",
                "for file in data_dir.glob('*'):\n",
                "    if file.is_file():\n",
                "        size_mb = file.stat().st_size / 1024 / 1024\n",
                "        print(f\"  {file.name} ({size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Structure Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the specific OpenAlex dataset: agent_scm_30year_yearly.csv\n",
                "print(\"=== LOADING AGENT SCM 30-YEAR DATASET ===\")\n",
                "\n",
                "# Load the main dataset\n",
                "target_file = data_dir / 'agent_scm_30year_yearly.csv'\n",
                "print(f\"Loading: {target_file.name} ({target_file.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
                "\n",
                "try:\n",
                "    # Load with low_memory=False to handle mixed types\n",
                "    df = pd.read_csv(target_file, low_memory=False)\n",
                "    print(f\"✅ Successfully loaded {len(df):,} records\")\n",
                "    print(f\"Shape: {df.shape}\")\n",
                "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error loading {target_file.name}: {e}\")\n",
                "    df = None\n",
                "\n",
                "if df is not None:\n",
                "    print(f\"\\n=== DATASET OVERVIEW ===\")\n",
                "    print(f\"Columns: {len(df.columns)}\")\n",
                "    print(f\"Records: {len(df):,}\")\n",
                "    \n",
                "    # Show basic info\n",
                "    print(f\"\\nFirst 5 column names:\")\n",
                "    for i, col in enumerate(df.columns[:5]):\n",
                "        print(f\"  {i+1}. {col}\")\n",
                "    \n",
                "    if len(df.columns) > 5:\n",
                "        print(f\"  ... and {len(df.columns) - 5} more columns\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4d91aa1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed column analysis\n",
                "if df is not None:\n",
                "    print(\"=== DETAILED COLUMN ANALYSIS ===\")\n",
                "    print(f\"Total columns: {len(df.columns)}\")\n",
                "    \n",
                "    # Analyze each column\n",
                "    column_info = []\n",
                "    for i, col in enumerate(df.columns):\n",
                "        dtype = str(df[col].dtype)\n",
                "        null_count = df[col].isnull().sum()\n",
                "        null_pct = (null_count / len(df)) * 100\n",
                "        unique_count = df[col].nunique()\n",
                "        \n",
                "        # Sample non-null values\n",
                "        sample_values = df[col].dropna().head(3).tolist()\n",
                "        sample_str = ', '.join([str(v)[:30] + ('...' if len(str(v)) > 30 else '') for v in sample_values])\n",
                "        \n",
                "        column_info.append({\n",
                "            'position': i + 1,\n",
                "            'name': col,\n",
                "            'dtype': dtype,\n",
                "            'null_count': null_count,\n",
                "            'null_pct': null_pct,\n",
                "            'unique_count': unique_count,\n",
                "            'sample_values': sample_str\n",
                "        })\n",
                "        \n",
                "        print(f\"  {i+1:2d}. {col:30} | {dtype:12} | {null_pct:5.1f}% null | {unique_count:8,} unique\")\n",
                "    \n",
                "    # Store for later use\n",
                "    globals()['column_info'] = column_info\n",
                "    \n",
                "    print(f\"\\nSample values from first few columns:\")\n",
                "    for col_info in column_info[:5]:\n",
                "        print(f\"  {col_info['name']:30}: {col_info['sample_values']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15aea35d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Map columns to OpenAlex standard fields\n",
                "if df is not None:\n",
                "    print(\"\\n=== OPENALEX FIELD MAPPING ===\")\n",
                "    \n",
                "    # Common OpenAlex field patterns\n",
                "    field_patterns = {\n",
                "        'id': ['id', 'openalex_id', 'work_id'],\n",
                "        'title': ['title', 'display_name', 'work_title'],\n",
                "        'abstract': ['abstract', 'abstract_inverted_index'],\n",
                "        'authors': ['authors', 'authorships', 'author_names'],\n",
                "        'year': ['publication_year', 'year', 'publication_date', 'published_date'],\n",
                "        'venue': ['primary_location', 'host_venue', 'journal', 'source'],\n",
                "        'keywords': ['keywords', 'concepts', 'topics', 'mesh_terms'],\n",
                "        'citations': ['cited_by_count', 'citation_count', 'citations'],\n",
                "        'doi': ['doi', 'ids'],\n",
                "        'type': ['type', 'work_type', 'publication_type'],\n",
                "        'open_access': ['open_access', 'is_oa'],\n",
                "        'language': ['language'],\n",
                "        'institutions': ['institutions', 'affiliations']\n",
                "    }\n",
                "    \n",
                "    identified_fields = {}\n",
                "    \n",
                "    for field_type, patterns in field_patterns.items():\n",
                "        matches = []\n",
                "        for col in df.columns:\n",
                "            col_lower = col.lower()\n",
                "            for pattern in patterns:\n",
                "                if pattern.lower() in col_lower:\n",
                "                    matches.append(col)\n",
                "                    break\n",
                "        \n",
                "        if matches:\n",
                "            # Prefer exact matches, then shortest name\n",
                "            best_match = min(matches, key=lambda x: (len(x), x))\n",
                "            identified_fields[field_type] = best_match\n",
                "            print(f\"  {field_type:15}: ✅ {best_match}\")\n",
                "        else:\n",
                "            print(f\"  {field_type:15}: ❌ NOT FOUND\")\n",
                "    \n",
                "    # Store field mapping globally\n",
                "    globals()['field_map'] = identified_fields\n",
                "    \n",
                "    print(f\"\\nMapped {len(identified_fields)} standard fields from {len(df.columns)} total columns\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "87c5872f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample data exploration\n",
                "if df is not None and 'field_map' in globals():\n",
                "    print(\"\\n=== SAMPLE DATA EXPLORATION ===\")\n",
                "    \n",
                "    # Show sample records\n",
                "    print(\"Sample records (first 3 rows):\")\n",
                "    display_cols = []\n",
                "    \n",
                "    # Priority display columns\n",
                "    priority_fields = ['title', 'year', 'authors', 'citations']\n",
                "    for field in priority_fields:\n",
                "        if field in field_map:\n",
                "            display_cols.append(field_map[field])\n",
                "    \n",
                "    # Add a few more interesting columns\n",
                "    remaining_cols = [col for col in df.columns if col not in display_cols][:3]\n",
                "    display_cols.extend(remaining_cols)\n",
                "    \n",
                "    if display_cols:\n",
                "        sample_df = df[display_cols].head(3).copy()\n",
                "        \n",
                "        # Truncate long text for display\n",
                "        for col in sample_df.columns:\n",
                "            if sample_df[col].dtype == 'object':\n",
                "                sample_df[col] = sample_df[col].astype(str).apply(\n",
                "                    lambda x: x[:60] + '...' if len(x) > 60 else x\n",
                "                )\n",
                "        \n",
                "        print(sample_df.to_string(index=False))\n",
                "    \n",
                "    # Basic statistics\n",
                "    print(f\"\\n=== BASIC STATISTICS ===\")\n",
                "    \n",
                "    # Year analysis\n",
                "    if 'year' in field_map:\n",
                "        year_col = field_map['year']\n",
                "        years = pd.to_numeric(df[year_col], errors='coerce').dropna()\n",
                "        if len(years) > 0:\n",
                "            print(f\"Publication years: {int(years.min())} - {int(years.max())}\")\n",
                "            print(f\"Total with valid years: {len(years):,} ({len(years)/len(df)*100:.1f}%)\")\n",
                "            \n",
                "            # Recent distribution\n",
                "            recent_years = years[years >= 2020]\n",
                "            if len(recent_years) > 0:\n",
                "                print(f\"Publications 2020+: {len(recent_years):,} ({len(recent_years)/len(years)*100:.1f}% of dated articles)\")\n",
                "    \n",
                "    # Citations analysis\n",
                "    if 'citations' in field_map:\n",
                "        cite_col = field_map['citations']\n",
                "        citations = pd.to_numeric(df[cite_col], errors='coerce').dropna()\n",
                "        if len(citations) > 0:\n",
                "            print(f\"Citations: mean={citations.mean():.1f}, median={citations.median():.0f}, max={citations.max():.0f}\")\n",
                "    \n",
                "    print(f\"Data quality: {df.isnull().sum().sum():,} total null values across all columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "482ffe27",
            "metadata": {},
            "source": [
                "## 2. Comprehensive Keyword Taxonomy Definition\n",
                "\n",
                "Defining our keyword categories for multi-dimensional analysis:\n",
                "- **Supply Chain**: Core SCM, operations, and technology terms\n",
                "- **Agency**: Theory, modeling, and governance concepts\n",
                "- **AI/LLM**: General AI, specific LLM terms, and applications\n",
                "- **Agent Terms**: General and compound agent terminology"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f21b15b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive keyword taxonomy for analysis\n",
                "KEYWORD_TAXONOMY = {\n",
                "    'supply_chain': {\n",
                "        'core': [\n",
                "            'supply chain', 'supply-chain', 'supply chains',\n",
                "            'logistics', 'procurement', 'sourcing',\n",
                "            'inventory management', 'distribution', 'warehousing',\n",
                "            'supplier', 'vendor management', 'supply network',\n",
                "            'supply chain management', 'SCM'\n",
                "        ],\n",
                "        'operations': [\n",
                "            'operations management', 'production planning', 'demand forecasting',\n",
                "            'capacity planning', 'lean manufacturing', 'just-in-time',\n",
                "            'value chain', 'operations research', 'supply planning'\n",
                "        ],\n",
                "        'technology': [\n",
                "            'supply chain technology', 'supply chain digitalization',\n",
                "            'supply chain analytics', 'blockchain supply chain',\n",
                "            'IoT supply chain', 'AI supply chain', 'digital supply chain',\n",
                "            'ERP', 'WMS', 'TMS', 'supply chain automation'\n",
                "        ]\n",
                "    },\n",
                "    'agency': {\n",
                "        'theory': [\n",
                "            'agency theory', 'principal-agent', 'principal agent',\n",
                "            'moral hazard', 'adverse selection', 'information asymmetry',\n",
                "            'agency costs', 'agency problems', 'agency relationship'\n",
                "        ],\n",
                "        'modeling': [\n",
                "            'agent-based modeling', 'agent based modeling', 'ABM',\n",
                "            'multi-agent system', 'multi agent system', 'MAS',\n",
                "            'autonomous agents', 'intelligent agents', 'software agents',\n",
                "            'agent-based simulation', 'agent based simulation'\n",
                "        ],\n",
                "        'governance': [\n",
                "            'governance mechanisms', 'monitoring', 'incentive alignment',\n",
                "            'contract theory', 'agency relationships', 'corporate governance'\n",
                "        ]\n",
                "    },\n",
                "    'ai_llm': {\n",
                "        'ai_general': [\n",
                "            'artificial intelligence', 'machine learning', 'deep learning',\n",
                "            'neural networks', 'AI', 'ML', 'DL',\n",
                "            'computer vision', 'natural language processing', 'NLP',\n",
                "            'reinforcement learning', 'supervised learning'\n",
                "        ],\n",
                "        'llm_specific': [\n",
                "            'large language model', 'large language models', 'LLM', 'LLMs',\n",
                "            'transformer', 'transformers', 'BERT', 'GPT',\n",
                "            'language model', 'foundation model', 'foundation models',\n",
                "            'generative AI', 'ChatGPT', 'OpenAI', 'pretrained model'\n",
                "        ],\n",
                "        'applications': [\n",
                "            'conversational AI', 'dialogue system', 'chatbot',\n",
                "            'text generation', 'language generation',\n",
                "            'prompt engineering', 'fine-tuning', 'AI assistant'\n",
                "        ]\n",
                "    },\n",
                "    'agent_terms': {\n",
                "        'general': [\n",
                "            'agent', 'agents', 'agented', 'agenting',\n",
                "            'agency', 'agencies'\n",
                "        ],\n",
                "        'compound': [\n",
                "            'AI agent', 'AI agents', 'artificial agent',\n",
                "            'digital agent', 'virtual agent', 'cognitive agent',\n",
                "            'conversational agent', 'autonomous agent', 'smart agent'\n",
                "        ]\n",
                "    }\n",
                "}\n",
                "\n",
                "# Create efficient regex patterns for matching\n",
                "def create_regex_pattern(keywords):\n",
                "    \"\"\"Create case-insensitive regex pattern with word boundaries\"\"\"\n",
                "    # Escape special regex characters and add word boundaries\n",
                "    escaped = []\n",
                "    for kw in keywords:\n",
                "        # Handle hyphenated terms and special cases\n",
                "        if '-' in kw:\n",
                "            # Allow both hyphenated and space versions\n",
                "            variants = [re.escape(kw), re.escape(kw.replace('-', ' '))]\n",
                "            escaped.extend(variants)\n",
                "        else:\n",
                "            escaped.append(re.escape(kw))\n",
                "    \n",
                "    # Create pattern with word boundaries\n",
                "    pattern = r'\\b(?:' + '|'.join(escaped) + r')\\b'\n",
                "    return pattern\n",
                "\n",
                "# Generate patterns for all categories\n",
                "PATTERNS = {}\n",
                "for category, subcategories in KEYWORD_TAXONOMY.items():\n",
                "    PATTERNS[category] = {}\n",
                "    all_keywords = []\n",
                "    for subcat, keywords in subcategories.items():\n",
                "        PATTERNS[category][subcat] = create_regex_pattern(keywords)\n",
                "        all_keywords.extend(keywords)\n",
                "    PATTERNS[category]['all'] = create_regex_pattern(all_keywords)\n",
                "\n",
                "print(\"=== KEYWORD TAXONOMY CREATED ===\")\n",
                "for category, subcats in KEYWORD_TAXONOMY.items():\n",
                "    total_keywords = sum(len(keywords) for keywords in subcats.values())\n",
                "    print(f\"{category:15}: {total_keywords:3d} keywords across {len(subcats)} subcategories\")\n",
                "\n",
                "print(f\"\\nRegex patterns generated for efficient text matching\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "36df98af",
            "metadata": {},
            "source": [
                "## 3. Efficient Text Analysis Functions\n",
                "\n",
                "Implementing fast keyword matching and publication categorization functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5cc28326",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Efficient text analysis functions\n",
                "def fast_keyword_match(text, pattern):\n",
                "    \"\"\"Fast regex-based keyword matching with case-insensitive search\"\"\"\n",
                "    if pd.isna(text) or text is None:\n",
                "        return False\n",
                "    try:\n",
                "        return bool(re.search(pattern, str(text), re.IGNORECASE))\n",
                "    except:\n",
                "        return False\n",
                "\n",
                "def extract_matching_terms(text, original_keywords):\n",
                "    \"\"\"Extract actual matching terms from text\"\"\"\n",
                "    if pd.isna(text) or text is None:\n",
                "        return []\n",
                "    \n",
                "    text_lower = str(text).lower()\n",
                "    matches = []\n",
                "    \n",
                "    for keyword in original_keywords:\n",
                "        # Use word boundaries for more precise matching\n",
                "        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n",
                "        if re.search(pattern, text_lower):\n",
                "            matches.append(keyword)\n",
                "    \n",
                "    return matches\n",
                "\n",
                "def analyze_publications_comprehensive(df, field_map):\n",
                "    \"\"\"Comprehensive publication analysis by keyword categories\"\"\"\n",
                "    \n",
                "    if df is None or len(df) == 0:\n",
                "        return None\n",
                "    \n",
                "    results = {\n",
                "        'total_publications': len(df),\n",
                "        'analysis_timestamp': datetime.now().isoformat(),\n",
                "        'by_category': {},\n",
                "        'temporal_data': {},\n",
                "        'matching_articles': {},\n",
                "        'field_coverage': {}\n",
                "    }\n",
                "    \n",
                "    # Determine text fields to search\n",
                "    search_fields = []\n",
                "    text_field_types = ['title', 'abstract', 'keywords']\n",
                "    \n",
                "    for field_type in text_field_types:\n",
                "        if field_type in field_map and field_map[field_type] in df.columns:\n",
                "            col_name = field_map[field_type]\n",
                "            # Check if field has meaningful content\n",
                "            non_null_count = df[col_name].notna().sum()\n",
                "            coverage = non_null_count / len(df) * 100\n",
                "            \n",
                "            search_fields.append((field_type, col_name))\n",
                "            results['field_coverage'][field_type] = {\n",
                "                'column': col_name,\n",
                "                'coverage_pct': coverage,\n",
                "                'non_null_count': non_null_count\n",
                "            }\n",
                "            print(f\"Will search {field_type:10}: {col_name:30} ({coverage:5.1f}% coverage)\")\n",
                "    \n",
                "    if not search_fields:\n",
                "        print(\"❌ No searchable text fields found!\")\n",
                "        return results\n",
                "    \n",
                "    # Year field for temporal analysis\n",
                "    year_field = field_map.get('year')\n",
                "    \n",
                "    print(f\"\\nAnalyzing {len(KEYWORD_TAXONOMY)} categories across {len(search_fields)} text fields...\")\n",
                "    \n",
                "    # Analyze each category\n",
                "    for category in KEYWORD_TAXONOMY.keys():\n",
                "        print(f\"\\n  Analyzing {category}...\")\n",
                "        \n",
                "        category_mask = pd.Series([False] * len(df))\n",
                "        subcategory_results = {}\n",
                "        subcategory_matches = {}\n",
                "        \n",
                "        # Check each subcategory\n",
                "        for subcat in KEYWORD_TAXONOMY[category].keys():\n",
                "            subcat_mask = pd.Series([False] * len(df))\n",
                "            subcat_matching_terms = []\n",
                "            \n",
                "            # Search in all specified fields\n",
                "            for field_name, col_name in search_fields:\n",
                "                if col_name in df.columns:\n",
                "                    try:\n",
                "                        field_matches = df[col_name].apply(\n",
                "                            lambda x: fast_keyword_match(x, PATTERNS[category][subcat])\n",
                "                        )\n",
                "                        subcat_mask |= field_matches\n",
                "                        \n",
                "                        # Track which terms were found (sample)\n",
                "                        sample_matches = df[field_matches][col_name].head(5).apply(\n",
                "                            lambda x: extract_matching_terms(x, KEYWORD_TAXONOMY[category][subcat])\n",
                "                        ).tolist()\n",
                "                        subcat_matching_terms.extend([term for terms in sample_matches for term in terms])\n",
                "                        \n",
                "                    except Exception as e:\n",
                "                        print(f\"    ⚠️  Error searching {field_name}: {e}\")\n",
                "            \n",
                "            subcategory_results[subcat] = {\n",
                "                'count': subcat_mask.sum(),\n",
                "                'percentage': (subcat_mask.sum() / len(df)) * 100,\n",
                "                'sample_terms': list(set(subcat_matching_terms[:10]))  # Top 10 unique terms\n",
                "            }\n",
                "            subcategory_matches[subcat] = subcat_mask\n",
                "            category_mask |= subcat_mask\n",
                "        \n",
                "        # Store category results\n",
                "        total_matches = category_mask.sum()\n",
                "        results['by_category'][category] = {\n",
                "            'total': total_matches,\n",
                "            'percentage': (total_matches / len(df)) * 100,\n",
                "            'subcategories': subcategory_results\n",
                "        }\n",
                "        \n",
                "        # Store matching article indices\n",
                "        results['matching_articles'][category] = df[category_mask].index.tolist()\n",
                "        \n",
                "        # Temporal analysis if year data available\n",
                "        if year_field and year_field in df.columns and total_matches > 0:\n",
                "            matching_df = df[category_mask]\n",
                "            years = pd.to_numeric(matching_df[year_field], errors='coerce').dropna()\n",
                "            \n",
                "            if len(years) > 0:\n",
                "                # Filter reasonable years (1990-2024)\n",
                "                valid_years = years[(years >= 1990) & (years <= 2024)]\n",
                "                if len(valid_years) > 0:\n",
                "                    year_counts = valid_years.value_counts().sort_index()\n",
                "                    results['temporal_data'][category] = {\n",
                "                        'year_counts': year_counts.to_dict(),\n",
                "                        'year_range': (int(valid_years.min()), int(valid_years.max())),\n",
                "                        'peak_year': int(year_counts.idxmax()) if len(year_counts) > 0 else None,\n",
                "                        'articles_with_years': len(valid_years),\n",
                "                        'coverage_pct': len(valid_years) / total_matches * 100\n",
                "                    }\n",
                "        \n",
                "        print(f\"    Found {total_matches:,} articles ({(total_matches/len(df)*100):.1f}%)\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "print(\"Text analysis functions ready for execution\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "80f099af",
            "metadata": {},
            "source": [
                "## 4. Execute Comprehensive Analysis\n",
                "\n",
                "Running the complete analysis on the agent SCM dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b0ed0d75",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execute comprehensive analysis\n",
                "if df is not None and 'field_map' in globals():\n",
                "    print(\"=== EXECUTING COMPREHENSIVE ANALYSIS ===\")\n",
                "    print(f\"Dataset: {len(df):,} publications\")\n",
                "    print(f\"Fields mapped: {list(field_map.keys())}\")\n",
                "    print(f\"Analysis categories: {list(KEYWORD_TAXONOMY.keys())}\")\n",
                "    \n",
                "    # Run the analysis\n",
                "    analysis_results = analyze_publications_comprehensive(df, field_map)\n",
                "    \n",
                "    if analysis_results:\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"ANALYSIS SUMMARY\")\n",
                "        print(\"=\"*60)\n",
                "        \n",
                "        total_pubs = analysis_results['total_publications']\n",
                "        print(f\"Total Publications: {total_pubs:,}\\n\")\n",
                "        \n",
                "        # Category overview\n",
                "        for category, data in analysis_results['by_category'].items():\n",
                "            print(f\"{category.upper().replace('_', ' '):20}: {data['total']:6,} articles ({data['percentage']:5.1f}%)\")\n",
                "            \n",
                "            # Show top subcategories\n",
                "            sorted_subcats = sorted(data['subcategories'].items(), \n",
                "                                  key=lambda x: x[1]['count'], reverse=True)\n",
                "            \n",
                "            for subcat, subdata in sorted_subcats[:2]:  # Top 2 subcategories\n",
                "                print(f\"  └─ {subcat:15}: {subdata['count']:6,} articles ({subdata['percentage']:5.1f}%)\")\n",
                "                if subdata['sample_terms']:\n",
                "                    terms_str = ', '.join(subdata['sample_terms'][:3])\n",
                "                    print(f\"     Sample terms: {terms_str}\")\n",
                "            print()\n",
                "        \n",
                "        # Field coverage summary\n",
                "        print(\"FIELD COVERAGE:\")\n",
                "        for field, coverage_data in analysis_results['field_coverage'].items():\n",
                "            print(f\"  {field:10}: {coverage_data['coverage_pct']:5.1f}% ({coverage_data['non_null_count']:,} records)\")\n",
                "        \n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "else:\n",
                "    print(\"❌ Cannot run analysis - data or field mapping not available\")\n",
                "    analysis_results = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze data structure and identify OpenAlex fields\n",
                "if df is not None:\n",
                "    print(\"=== COLUMN ANALYSIS ===\")\n",
                "    print(f\"Total columns: {len(df.columns)}\")\n",
                "    \n",
                "    # Map common OpenAlex field patterns\n",
                "    field_mapping = {\n",
                "        'title': ['title', 'display_name', 'work_title'],\n",
                "        'abstract': ['abstract', 'abstract_inverted_index'],\n",
                "        'keywords': ['keywords', 'concepts', 'topics', 'mesh_terms'],\n",
                "        'authors': ['authors', 'authorships', 'author_names'],\n",
                "        'year': ['publication_year', 'year', 'publication_date', 'published_date'],\n",
                "        'venue': ['primary_location', 'host_venue', 'journal', 'source'],\n",
                "        'doi': ['doi', 'ids'],\n",
                "        'citations': ['cited_by_count', 'citation_count', 'citations'],\n",
                "        'type': ['type', 'work_type', 'publication_type']\n",
                "    }\n",
                "    \n",
                "    identified_fields = {}\n",
                "    \n",
                "    print(\"\\nIdentified fields:\")\n",
                "    for field_type, possible_names in field_mapping.items():\n",
                "        matches = []\n",
                "        for col in df.columns:\n",
                "            if any(name.lower() in col.lower() for name in possible_names):\n",
                "                matches.append(col)\n",
                "        \n",
                "        if matches:\n",
                "            # Prefer exact matches, then shortest name\n",
                "            best_match = min(matches, key=len)\n",
                "            identified_fields[field_type] = best_match\n",
                "            print(f\"  {field_type:12}: {best_match}\")\n",
                "        else:\n",
                "            print(f\"  {field_type:12}: ❌ NOT FOUND\")\n",
                "    \n",
                "    print(f\"\\nAll columns:\")\n",
                "    for i, col in enumerate(df.columns, 1):\n",
                "        dtype = str(df[col].dtype)\n",
                "        null_pct = (df[col].isnull().sum() / len(df)) * 100\n",
                "        print(f\"  {i:2d}. {col:30} | {dtype:10} | {null_pct:.1f}% null\")\n",
                "        \n",
                "    # Store field mapping for later use\n",
                "    globals()['field_map'] = identified_fields"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Supply Chain & Agency Keyword Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive keyword definitions for multi-level analysis\n",
                "KEYWORD_TAXONOMY = {\n",
                "    'supply_chain': {\n",
                "        'core': [\n",
                "            'supply chain', 'supply-chain', 'supply chains',\n",
                "            'logistics', 'procurement', 'sourcing',\n",
                "            'inventory management', 'distribution', 'warehousing',\n",
                "            'supplier', 'vendor management', 'supply network'\n",
                "        ],\n",
                "        'operations': [\n",
                "            'operations management', 'production planning', 'demand forecasting',\n",
                "            'capacity planning', 'lean manufacturing', 'just-in-time',\n",
                "            'value chain', 'supply chain management', 'SCM'\n",
                "        ],\n",
                "        'technology': [\n",
                "            'supply chain technology', 'supply chain digitalization',\n",
                "            'supply chain analytics', 'blockchain supply chain',\n",
                "            'IoT supply chain', 'AI supply chain',\n",
                "            'ERP', 'WMS', 'TMS'\n",
                "        ]\n",
                "    },\n",
                "    'agency': {\n",
                "        'theory': [\n",
                "            'agency theory', 'principal-agent', 'principal agent',\n",
                "            'moral hazard', 'adverse selection', 'information asymmetry',\n",
                "            'agency costs', 'agency problems'\n",
                "        ],\n",
                "        'modeling': [\n",
                "            'agent-based modeling', 'agent based modeling', 'ABM',\n",
                "            'multi-agent system', 'multi agent system', 'MAS',\n",
                "            'autonomous agents', 'intelligent agents', 'software agents'\n",
                "        ],\n",
                "        'governance': [\n",
                "            'governance mechanisms', 'monitoring', 'incentive alignment',\n",
                "            'contract theory', 'agency relationships'\n",
                "        ]\n",
                "    },\n",
                "    'ai_llm': {\n",
                "        'ai_general': [\n",
                "            'artificial intelligence', 'machine learning', 'deep learning',\n",
                "            'neural networks', 'AI', 'ML', 'DL',\n",
                "            'computer vision', 'natural language processing', 'NLP'\n",
                "        ],\n",
                "        'llm_specific': [\n",
                "            'large language model', 'large language models', 'LLM', 'LLMs',\n",
                "            'transformer', 'transformers', 'BERT', 'GPT',\n",
                "            'language model', 'foundation model', 'foundation models',\n",
                "            'generative AI', 'ChatGPT', 'OpenAI'\n",
                "        ],\n",
                "        'applications': [\n",
                "            'conversational AI', 'dialogue system', 'chatbot',\n",
                "            'text generation', 'language generation',\n",
                "            'prompt engineering', 'fine-tuning'\n",
                "        ]\n",
                "    },\n",
                "    'agent_terms': {\n",
                "        'general': [\n",
                "            'agent', 'agents', 'agented', 'agenting',\n",
                "            'agency', 'agencies'\n",
                "        ],\n",
                "        'compound': [\n",
                "            'AI agent', 'AI agents', 'artificial agent',\n",
                "            'digital agent', 'virtual agent', 'cognitive agent',\n",
                "            'conversational agent', 'autonomous agent'\n",
                "        ]\n",
                "    }\n",
                "}\n",
                "\n",
                "# Create efficient regex patterns\n",
                "def create_pattern(keywords):\n",
                "    \"\"\"Create case-insensitive regex pattern from keyword list\"\"\"\n",
                "    escaped = [re.escape(kw.lower()) for kw in keywords]\n",
                "    return '|'.join(escaped)\n",
                "\n",
                "PATTERNS = {}\n",
                "for category, subcategories in KEYWORD_TAXONOMY.items():\n",
                "    PATTERNS[category] = {}\n",
                "    all_keywords = []\n",
                "    for subcat, keywords in subcategories.items():\n",
                "        PATTERNS[category][subcat] = create_pattern(keywords)\n",
                "        all_keywords.extend(keywords)\n",
                "    PATTERNS[category]['all'] = create_pattern(all_keywords)\n",
                "\n",
                "print(\"=== KEYWORD TAXONOMY CREATED ===\")\n",
                "for category, subcats in KEYWORD_TAXONOMY.items():\n",
                "    total_keywords = sum(len(keywords) for keywords in subcats.values())\n",
                "    print(f\"{category:15}: {total_keywords:3d} keywords across {len(subcats)} subcategories\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Efficient Text Analysis Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fast_keyword_match(text, pattern):\n",
                "    \"\"\"Fast regex-based keyword matching\"\"\"\n",
                "    if pd.isna(text) or text is None:\n",
                "        return False\n",
                "    return bool(re.search(pattern, str(text).lower()))\n",
                "\n",
                "def extract_matching_terms(text, pattern, original_keywords):\n",
                "    \"\"\"Extract actual matching terms from text\"\"\"\n",
                "    if pd.isna(text) or text is None:\n",
                "        return []\n",
                "    \n",
                "    text_lower = str(text).lower()\n",
                "    matches = []\n",
                "    \n",
                "    for keyword in original_keywords:\n",
                "        if keyword.lower() in text_lower:\n",
                "            matches.append(keyword)\n",
                "    \n",
                "    return matches\n",
                "\n",
                "def analyze_publications_by_category(df, field_map):\n",
                "    \"\"\"Comprehensive publication analysis by keyword categories\"\"\"\n",
                "    \n",
                "    results = {\n",
                "        'total_publications': len(df),\n",
                "        'by_category': {},\n",
                "        'temporal_data': {},\n",
                "        'matching_articles': {}\n",
                "    }\n",
                "    \n",
                "    # Text fields to search\n",
                "    search_fields = []\n",
                "    if 'title' in field_map:\n",
                "        search_fields.append(('title', field_map['title']))\n",
                "    if 'abstract' in field_map:\n",
                "        search_fields.append(('abstract', field_map['abstract']))\n",
                "    if 'keywords' in field_map:\n",
                "        search_fields.append(('keywords', field_map['keywords']))\n",
                "    \n",
                "    print(f\"Searching in fields: {[f[0] for f in search_fields]}\")\n",
                "    \n",
                "    # Year field for temporal analysis\n",
                "    year_field = field_map.get('year')\n",
                "    \n",
                "    # Analyze each category\n",
                "    for category in KEYWORD_TAXONOMY.keys():\n",
                "        print(f\"\\nAnalyzing {category}...\")\n",
                "        \n",
                "        category_mask = pd.Series([False] * len(df))\n",
                "        subcategory_results = {}\n",
                "        \n",
                "        # Check each subcategory\n",
                "        for subcat in KEYWORD_TAXONOMY[category].keys():\n",
                "            subcat_mask = pd.Series([False] * len(df))\n",
                "            \n",
                "            # Search in all specified fields\n",
                "            for field_name, col_name in search_fields:\n",
                "                if col_name in df.columns:\n",
                "                    field_matches = df[col_name].apply(\n",
                "                        lambda x: fast_keyword_match(x, PATTERNS[category][subcat])\n",
                "                    )\n",
                "                    subcat_mask |= field_matches\n",
                "            \n",
                "            subcategory_results[subcat] = subcat_mask.sum()\n",
                "            category_mask |= subcat_mask\n",
                "        \n",
                "        # Store results\n",
                "        results['by_category'][category] = {\n",
                "            'total': category_mask.sum(),\n",
                "            'percentage': (category_mask.sum() / len(df)) * 100,\n",
                "            'subcategories': subcategory_results\n",
                "        }\n",
                "        \n",
                "        # Store matching article indices\n",
                "        results['matching_articles'][category] = df[category_mask].index.tolist()\n",
                "        \n",
                "        # Temporal analysis if year data available\n",
                "        if year_field and year_field in df.columns:\n",
                "            matching_df = df[category_mask]\n",
                "            if len(matching_df) > 0:\n",
                "                # Clean year data\n",
                "                years = pd.to_numeric(matching_df[year_field], errors='coerce').dropna()\n",
                "                if len(years) > 0:\n",
                "                    year_counts = years.value_counts().sort_index()\n",
                "                    results['temporal_data'][category] = {\n",
                "                        'year_counts': year_counts.to_dict(),\n",
                "                        'year_range': (int(years.min()), int(years.max())),\n",
                "                        'peak_year': int(year_counts.idxmax()),\n",
                "                        'articles_with_years': len(years)\n",
                "                    }\n",
                "        \n",
                "        print(f\"  Found {results['by_category'][category]['total']:,} articles ({results['by_category'][category]['percentage']:.1f}%)\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "print(\"Analysis functions ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Execute Comprehensive Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the comprehensive analysis\n",
                "if df is not None and 'field_map' in globals():\n",
                "    print(\"=== STARTING COMPREHENSIVE ANALYSIS ===\")\n",
                "    print(f\"Dataset: {len(df):,} publications\")\n",
                "    print(f\"Fields available: {list(field_map.keys())}\")\n",
                "    \n",
                "    # Execute analysis\n",
                "    analysis_results = analyze_publications_by_category(df, field_map)\n",
                "    \n",
                "    print(\"\\n=== ANALYSIS SUMMARY ===\")\n",
                "    for category, data in analysis_results['by_category'].items():\n",
                "        print(f\"{category.upper():15}: {data['total']:6,} articles ({data['percentage']:5.1f}%)\")\n",
                "        \n",
                "        # Show subcategory breakdown\n",
                "        for subcat, count in data['subcategories'].items():\n",
                "            pct = (count / analysis_results['total_publications']) * 100\n",
                "            print(f\"  └─ {subcat:12}: {count:6,} articles ({pct:5.1f}%)\")\n",
                "        print()\n",
                "    \n",
                "    # Intersection analysis\n",
                "    print(\"=== INTERSECTION ANALYSIS ===\")\n",
                "    categories = list(analysis_results['matching_articles'].keys())\n",
                "    \n",
                "    for i, cat1 in enumerate(categories):\n",
                "        for cat2 in categories[i+1:]:\n",
                "            set1 = set(analysis_results['matching_articles'][cat1])\n",
                "            set2 = set(analysis_results['matching_articles'][cat2])\n",
                "            intersection = len(set1 & set2)\n",
                "            \n",
                "            if intersection > 0:\n",
                "                total = analysis_results['total_publications']\n",
                "                pct = (intersection / total) * 100\n",
                "                print(f\"{cat1} ∩ {cat2}: {intersection:,} articles ({pct:.2f}%)\")\n",
                "else:\n",
                "    print(\"❌ Cannot run analysis - data or field mapping not available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Temporal Evolution Analysis - Focus on 'Agent' Terms"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deep dive into agent term evolution over time\n",
                "def analyze_agent_evolution(df, field_map, analysis_results):\n",
                "    \"\"\"Detailed analysis of agent term evolution and AI/LLM connections\"\"\"\n",
                "    \n",
                "    if 'year' not in field_map:\n",
                "        print(\"❌ No year field available for temporal analysis\")\n",
                "        return None\n",
                "    \n",
                "    year_field = field_map['year']\n",
                "    \n",
                "    # Get agent-related articles\n",
                "    agent_indices = set(analysis_results['matching_articles'].get('agency', []))\n",
                "    agent_term_indices = set(analysis_results['matching_articles'].get('agent_terms', []))\n",
                "    ai_indices = set(analysis_results['matching_articles'].get('ai_llm', []))\n",
                "    \n",
                "    # Combine agent-related terms\n",
                "    all_agent_indices = agent_indices | agent_term_indices\n",
                "    \n",
                "    print(f\"=== AGENT TERM EVOLUTION ANALYSIS ===\")\n",
                "    print(f\"Total agent-related articles: {len(all_agent_indices):,}\")\n",
                "    print(f\"Agent + AI intersection: {len(all_agent_indices & ai_indices):,}\")\n",
                "    \n",
                "    # Create temporal analysis\n",
                "    agent_df = df.loc[list(all_agent_indices)].copy()\n",
                "    ai_df = df.loc[list(ai_indices)].copy()\n",
                "    agent_ai_df = df.loc[list(all_agent_indices & ai_indices)].copy()\n",
                "    \n",
                "    # Clean year data\n",
                "    for temp_df, name in [(agent_df, 'agent'), (ai_df, 'ai'), (agent_ai_df, 'agent_ai')]:\n",
                "        temp_df['clean_year'] = pd.to_numeric(temp_df[year_field], errors='coerce')\n",
                "        temp_df = temp_df.dropna(subset=['clean_year'])\n",
                "        temp_df['clean_year'] = temp_df['clean_year'].astype(int)\n",
                "        \n",
                "        # Filter reasonable years (1990-2024)\n",
                "        temp_df = temp_df[(temp_df['clean_year'] >= 1990) & (temp_df['clean_year'] <= 2024)]\n",
                "        globals()[f'{name}_df_clean'] = temp_df\n",
                "    \n",
                "    # Year-by-year analysis\n",
                "    year_analysis = {}\n",
                "    \n",
                "    all_years = set()\n",
                "    if len(agent_df_clean) > 0:\n",
                "        all_years.update(agent_df_clean['clean_year'].unique())\n",
                "    if len(ai_df_clean) > 0:\n",
                "        all_years.update(ai_df_clean['clean_year'].unique())\n",
                "    \n",
                "    for year in sorted(all_years):\n",
                "        agent_count = len(agent_df_clean[agent_df_clean['clean_year'] == year])\n",
                "        ai_count = len(ai_df_clean[ai_df_clean['clean_year'] == year])\n",
                "        agent_ai_count = len(agent_ai_df_clean[agent_ai_df_clean['clean_year'] == year])\n",
                "        \n",
                "        year_analysis[year] = {\n",
                "            'agent_articles': agent_count,\n",
                "            'ai_articles': ai_count,\n",
                "            'agent_ai_articles': agent_ai_count,\n",
                "            'agent_ai_ratio': agent_ai_count / max(agent_count, 1)\n",
                "        }\n",
                "    \n",
                "    return {\n",
                "        'year_analysis': year_analysis,\n",
                "        'agent_df': agent_df_clean,\n",
                "        'ai_df': ai_df_clean,\n",
                "        'agent_ai_df': agent_ai_df_clean,\n",
                "        'summary': {\n",
                "            'total_agent_articles': len(agent_df_clean),\n",
                "            'total_ai_articles': len(ai_df_clean),\n",
                "            'total_agent_ai_articles': len(agent_ai_df_clean),\n",
                "            'year_range': (min(all_years) if all_years else None, max(all_years) if all_years else None)\n",
                "        }\n",
                "    }\n",
                "\n",
                "# Execute agent evolution analysis\n",
                "if df is not None and 'analysis_results' in globals():\n",
                "    evolution_results = analyze_agent_evolution(df, field_map, analysis_results)\n",
                "    \n",
                "    if evolution_results:\n",
                "        print(\"\\n=== EVOLUTION SUMMARY ===\")\n",
                "        summary = evolution_results['summary']\n",
                "        print(f\"Agent articles: {summary['total_agent_articles']:,}\")\n",
                "        print(f\"AI articles: {summary['total_ai_articles']:,}\")\n",
                "        print(f\"Agent+AI articles: {summary['total_agent_ai_articles']:,}\")\n",
                "        print(f\"Year range: {summary['year_range'][0]}-{summary['year_range'][1]}\")\n",
                "        \n",
                "        # Show key trends\n",
                "        print(\"\\n=== KEY TRENDS (Last 10 Years) ===\")\n",
                "        recent_years = sorted([y for y in evolution_results['year_analysis'].keys() if y >= 2014])\n",
                "        \n",
                "        for year in recent_years[-10:]:  # Last 10 years\n",
                "            data = evolution_results['year_analysis'][year]\n",
                "            ratio = data['agent_ai_ratio'] * 100\n",
                "            print(f\"{year}: Agent={data['agent_articles']:3d}, AI={data['ai_articles']:4d}, Agent+AI={data['agent_ai_articles']:3d} ({ratio:.1f}%)\")\n",
                "else:\n",
                "    print(\"❌ Cannot run evolution analysis\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualization Dashboard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comprehensive visualizations\n",
                "def create_analysis_dashboard(analysis_results, evolution_results=None):\n",
                "    \"\"\"Create a comprehensive visualization dashboard\"\"\"\n",
                "    \n",
                "    fig = make_subplots(\n",
                "        rows=3, cols=2,\n",
                "        subplot_titles=[\n",
                "            'Publications by Category',\n",
                "            'Category Intersections',\n",
                "            'Temporal Evolution: Agent Terms',\n",
                "            'Agent-AI Connection Over Time',\n",
                "            'Supply Chain vs Agency Publications',\n",
                "            'Research Domain Distribution'\n",
                "        ],\n",
                "        specs=[\n",
                "            [{'type': 'bar'}, {'type': 'scatter'}],\n",
                "            [{'type': 'scatter'}, {'type': 'scatter'}],\n",
                "            [{'type': 'bar'}, {'type': 'pie'}]\n",
                "        ]\n",
                "    )\n",
                "    \n",
                "    # 1. Publications by Category\n",
                "    categories = list(analysis_results['by_category'].keys())\n",
                "    counts = [analysis_results['by_category'][cat]['total'] for cat in categories]\n",
                "    \n",
                "    fig.add_trace(\n",
                "        go.Bar(name='Publications', x=categories, y=counts, marker_color='lightblue'),\n",
                "        row=1, col=1\n",
                "    )\n",
                "    \n",
                "    # 2. Temporal evolution if available\n",
                "    if evolution_results and 'year_analysis' in evolution_results:\n",
                "        years = sorted(evolution_results['year_analysis'].keys())\n",
                "        agent_counts = [evolution_results['year_analysis'][y]['agent_articles'] for y in years]\n",
                "        ai_counts = [evolution_results['year_analysis'][y]['ai_articles'] for y in years]\n",
                "        \n",
                "        fig.add_trace(\n",
                "            go.Scatter(x=years, y=agent_counts, mode='lines+markers', name='Agent Articles', line=dict(color='red')),\n",
                "            row=2, col=1\n",
                "        )\n",
                "        \n",
                "        fig.add_trace(\n",
                "            go.Scatter(x=years, y=ai_counts, mode='lines+markers', name='AI Articles', line=dict(color='blue')),\n",
                "            row=2, col=1\n",
                "        )\n",
                "        \n",
                "        # Agent-AI connection ratio\n",
                "        ratios = [evolution_results['year_analysis'][y]['agent_ai_ratio'] * 100 for y in years]\n",
                "        fig.add_trace(\n",
                "            go.Scatter(x=years, y=ratios, mode='lines+markers', name='Agent-AI Connection %', line=dict(color='green')),\n",
                "            row=2, col=2\n",
                "        )\n",
                "    \n",
                "    # 3. Domain distribution pie chart\n",
                "    fig.add_trace(\n",
                "        go.Pie(labels=categories, values=counts, name=\"Domains\"),\n",
                "        row=3, col=2\n",
                "    )\n",
                "    \n",
                "    # Update layout\n",
                "    fig.update_layout(\n",
                "        height=1200,\n",
                "        title_text=\"Research Publications Analysis Dashboard\",\n",
                "        showlegend=True\n",
                "    )\n",
                "    \n",
                "    return fig\n",
                "\n",
                "# Generate visualizations\n",
                "if 'analysis_results' in globals():\n",
                "    print(\"Creating visualization dashboard...\")\n",
                "    \n",
                "    # Create dashboard\n",
                "    dashboard = create_analysis_dashboard(\n",
                "        analysis_results, \n",
                "        evolution_results if 'evolution_results' in globals() else None\n",
                "    )\n",
                "    \n",
                "    # Display\n",
                "    dashboard.show()\n",
                "    \n",
                "    # Save as HTML\n",
                "    output_path = data_dir.parent / 'reports' / 'analysis_dashboard.html'\n",
                "    output_path.parent.mkdir(exist_ok=True)\n",
                "    dashboard.write_html(str(output_path))\n",
                "    print(f\"Dashboard saved to: {output_path}\")\n",
                "    \n",
                "    # Create simple matplotlib plots for backup\n",
                "    plt.figure(figsize=(15, 10))\n",
                "    \n",
                "    # Plot 1: Category overview\n",
                "    plt.subplot(2, 3, 1)\n",
                "    categories = list(analysis_results['by_category'].keys())\n",
                "    counts = [analysis_results['by_category'][cat]['total'] for cat in categories]\n",
                "    plt.bar(categories, counts, color='skyblue')\n",
                "    plt.title('Publications by Category')\n",
                "    plt.xticks(rotation=45)\n",
                "    \n",
                "    # Plot 2: Temporal evolution (if available)\n",
                "    if 'evolution_results' in globals() and evolution_results:\n",
                "        plt.subplot(2, 3, 2)\n",
                "        years = sorted(evolution_results['year_analysis'].keys())\n",
                "        agent_counts = [evolution_results['year_analysis'][y]['agent_articles'] for y in years]\n",
                "        ai_counts = [evolution_results['year_analysis'][y]['ai_articles'] for y in years]\n",
                "        \n",
                "        plt.plot(years, agent_counts, 'r-o', label='Agent Articles', markersize=4)\n",
                "        plt.plot(years, ai_counts, 'b-s', label='AI Articles', markersize=4)\n",
                "        plt.title('Temporal Evolution')\n",
                "        plt.xlabel('Year')\n",
                "        plt.ylabel('Number of Articles')\n",
                "        plt.legend()\n",
                "        plt.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    \n",
                "    # Save matplotlib figure\n",
                "    plot_path = data_dir.parent / 'reports' / 'analysis_plots.png'\n",
                "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
                "    print(f\"Static plots saved to: {plot_path}\")\n",
                "    \n",
                "    plt.show()\n",
                "    \n",
                "else:\n",
                "    print(\"❌ No analysis results available for visualization\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Generate Analysis Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate comprehensive analysis report\n",
                "def generate_analysis_report(analysis_results, evolution_results=None, df=None):\n",
                "    \"\"\"Generate a comprehensive markdown report\"\"\"\n",
                "    \n",
                "    report = []\n",
                "    report.append(\"# OpenAlex Publications Analysis Report\")\n",
                "    report.append(f\"*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\")\n",
                "    \n",
                "    # Executive Summary\n",
                "    report.append(\"## Executive Summary\")\n",
                "    total = analysis_results['total_publications']\n",
                "    report.append(f\"- **Total Publications Analyzed**: {total:,}\")\n",
                "    \n",
                "    for category, data in analysis_results['by_category'].items():\n",
                "        report.append(f\"- **{category.title()} Publications**: {data['total']:,} ({data['percentage']:.1f}%)\")\n",
                "    \n",
                "    # Detailed Category Analysis\n",
                "    report.append(\"\\n## Detailed Analysis by Category\")\n",
                "    \n",
                "    for category, data in analysis_results['by_category'].items():\n",
                "        report.append(f\"\\n### {category.title().replace('_', ' ')} Publications\")\n",
                "        report.append(f\"- **Total**: {data['total']:,} publications ({data['percentage']:.1f}% of dataset)\")\n",
                "        report.append(f\"- **Subcategory Breakdown**:\")\n",
                "        \n",
                "        for subcat, count in data['subcategories'].items():\n",
                "            pct = (count / total) * 100\n",
                "            report.append(f\"  - {subcat.title()}: {count:,} articles ({pct:.1f}%)\")\n",
                "    \n",
                "    # Temporal Analysis\n",
                "    if evolution_results:\n",
                "        report.append(\"\\n## Temporal Evolution Analysis\")\n",
                "        \n",
                "        summary = evolution_results['summary']\n",
                "        report.append(f\"- **Analysis Period**: {summary['year_range'][0]}-{summary['year_range'][1]}\")\n",
                "        report.append(f\"- **Agent-related Publications**: {summary['total_agent_articles']:,}\")\n",
                "        report.append(f\"- **AI-related Publications**: {summary['total_ai_articles']:,}\")\n",
                "        report.append(f\"- **Agent+AI Publications**: {summary['total_agent_ai_articles']:,}\")\n",
                "        \n",
                "        # Key trends\n",
                "        report.append(\"\\n### Key Trends (2020-2024)\")\n",
                "        recent_years = [y for y in evolution_results['year_analysis'].keys() if y >= 2020]\n",
                "        \n",
                "        if recent_years:\n",
                "            report.append(\"| Year | Agent Articles | AI Articles | Agent+AI | Connection Rate |\")\n",
                "            report.append(\"|------|----------------|-------------|----------|-----------------|\")\n",
                "            \n",
                "            for year in sorted(recent_years):\n",
                "                data = evolution_results['year_analysis'][year]\n",
                "                ratio = data['agent_ai_ratio'] * 100\n",
                "                report.append(f\"| {year} | {data['agent_articles']:,} | {data['ai_articles']:,} | {data['agent_ai_articles']:,} | {ratio:.1f}% |\")\n",
                "    \n",
                "    # Intersection Analysis\n",
                "    report.append(\"\\n## Category Intersections\")\n",
                "    categories = list(analysis_results['matching_articles'].keys())\n",
                "    \n",
                "    report.append(\"| Category 1 | Category 2 | Intersection | Percentage |\")\n",
                "    report.append(\"|------------|------------|--------------|------------|\")\n",
                "    \n",
                "    for i, cat1 in enumerate(categories):\n",
                "        for cat2 in categories[i+1:]:\n",
                "            set1 = set(analysis_results['matching_articles'][cat1])\n",
                "            set2 = set(analysis_results['matching_articles'][cat2])\n",
                "            intersection = len(set1 & set2)\n",
                "            \n",
                "            if intersection > 0:\n",
                "                pct = (intersection / total) * 100\n",
                "                report.append(f\"| {cat1.title()} | {cat2.title()} | {intersection:,} | {pct:.2f}% |\")\n",
                "    \n",
                "    # Recommendations\n",
                "    report.append(\"\\n## Key Findings & Recommendations\")\n",
                "    \n",
                "    # Find dominant categories\n",
                "    sorted_cats = sorted(analysis_results['by_category'].items(), \n",
                "                        key=lambda x: x[1]['total'], reverse=True)\n",
                "    \n",
                "    report.append(f\"1. **Dominant Research Area**: {sorted_cats[0][0].title().replace('_', ' ')} represents the largest category with {sorted_cats[0][1]['total']:,} publications\")\n",
                "    \n",
                "    if evolution_results:\n",
                "        # Find growth trends\n",
                "        recent_data = {year: data for year, data in evolution_results['year_analysis'].items() if year >= 2020}\n",
                "        if len(recent_data) >= 2:\n",
                "            years = sorted(recent_data.keys())\n",
                "            ai_growth = recent_data[years[-1]]['ai_articles'] - recent_data[years[0]]['ai_articles']\n",
                "            report.append(f\"2. **AI Research Growth**: AI-related publications showed {'positive' if ai_growth > 0 else 'negative'} growth in recent years\")\n",
                "    \n",
                "    # Check supply chain relevance\n",
                "    sc_relevant = analysis_results['by_category'].get('supply_chain', {}).get('total', 0)\n",
                "    sc_percentage = (sc_relevant / total) * 100\n",
                "    \n",
                "    if sc_percentage < 10:\n",
                "        report.append(f\"3. **Supply Chain Relevance**: Only {sc_percentage:.1f}% of publications are supply chain-related. Consider refining search criteria for better relevance.\")\n",
                "    else:\n",
                "        report.append(f\"3. **Supply Chain Relevance**: {sc_percentage:.1f}% of publications are supply chain-related, indicating good dataset relevance.\")\n",
                "    \n",
                "    return \"\\n\".join(report)\n",
                "\n",
                "# Generate and save report\n",
                "if 'analysis_results' in globals():\n",
                "    print(\"Generating comprehensive analysis report...\")\n",
                "    \n",
                "    report_content = generate_analysis_report(\n",
                "        analysis_results,\n",
                "        evolution_results if 'evolution_results' in globals() else None,\n",
                "        df\n",
                "    )\n",
                "    \n",
                "    # Save report\n",
                "    report_path = data_dir.parent / 'reports' / 'openalex_analysis_report.md'\n",
                "    report_path.parent.mkdir(exist_ok=True)\n",
                "    \n",
                "    with open(report_path, 'w', encoding='utf-8') as f:\n",
                "        f.write(report_content)\n",
                "    \n",
                "    print(f\"✅ Report saved to: {report_path}\")\n",
                "    \n",
                "    # Display summary\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"ANALYSIS COMPLETE\")\n",
                "    print(\"=\"*50)\n",
                "    print(f\"📊 Dashboard: reports/analysis_dashboard.html\")\n",
                "    print(f\"📈 Plots: reports/analysis_plots.png\")\n",
                "    print(f\"📄 Report: reports/openalex_analysis_report.md\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "else:\n",
                "    print(\"❌ Cannot generate report - analysis results not available\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
