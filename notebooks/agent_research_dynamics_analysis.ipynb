{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04aa902f",
   "metadata": {},
   "source": [
    "# Comprehensive Keyword Analysis Module Demo\n",
    "\n",
    "This notebook demonstrates the full capabilities of the Keyword Analysis Module for the \"Agentic AI in SCM\" Systematic Literature Review.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- API-based keyword extraction\n",
    "- NLP-based keyword extraction (TF-IDF, RAKE, YAKE)\n",
    "- Semantic analysis with BGE-M3 embeddings\n",
    "- Temporal trend analysis\n",
    "- Keyword lifecycle analysis\n",
    "- Comparative time period analysis\n",
    "- Comprehensive visualizations\n",
    "- Interactive dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de77ad",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8757b16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Diagnostics:\n",
      "Python executable: /opt/conda/envs/tsi/bin/python\n",
      "Python version: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:38:00) [GCC 12.3.0]\n",
      "Current working directory: /workspaces/tsi-sota-ai/notebooks\n",
      "Project root detected: /workspaces/tsi-sota-ai/notebooks\n",
      "‚úÖ Running in Docker/devcontainer\n",
      "‚úÖ Numpy import successful: 1.26.4 from /opt/conda/envs/tsi/lib/python3.11/site-packages/numpy/__init__.py\n",
      "\n",
      "üì¶ Checking key dependencies:\n",
      "‚úÖ pandas available\n",
      "‚úÖ yaml available\n",
      "‚úÖ requests available\n"
     ]
    }
   ],
   "source": [
    "# Environment Diagnostics - Check before imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Environment Diagnostics:\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root detected: {Path.cwd()}\")\n",
    "\n",
    "# Check if we're in a devcontainer\n",
    "if os.path.exists('/.dockerenv'):\n",
    "    print(\"‚úÖ Running in Docker/devcontainer\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not in devcontainer\")\n",
    "\n",
    "# Check numpy specifically\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"‚úÖ Numpy import successful: {np.__version__} from {np.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Numpy import failed: {e}\")\n",
    "    # Try to diagnose the issue\n",
    "    import subprocess\n",
    "    result = subprocess.run([sys.executable, '-c', 'import numpy; print(numpy.__file__)'], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Numpy path from subprocess: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(f\"Subprocess error: {result.stderr}\")\n",
    "\n",
    "print(\"\\nüì¶ Checking key dependencies:\")\n",
    "for package in ['pandas', 'yaml', 'requests']:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} available\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package} not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d72ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded from: /workspaces/tsi-sota-ai/config/slr_config.yaml\n",
      "Configuration summary:\n",
      "- Keyword Analysis: 2 settings\n",
      "- Semantic Analysis: 3 settings\n",
      "- Temporal Analysis: 0 settings\n",
      "- Visualization: 6 settings\n",
      "- Test queries: 3 queries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Load configuration\n",
    "config_path = '/workspaces/tsi-sota-ai/config/slr_config.yaml'\n",
    "\n",
    "# Check if config file exists\n",
    "if not os.path.exists(config_path):\n",
    "    print(f\"‚ö†Ô∏è Config file not found at: {config_path}\")\n",
    "    print(\"Looking for alternative config locations...\")\n",
    "    \n",
    "    # Try alternative paths\n",
    "    alt_paths = [\n",
    "        os.path.join(project_root, 'config', 'slr_config.yaml'),\n",
    "        os.path.join(os.getcwd(), 'config', 'slr_config.yaml'),\n",
    "        os.path.join(os.path.dirname(os.getcwd()), 'config', 'slr_config.yaml')\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alt_paths:\n",
    "        if os.path.exists(alt_path):\n",
    "            config_path = alt_path\n",
    "            print(f\"‚úÖ Found config at: {config_path}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚ùå No config file found. Creating minimal config...\")\n",
    "        config = {\n",
    "            'keyword_analysis': {},\n",
    "            'semantic_analysis': {},\n",
    "            'temporal_analysis': {},\n",
    "            'visualization': {},\n",
    "            'test_search_queries': ['agent AI supply chain', 'autonomous agents logistics']\n",
    "        }\n",
    "        print(\"Using default configuration.\")\n",
    "\n",
    "if 'config' not in locals():\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(f\"‚úÖ Configuration loaded from: {config_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading config: {e}\")\n",
    "        config = {\n",
    "            'keyword_analysis': {},\n",
    "            'semantic_analysis': {},\n",
    "            'temporal_analysis': {},\n",
    "            'visualization': {},\n",
    "            'test_search_queries': ['agent AI supply chain', 'autonomous agents logistics']\n",
    "        }\n",
    "        print(\"Using fallback configuration.\")\n",
    "\n",
    "print(\"Configuration summary:\")\n",
    "print(f\"- Keyword Analysis: {len(config.get('keyword_analysis', {}))} settings\")\n",
    "print(f\"- Semantic Analysis: {len(config.get('semantic_analysis', {}))} settings\")\n",
    "print(f\"- Temporal Analysis: {len(config.get('temporal_analysis', {}))} settings\")\n",
    "print(f\"- Visualization: {len(config.get('visualization', {}))} settings\")\n",
    "print(f\"- Test queries: {len(config.get('test_search_queries', []))} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7833d4",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition\n",
    "\n",
    "First, let's acquire some sample data using our test search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06872345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# For the devcontainer path, update your import cell:\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add slr_core to Python path\n",
    "sys.path.append('/workspaces/tsi-sota-ai')  # Add project root, not just slr_core\n",
    "\n",
    "# Correct imports based on your actual module structure:\n",
    "try:\n",
    "    from slr_core.data_acquirer import DataAcquirer\n",
    "    from slr_core.keyword_analysis import KeywordExtractor, SemanticAnalyzer, TemporalAnalyzer, Visualizer\n",
    "    from slr_core.config_manager import ConfigManager\n",
    "    print(\"‚úÖ Imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    \n",
    "    # If above fails, try individual imports to debug:\n",
    "    try:\n",
    "        from slr_core.data_acquirer import DataAcquirer\n",
    "        print(\"‚úÖ DataAcquirer imported\")\n",
    "    except ImportError as e1:\n",
    "        print(f\"‚ùå DataAcquirer failed: {e1}\")\n",
    "    \n",
    "    try:\n",
    "        from slr_core.keyword_analysis import KeywordExtractor\n",
    "        print(\"‚úÖ KeywordExtractor imported\")\n",
    "    except ImportError as e2:\n",
    "        print(f\"‚ùå KeywordExtractor failed: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406eccdf",
   "metadata": {},
   "source": [
    "## 2.1 Semantic Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69394b43",
   "metadata": {},
   "source": [
    "### Let's check how many publications we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f410e6f",
   "metadata": {},
   "source": [
    "This improved approach:\n",
    "\n",
    "- Uses the official bulk search endpoint as documented in the Semantic Scholar API docs\n",
    "- Implements pagination-based estimation following the tutorial guidance\n",
    "- Handles different response scenarios (exact count, partial results, pagination limits)\n",
    "- Provides strategic recommendations based on estimated counts\n",
    "- Tests alternative query strategies to help optimize your search\n",
    "- Includes proper error handling and diagnostics\n",
    "\n",
    "\n",
    "The key improvements are:\n",
    "\n",
    "- Direct API interaction using the existing client infrastructure\n",
    "- Minimal data transfer by requesting only paperId field for counting\n",
    "- Smart estimation logic that adapts to different response patterns\n",
    "- Strategic guidance for handling different dataset sizes\n",
    "- Query optimization suggestions based on comparative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a22b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully from /workspaces/tsi-sota-ai/slr_core/../config/slr_config.yaml\n",
      "Warning: Environment variable SEMANTIC_SCHOLAR_API_KEY_API_KEY not set for SEMANTIC_SCHOLAR_API_KEY.\n",
      "Info: No Semantic Scholar API key found. Using public access with shared rate limits.\n",
      "üîç Estimating publication count for query:\n",
      "   Query: agent AND (scm OR \"supply chain management\" OR logistics)\n",
      "   Year: 2025\n",
      "   Source: Semantic Scholar only\n",
      "   Method: Pagination-based estimation\n",
      "Making initial request to estimate total count...\n",
      "\n",
      "üìä Estimated publications: 2\n",
      "   Method: Exact count from API response\n",
      "\n",
      "üí° Recommendation: Fetch all publications (2 is manageable)\n",
      "\n",
      "üîÑ Testing alternative query strategies for comparison:\n",
      "Making initial request to estimate total count...\n",
      "   Simpler query: 'agent scm' -> ~24 papers\n",
      "Making initial request to estimate total count...\n",
      "   Quoted phrases: '\"agent\" AND \"supply chain\"' -> ~110 papers\n",
      "Making initial request to estimate total count...\n",
      "   Broader terms: 'autonomous OR agent OR AI supply chain' -> ~5 papers\n",
      "Making initial request to estimate total count...\n",
      "   Specific field: 'multiagent supply chain' -> ~1 papers\n",
      "\n",
      "üìö Reference Documentation:\n",
      "   ‚Ä¢ API Docs: https://api.semanticscholar.org/api-docs/\n",
      "   ‚Ä¢ Pagination Tutorial: https://www.semanticscholar.org/product/api/tutorial#pagination\n",
      "   ‚Ä¢ Bulk Search: https://api.semanticscholar.org/api-docs/#tag/Paper-Data/operation/get_graph_paper_bulk_search\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Implement proper count estimation in SemanticScholarAPIClient\n",
      "   2. Add pagination-aware methods to DataAcquirer\n",
      "   3. Consider implementing query optimization based on count estimates\n",
      "   4. Add caching for count estimates to avoid repeated API calls\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize with ConfigManager\n",
    "config_manager = ConfigManager()\n",
    "data_acquirer = DataAcquirer(config_manager=config_manager)\n",
    "\n",
    "# Define your specific query\n",
    "search_query = 'agent AND (scm OR \"supply chain management\" OR logistics)'\n",
    "target_year = 2025\n",
    "\n",
    "print(f\"üîç Estimating publication count for query:\")\n",
    "print(f\"   Query: {search_query}\")\n",
    "print(f\"   Year: {target_year}\")\n",
    "print(f\"   Source: Semantic Scholar only\")\n",
    "print(f\"   Method: Pagination-based estimation\")\n",
    "\n",
    "def estimate_semantic_scholar_count(data_acquirer, query, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Estimate total publication count using Semantic Scholar API pagination.\n",
    "    Based on: https://api.semanticscholar.org/api-docs/#tag/Paper-Data/operation/get_graph_paper_bulk_search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the Semantic Scholar client directly\n",
    "        semantic_client = data_acquirer.clients.get(\"SemanticScholar\")\n",
    "        if not semantic_client:\n",
    "            return None, \"Semantic Scholar client not available\"\n",
    "        \n",
    "        # Use the bulk search endpoint with minimal fields for efficiency\n",
    "        search_url = f\"{semantic_client.base_url}paper/search/bulk\"\n",
    "        \n",
    "        # Construct query with year filter\n",
    "        if start_year == end_year:\n",
    "            year_filter = str(start_year)\n",
    "        else:\n",
    "            year_filter = f\"{start_year}-{end_year}\"\n",
    "        \n",
    "        params = {\n",
    "            'query': query,\n",
    "            'year': year_filter,\n",
    "            'fields': 'paperId',  # Minimal field to reduce response size\n",
    "            'limit': 1000,  # Maximum allowed per request\n",
    "            'offset': 0\n",
    "        }\n",
    "        \n",
    "        print(f\"Making initial request to estimate total count...\")\n",
    "        \n",
    "        # Make request using the client's existing method\n",
    "        from slr_core.api_clients import make_request_with_retry\n",
    "        \n",
    "        response_data = make_request_with_retry(\n",
    "            search_url,\n",
    "            params=params,\n",
    "            headers=semantic_client.headers,\n",
    "            delay_seconds=1\n",
    "        )\n",
    "        \n",
    "        if response_data and 'data' in response_data:\n",
    "            # Check if we have pagination information\n",
    "            total_papers = response_data.get('total', None)\n",
    "            if total_papers is not None:\n",
    "                return total_papers, \"Exact count from API response\"\n",
    "            \n",
    "            # If no total field, estimate based on pagination behavior\n",
    "            first_batch_size = len(response_data['data'])\n",
    "            \n",
    "            if first_batch_size < 1000:\n",
    "                # If first batch is less than max, that's likely the total\n",
    "                return first_batch_size, \"Complete results in first batch\"\n",
    "            \n",
    "            # For larger datasets, we need to sample to estimate\n",
    "            # Try a few more requests with different offsets to estimate\n",
    "            sample_offsets = [1000, 5000, 10000]\n",
    "            valid_samples = []\n",
    "            \n",
    "            for offset in sample_offsets:\n",
    "                params['offset'] = offset\n",
    "                sample_response = make_request_with_retry(\n",
    "                    search_url,\n",
    "                    params=params,\n",
    "                    headers=semantic_client.headers,\n",
    "                    delay_seconds=1\n",
    "                )\n",
    "                \n",
    "                if sample_response and 'data' in sample_response:\n",
    "                    batch_size = len(sample_response['data'])\n",
    "                    if batch_size > 0:\n",
    "                        valid_samples.append(offset + batch_size)\n",
    "                    else:\n",
    "                        # Hit the end, estimate based on this offset\n",
    "                        return offset, f\"Estimated based on pagination end at offset {offset}\"\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if valid_samples:\n",
    "                # Estimate based on highest valid offset\n",
    "                max_valid = max(valid_samples)\n",
    "                return max_valid, f\"Estimated based on sampling (minimum {max_valid})\"\n",
    "            else:\n",
    "                return 1000, \"Conservative estimate (at least 1000)\"\n",
    "                \n",
    "        else:\n",
    "            return None, \"No valid response from API\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None, f\"Error during estimation: {str(e)}\"\n",
    "\n",
    "try:\n",
    "    # Try the estimation method\n",
    "    estimated_count, method_info = estimate_semantic_scholar_count(\n",
    "        data_acquirer, search_query, target_year, target_year\n",
    "    )\n",
    "    \n",
    "    if estimated_count is not None:\n",
    "        print(f\"\\nüìä Estimated publications: {estimated_count:,}\")\n",
    "        print(f\"   Method: {method_info}\")\n",
    "        \n",
    "        # Provide guidance based on count\n",
    "        if estimated_count < 1000:\n",
    "            print(f\"\\nüí° Recommendation: Fetch all publications ({estimated_count} is manageable)\")\n",
    "            suggested_strategy = \"fetch_all\"\n",
    "        elif estimated_count < 10000:\n",
    "            print(f\"\\nüí° Recommendation: Use batch processing with 1000-paper chunks\")\n",
    "            suggested_strategy = \"batch_processing\"\n",
    "        elif estimated_count < 50000:\n",
    "            print(f\"\\nüí° Recommendation: Use pagination with time-based filtering\")\n",
    "            suggested_strategy = \"time_filtered_pagination\"\n",
    "        else:\n",
    "            print(f\"\\nüí° Recommendation: Refine query or use temporal segmentation\")\n",
    "            suggested_strategy = \"query_refinement\"\n",
    "            \n",
    "        # Test alternative query strategies\n",
    "        print(f\"\\nüîÑ Testing alternative query strategies for comparison:\")\n",
    "        \n",
    "        alternative_queries = [\n",
    "            ('Simpler query', 'agent scm'),\n",
    "            ('Quoted phrases', '\"agent\" AND \"supply chain\"'),\n",
    "            ('Broader terms', 'autonomous OR agent OR AI supply chain'),\n",
    "            ('Specific field', 'multiagent supply chain'),\n",
    "        ]\n",
    "        \n",
    "        for desc, alt_query in alternative_queries:\n",
    "            try:\n",
    "                alt_count, alt_method = estimate_semantic_scholar_count(\n",
    "                    data_acquirer, alt_query, target_year, target_year\n",
    "                )\n",
    "                if alt_count is not None:\n",
    "                    print(f\"   {desc}: '{alt_query}' -> ~{alt_count:,} papers\")\n",
    "                else:\n",
    "                    print(f\"   {desc}: '{alt_query}' -> Error: {alt_method}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   {desc}: '{alt_query}' -> Exception: {str(e)}\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"\\n‚ùå Could not estimate count: {method_info}\")\n",
    "        \n",
    "        # Fallback to the original approach\n",
    "        print(f\"\\nüîÑ Falling back to sample-based estimation...\")\n",
    "        \n",
    "        results = data_acquirer.fetch_all_sources(\n",
    "            query=search_query,\n",
    "            start_year=target_year,\n",
    "            end_year=target_year,\n",
    "            max_results_per_source=1\n",
    "        )\n",
    "        \n",
    "        if 'SemanticScholar' in results and results['SemanticScholar']:\n",
    "            print(f\"‚úÖ API is working - got sample results\")\n",
    "            print(f\"üìù Consider implementing pagination-based counting in the SemanticScholarAPIClient\")\n",
    "        else:\n",
    "            print(f\"‚ùå No results from API - check query syntax and API access\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during count estimation: {str(e)}\")\n",
    "    \n",
    "    # Diagnostic information\n",
    "    print(f\"\\nüîß Diagnostic information:\")\n",
    "    try:\n",
    "        semantic_client = data_acquirer.clients.get(\"SemanticScholar\")\n",
    "        if semantic_client:\n",
    "            print(f\"   ‚úÖ Semantic Scholar client available\")\n",
    "            print(f\"   Base URL: {semantic_client.base_url}\")\n",
    "            print(f\"   Headers: {len(semantic_client.headers)} header(s) configured\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Semantic Scholar client not found\")\n",
    "            print(f\"   Available clients: {list(data_acquirer.clients.keys())}\")\n",
    "    except Exception as diag_e:\n",
    "        print(f\"   Error in diagnostics: {diag_e}\")\n",
    "\n",
    "print(f\"\\nüìö Reference Documentation:\")\n",
    "print(f\"   ‚Ä¢ API Docs: https://api.semanticscholar.org/api-docs/\")\n",
    "print(f\"   ‚Ä¢ Pagination Tutorial: https://www.semanticscholar.org/product/api/tutorial#pagination\")\n",
    "print(f\"   ‚Ä¢ Bulk Search: https://api.semanticscholar.org/api-docs/#tag/Paper-Data/operation/get_graph_paper_bulk_search\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Implement proper count estimation in SemanticScholarAPIClient\")\n",
    "print(f\"   2. Add pagination-aware methods to DataAcquirer\")\n",
    "print(f\"   3. Consider implementing query optimization based on count estimates\")\n",
    "print(f\"   4. Add caching for count estimates to avoid repeated API calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf2eee",
   "metadata": {},
   "source": [
    "### Let's fetch data for 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7311b6",
   "metadata": {},
   "source": [
    "This corrected version:\n",
    "\n",
    "- Uses ONLY Semantic Scholar - removed all other sources\n",
    "- Uses correct output directory - data instead of /outputs\n",
    "- Creates directories if needed - ensures the output path exists\n",
    "- Handles the 429 rate limiting that's causing the Semantic Scholar API to fail\n",
    "- Stores results in the proper location with the slr_raw subfolder for raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a90db4",
   "metadata": {},
   "source": [
    "Key improvements in this development-focused version:\n",
    "\n",
    "1. No CSV saving - Everything stays in memory as DataFrames\n",
    "2. Approach tracking - Each publication gets metadata about:\n",
    "- approach_id: Unique identifier for the method used\n",
    "- query_used: The exact query string that found this publication\n",
    "- fetch_method: The technical method used (direct_client, fetch_all_sources, etc.)\n",
    "- fetch_timestamp: When it was retrieved\n",
    "3. Overlap analysis - Shows which approaches found the same papers\n",
    "4. Deduplication with provenance - Keeps track of which approach found each unique paper first\n",
    "5. Comprehensive reporting - Shows statistics by approach and query\n",
    "6. Memory-efficient - Works with DataFrames for faster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a97482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully from /workspaces/tsi-sota-ai/slr_core/../config/slr_config.yaml\n",
      "Warning: Environment variable SEMANTIC_SCHOLAR_API_KEY_API_KEY not set for SEMANTIC_SCHOLAR_API_KEY.\n",
      "Info: No Semantic Scholar API key found. Using public access with shared rate limits.\n",
      "üîç Fetching publications from SEMANTIC SCHOLAR ONLY:\n",
      "   Query: agent AND (scm OR \"supply chain management\" OR logistics)\n",
      "   Year: 2025\n",
      "   Expected: ~165 results (based on UI)\n",
      "   Mode: Development (DataFrames only, no CSV saving)\n",
      "\n",
      "üì• Attempting to fetch publications from Semantic Scholar...\n",
      "‚è±Ô∏è Adding 5-second delays between API calls to respect rate limits...\n",
      "\n",
      "1. Using Semantic Scholar client directly...\n",
      "   Approach ID: method_1_direct_client\n",
      "   Query: agent AND (scm OR \"supply chain management\" OR logistics)\n",
      "   ‚úÖ Semantic Scholar client found\n",
      "[SemanticScholarAPIClient] Fetching from https://api.semanticscholar.org/graph/v1/: 'agent AND (scm OR \"supply chain management\" OR logistics)' from 2025-2025 (max: 200)\n",
      "Request failed (attempt 1/3): 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=agent+AND+%28scm+OR+%22supply+chain+management%22+OR+logistics%29&offset=0&limit=100&fields=title%2Cauthors%2Cyear%2CcitationCount&year=2025\n",
      "Request failed (attempt 2/3): 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=agent+AND+%28scm+OR+%22supply+chain+management%22+OR+logistics%29&offset=0&limit=100&fields=title%2Cauthors%2Cyear%2CcitationCount&year=2025\n",
      "No 'data' field in response\n",
      "   ‚ùå SemanticScholar: No publications found\n",
      "   ‚è±Ô∏è Waiting 5 seconds before next query...\n",
      "\n",
      "3. Trying alternative query variations...\n",
      "\n",
      "üîÑ Alternative query (1/5):\n",
      "   Approach ID: method_3a_simple\n",
      "   Query: 'agent scm'\n",
      "[SemanticScholarAPIClient] Fetching from https://api.semanticscholar.org/graph/v1/: 'agent scm' from 2025-2025 (max: 50)\n",
      "Retrieved 50 papers in this batch, total: 50\n",
      "   ‚úÖ Results: 50 publications\n",
      "   ‚è±Ô∏è Waiting 5 seconds before next query...\n",
      "\n",
      "üîÑ Alternative query (2/5):\n",
      "   Approach ID: method_3b_quoted\n",
      "   Query: '\"supply chain\" agent'\n",
      "[SemanticScholarAPIClient] Fetching from https://api.semanticscholar.org/graph/v1/: '\"supply chain\" agent' from 2025-2025 (max: 50)\n",
      "Request failed (attempt 1/3): 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=%22supply+chain%22+agent&offset=0&limit=50&fields=title%2Cauthors%2Cyear%2CcitationCount&year=2025\n",
      "Request failed (attempt 2/3): 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=%22supply+chain%22+agent&offset=0&limit=50&fields=title%2Cauthors%2Cyear%2CcitationCount&year=2025\n",
      "Request failed (attempt 3/3): 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=%22supply+chain%22+agent&offset=0&limit=50&fields=title%2Cauthors%2Cyear%2CcitationCount&year=2025\n",
      "Error fetching from Semantic Scholar API: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=%22supply+chain%22+agent&offset=0&limit=50&fields=title%2Cauthors%2Cyear%2CcitationCount&year=2025\n",
      "   ‚ùå No results for query: \"supply chain\" agent\n",
      "   ‚è±Ô∏è Waiting 5 seconds before next query...\n",
      "\n",
      "üîÑ Alternative query (3/5):\n",
      "   Approach ID: method_3c_autonomous\n",
      "   Query: 'autonomous agent logistics'\n",
      "[SemanticScholarAPIClient] Fetching from https://api.semanticscholar.org/graph/v1/: 'autonomous agent logistics' from 2025-2025 (max: 50)\n",
      "Retrieved 50 papers in this batch, total: 50\n",
      "   ‚úÖ Results: 50 publications\n",
      "   ‚è±Ô∏è Waiting 5 seconds before next query...\n",
      "\n",
      "üîÑ Alternative query (4/5):\n",
      "   Approach ID: method_3d_agent_based\n",
      "   Query: 'agent-based supply chain'\n",
      "[SemanticScholarAPIClient] Fetching from https://api.semanticscholar.org/graph/v1/: 'agent-based supply chain' from 2025-2025 (max: 50)\n",
      "Retrieved 50 papers in this batch, total: 50\n",
      "   ‚úÖ Results: 50 publications\n",
      "   ‚è±Ô∏è Waiting 5 seconds before next query...\n",
      "\n",
      "üîÑ Alternative query (5/5):\n",
      "   Approach ID: method_3e_multi_agent\n",
      "   Query: 'multi-agent supply chain'\n",
      "[SemanticScholarAPIClient] Fetching from https://api.semanticscholar.org/graph/v1/: 'multi-agent supply chain' from 2025-2025 (max: 50)\n",
      "Retrieved 50 papers in this batch, total: 50\n",
      "   ‚úÖ Results: 50 publications\n",
      "\n",
      "üìä Creating comprehensive DataFrame...\n",
      "   Total publications collected: 200\n",
      "\n",
      "üìã Publications DataFrame created:\n",
      "   Shape: (200, 17)\n",
      "   Columns: ['doi', 'title', 'abstract', 'authors', 'publication_date', 'keywords', 'citation_count', 'reference_count', 'venue', 'publication_types', 'open_access_pdf', 'paper_id', 'source', 'approach_id', 'query_used', 'fetch_method', 'fetch_timestamp']\n",
      "\n",
      "üîç Results by Approach:\n",
      "   method_3a_simple: 50 publications (query: 'agent scm')\n",
      "   method_3c_autonomous: 50 publications (query: 'autonomous agent logistics')\n",
      "   method_3d_agent_based: 50 publications (query: 'agent-based supply chain')\n",
      "   method_3e_multi_agent: 50 publications (query: 'multi-agent supply chain')\n",
      "\n",
      "üîë Results by Query:\n",
      "   'agent scm': 50 publications\n",
      "   'autonomous agent logistics': 50 publications\n",
      "   'agent-based supply chain': 50 publications\n",
      "   'multi-agent supply chain': 50 publications\n",
      "\n",
      "üîÑ Deduplication analysis:\n",
      "   After deduplication: 1 unique publications (removed 199 duplicates)\n",
      "\n",
      "üìà Unique publications by approach (after deduplication):\n",
      "   method_3a_simple: 1 unique publications (query: 'agent scm')\n",
      "\n",
      "üìà Basic Statistics:\n",
      "   Unique DOIs: 0\n",
      "   Unique titles: 1\n",
      "\n",
      "üìö Sample Publications (with approach tracking):\n",
      "\n",
      "   1. Enhancing supply chain resilience with multi-agent systems and machine learning: a framework for adaptive decision-making\n",
      "      DOI: None\n",
      "      Approach: method_3a_simple\n",
      "      Query: agent scm\n",
      "      Method: direct_client_alternative\n",
      "      Citations: 0\n",
      "      Abstract: \n",
      "\n",
      "‚úÖ Publications DataFrame ready for keyword analysis!\n",
      "   Ready to proceed with keyword extraction and analysis\n",
      "   Using deduplicated DataFrame with 1 publications\n",
      "\n",
      "üîß DataAcquirer Debug Information:\n",
      "   Available clients: ['CORE', 'arXiv', 'OpenAlex', 'SemanticScholar']\n",
      "   fetch_all_sources signature: (query: str, start_year: int, end_year: int, max_results_per_source: int = 100) -> Dict[str, List[Dict[str, Any]]]\n",
      "\n",
      "üîç Fetch Analysis Summary:\n",
      "   Total API calls made: 7\n",
      "   Total publications collected: 200\n",
      "   Unique publications after dedup: 1\n",
      "   Rate limiting protection: 5-second delays between calls\n",
      "\n",
      "üîë No structured keywords found, will use NLP extraction from titles/abstracts\n",
      "\n",
      "üìä Development Mode: Data ready in memory for analysis!\n",
      "üìà Variables available:\n",
      "   - publications_df_dedup: Deduplicated DataFrame (1 rows)\n",
      "   - sample_publications: List of publication dicts for analysis\n",
      "   - all_publication_data: Raw data with duplicates (200 records)\n"
     ]
    }
   ],
   "source": [
    "# Let's fetch data into DataFrames with approach tracking (no CSV saving during dev)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize with ConfigManager\n",
    "config_manager = ConfigManager()\n",
    "data_acquirer = DataAcquirer(config_manager=config_manager)\n",
    "\n",
    "# Define your specific query (same as Semantic Scholar UI)\n",
    "search_query = 'agent AND (scm OR \"supply chain management\" OR logistics)'\n",
    "target_year = 2025\n",
    "\n",
    "print(f\"üîç Fetching publications from SEMANTIC SCHOLAR ONLY:\")\n",
    "print(f\"   Query: {search_query}\")\n",
    "print(f\"   Year: {target_year}\")\n",
    "print(f\"   Expected: ~165 results (based on UI)\")\n",
    "print(f\"   Mode: Development (DataFrames only, no CSV saving)\")\n",
    "\n",
    "# Initialize list to collect all publication data with metadata\n",
    "all_publication_data = []\n",
    "\n",
    "print(f\"\\nüì• Attempting to fetch publications from Semantic Scholar...\")\n",
    "print(f\"‚è±Ô∏è Adding 5-second delays between API calls to respect rate limits...\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Use the Semantic Scholar client directly\n",
    "    approach_id = \"method_1_direct_client\"\n",
    "    query_used = search_query\n",
    "    \n",
    "    print(f\"\\n1. Using Semantic Scholar client directly...\")\n",
    "    print(f\"   Approach ID: {approach_id}\")\n",
    "    print(f\"   Query: {query_used}\")\n",
    "    \n",
    "    # Get the Semantic Scholar client\n",
    "    semantic_client = data_acquirer.clients.get(\"SemanticScholar\")\n",
    "    if semantic_client:\n",
    "        print(f\"   ‚úÖ Semantic Scholar client found\")\n",
    "        \n",
    "        # Use the client's fetch method directly\n",
    "        publications = semantic_client.fetch_publications(\n",
    "            query=query_used,\n",
    "            start_year=target_year,\n",
    "            end_year=target_year,\n",
    "            max_results=200\n",
    "        )\n",
    "        \n",
    "        if publications:\n",
    "            print(f\"   ‚úÖ SemanticScholar: {len(publications)} publications\")\n",
    "            # Add approach metadata to each publication\n",
    "            for pub in publications:\n",
    "                pub['approach_id'] = approach_id\n",
    "                pub['query_used'] = query_used\n",
    "                pub['fetch_method'] = 'direct_client'\n",
    "                pub['fetch_timestamp'] = datetime.now().isoformat()\n",
    "            all_publication_data.extend(publications)\n",
    "        else:\n",
    "            print(f\"   ‚ùå SemanticScholar: No publications found\")\n",
    "            \n",
    "        # Sleep before next API call\n",
    "        print(f\"   ‚è±Ô∏è Waiting 5 seconds before next query...\")\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        print(f\"   ‚ùå Semantic Scholar client not available\")\n",
    "        \n",
    "        # Method 2: Try using fetch_all_sources without the sources parameter\n",
    "        approach_id = \"method_2_fetch_all_sources\"\n",
    "        print(f\"\\n2. Using fetch_all_sources (filter to SS only)...\")\n",
    "        print(f\"   Approach ID: {approach_id}\")\n",
    "        print(f\"   Query: {query_used}\")\n",
    "        \n",
    "        results = data_acquirer.fetch_all_sources(\n",
    "            query=query_used,\n",
    "            start_year=target_year,\n",
    "            end_year=target_year,\n",
    "            max_results_per_source=200\n",
    "        )\n",
    "        \n",
    "        # Filter to only Semantic Scholar results\n",
    "        for source, publications in results.items():\n",
    "            if 'semantic' in source.lower() or 'scholar' in source.lower():\n",
    "                if publications:\n",
    "                    print(f\"   ‚úÖ {source}: {len(publications)} publications\")\n",
    "                    # Add approach metadata to each publication\n",
    "                    for pub in publications:\n",
    "                        pub['approach_id'] = approach_id\n",
    "                        pub['query_used'] = query_used\n",
    "                        pub['fetch_method'] = 'fetch_all_sources'\n",
    "                        pub['fetch_timestamp'] = datetime.now().isoformat()\n",
    "                        pub['original_source'] = source\n",
    "                    all_publication_data.extend(publications)\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {source}: No publications found\")\n",
    "            else:\n",
    "                print(f\"   üö´ Skipping {source} (not Semantic Scholar)\")\n",
    "        \n",
    "        # Sleep before alternative queries\n",
    "        print(f\"   ‚è±Ô∏è Waiting 5 seconds before alternative queries...\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    # Method 3: Try different query variations with metadata tracking\n",
    "    alternative_queries = [\n",
    "        ('method_3a_simple', 'agent scm'),\n",
    "        ('method_3b_quoted', '\"supply chain\" agent'),\n",
    "        ('method_3c_autonomous', 'autonomous agent logistics'),\n",
    "        ('method_3d_agent_based', 'agent-based supply chain'),\n",
    "        ('method_3e_multi_agent', 'multi-agent supply chain')\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n3. Trying alternative query variations...\")\n",
    "    \n",
    "    for i, (approach_id, alt_query) in enumerate(alternative_queries):\n",
    "        print(f\"\\nüîÑ Alternative query ({i+1}/{len(alternative_queries)}):\")\n",
    "        print(f\"   Approach ID: {approach_id}\")\n",
    "        print(f\"   Query: '{alt_query}'\")\n",
    "        \n",
    "        try:\n",
    "            if semantic_client:\n",
    "                alt_publications = semantic_client.fetch_publications(\n",
    "                    query=alt_query,\n",
    "                    start_year=target_year,\n",
    "                    end_year=target_year,\n",
    "                    max_results=50\n",
    "                )\n",
    "                if alt_publications:\n",
    "                    print(f\"   ‚úÖ Results: {len(alt_publications)} publications\")\n",
    "                    # Add approach metadata to each publication\n",
    "                    for pub in alt_publications:\n",
    "                        pub['approach_id'] = approach_id\n",
    "                        pub['query_used'] = alt_query\n",
    "                        pub['fetch_method'] = 'direct_client_alternative'\n",
    "                        pub['fetch_timestamp'] = datetime.now().isoformat()\n",
    "                    all_publication_data.extend(alt_publications)\n",
    "                else:\n",
    "                    print(f\"   ‚ùå No results for query: {alt_query}\")\n",
    "            else:\n",
    "                # Fallback to fetch_all_sources and filter\n",
    "                alt_results = data_acquirer.fetch_all_sources(\n",
    "                    query=alt_query,\n",
    "                    start_year=target_year,\n",
    "                    end_year=target_year,\n",
    "                    max_results_per_source=50\n",
    "                )\n",
    "                \n",
    "                for source, pubs in alt_results.items():\n",
    "                    if 'semantic' in source.lower() or 'scholar' in source.lower():\n",
    "                        if pubs:\n",
    "                            print(f\"   ‚úÖ {source}: {len(pubs)} publications\")\n",
    "                            # Add approach metadata to each publication\n",
    "                            for pub in pubs:\n",
    "                                pub['approach_id'] = approach_id\n",
    "                                pub['query_used'] = alt_query\n",
    "                                pub['fetch_method'] = 'fetch_all_sources_alternative'\n",
    "                                pub['fetch_timestamp'] = datetime.now().isoformat()\n",
    "                                pub['original_source'] = source\n",
    "                            all_publication_data.extend(pubs)\n",
    "            \n",
    "            # Sleep between alternative queries (except after the last one)\n",
    "            if i < len(alternative_queries) - 1:\n",
    "                print(f\"   ‚è±Ô∏è Waiting 5 seconds before next query...\")\n",
    "                time.sleep(5)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error with query '{alt_query}': {str(e)}\")\n",
    "            # Still sleep on error to avoid hammering the API\n",
    "            if i < len(alternative_queries) - 1:\n",
    "                print(f\"   ‚è±Ô∏è Waiting 5 seconds before next query...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during publication fetching: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Create comprehensive DataFrame with all results\n",
    "print(f\"\\nüìä Creating comprehensive DataFrame...\")\n",
    "print(f\"   Total publications collected: {len(all_publication_data)}\")\n",
    "\n",
    "if all_publication_data:\n",
    "    # Convert to DataFrame\n",
    "    publications_df = pd.DataFrame(all_publication_data)\n",
    "    \n",
    "    print(f\"\\nüìã Publications DataFrame created:\")\n",
    "    print(f\"   Shape: {publications_df.shape}\")\n",
    "    print(f\"   Columns: {list(publications_df.columns)}\")\n",
    "    \n",
    "    # Show approach distribution\n",
    "    if 'approach_id' in publications_df.columns:\n",
    "        approach_dist = publications_df['approach_id'].value_counts()\n",
    "        print(f\"\\nüîç Results by Approach:\")\n",
    "        for approach, count in approach_dist.items():\n",
    "            # Get sample query for this approach\n",
    "            sample_query = publications_df[publications_df['approach_id'] == approach]['query_used'].iloc[0]\n",
    "            print(f\"   {approach}: {count} publications (query: '{sample_query}')\")\n",
    "    \n",
    "    # Show query distribution\n",
    "    if 'query_used' in publications_df.columns:\n",
    "        query_dist = publications_df['query_used'].value_counts()\n",
    "        print(f\"\\nüîë Results by Query:\")\n",
    "        for query, count in query_dist.items():\n",
    "            print(f\"   '{query}': {count} publications\")\n",
    "    \n",
    "    # Remove duplicates based on title or DOI, keeping track of which approach found them first\n",
    "    print(f\"\\nüîÑ Deduplication analysis:\")\n",
    "    initial_count = len(publications_df)\n",
    "    \n",
    "    # Before deduplication, let's see which approaches found the same papers\n",
    "    if 'doi' in publications_df.columns:\n",
    "        # Group by DOI to see overlaps\n",
    "        doi_groups = publications_df[publications_df['doi'].notna()].groupby('doi')['approach_id'].apply(list)\n",
    "        overlapping_dois = doi_groups[doi_groups.apply(len) > 1]\n",
    "        if len(overlapping_dois) > 0:\n",
    "            print(f\"   üìä Found {len(overlapping_dois)} DOIs discovered by multiple approaches:\")\n",
    "            for doi, approaches in overlapping_dois.head(3).items():\n",
    "                print(f\"      DOI: {doi[:50]}... found by: {approaches}\")\n",
    "    \n",
    "    # Deduplicate keeping the first occurrence (which preserves approach priority)\n",
    "    if 'doi' in publications_df.columns:\n",
    "        publications_df_dedup = publications_df.drop_duplicates(subset=['doi'], keep='first')\n",
    "    elif 'title' in publications_df.columns:\n",
    "        publications_df_dedup = publications_df.drop_duplicates(subset=['title'], keep='first')\n",
    "    else:\n",
    "        publications_df_dedup = publications_df.copy()\n",
    "    \n",
    "    final_count = len(publications_df_dedup)\n",
    "    print(f\"   After deduplication: {final_count} unique publications (removed {initial_count - final_count} duplicates)\")\n",
    "    \n",
    "    # Show final approach distribution after deduplication\n",
    "    if 'approach_id' in publications_df_dedup.columns:\n",
    "        final_approach_dist = publications_df_dedup['approach_id'].value_counts()\n",
    "        print(f\"\\nüìà Unique publications by approach (after deduplication):\")\n",
    "        for approach, count in final_approach_dist.items():\n",
    "            sample_query = publications_df_dedup[publications_df_dedup['approach_id'] == approach]['query_used'].iloc[0]\n",
    "            print(f\"   {approach}: {count} unique publications (query: '{sample_query}')\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nüìà Basic Statistics:\")\n",
    "    print(f\"   Unique DOIs: {publications_df_dedup['doi'].nunique() if 'doi' in publications_df_dedup.columns else 'N/A'}\")\n",
    "    print(f\"   Unique titles: {publications_df_dedup['title'].nunique() if 'title' in publications_df_dedup.columns else 'N/A'}\")\n",
    "    \n",
    "    # Year distribution\n",
    "    if 'year' in publications_df_dedup.columns:\n",
    "        year_dist = publications_df_dedup['year'].value_counts().sort_index()\n",
    "        print(f\"\\nüìÖ Year Distribution:\")\n",
    "        for year, count in year_dist.items():\n",
    "            if pd.notna(year):\n",
    "                print(f\"   {int(year)}: {count} publications\")\n",
    "    \n",
    "    # Show sample publications with approach info\n",
    "    print(f\"\\nüìö Sample Publications (with approach tracking):\")\n",
    "    sample_size = min(3, len(publications_df_dedup))\n",
    "    for i in range(sample_size):\n",
    "        pub = publications_df_dedup.iloc[i]\n",
    "        print(f\"\\n   {i+1}. {pub.get('title', 'No title')}\")\n",
    "        print(f\"      DOI: {pub.get('doi', 'No DOI')}\")\n",
    "        print(f\"      Approach: {pub.get('approach_id', 'N/A')}\")\n",
    "        print(f\"      Query: {pub.get('query_used', 'N/A')}\")\n",
    "        print(f\"      Method: {pub.get('fetch_method', 'N/A')}\")\n",
    "        print(f\"      Citations: {pub.get('citation_count', 'N/A')}\")\n",
    "        if 'abstract' in pub and pd.notna(pub['abstract']):\n",
    "            abstract = str(pub['abstract'])[:150] + \"...\" if len(str(pub['abstract'])) > 150 else str(pub['abstract'])\n",
    "            print(f\"      Abstract: {abstract}\")\n",
    "    \n",
    "    # Store the deduplicated DataFrame for analysis\n",
    "    sample_publications = publications_df_dedup.to_dict('records')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Publications DataFrame ready for keyword analysis!\")\n",
    "    print(f\"   Ready to proceed with keyword extraction and analysis\")\n",
    "    print(f\"   Using deduplicated DataFrame with {len(sample_publications)} publications\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No publications retrieved from any approach.\")\n",
    "    print(f\"   This is likely due to rate limiting (429 errors).\")\n",
    "    \n",
    "    # Create empty DataFrame for testing\n",
    "    publications_df_dedup = pd.DataFrame()\n",
    "    sample_publications = []\n",
    "\n",
    "# Debug information about the DataAcquirer\n",
    "print(f\"\\nüîß DataAcquirer Debug Information:\")\n",
    "try:\n",
    "    print(f\"   Available clients: {list(data_acquirer.clients.keys())}\")\n",
    "    \n",
    "    # Check fetch_all_sources method signature\n",
    "    import inspect\n",
    "    sig = inspect.signature(data_acquirer.fetch_all_sources)\n",
    "    print(f\"   fetch_all_sources signature: {sig}\")\n",
    "    \n",
    "except Exception as debug_e:\n",
    "    print(f\"   Error in debug: {debug_e}\")\n",
    "\n",
    "# Analysis summary\n",
    "print(f\"\\nüîç Fetch Analysis Summary:\")\n",
    "print(f\"   Total API calls made: {len(alternative_queries) + 2}\")  # main + alternatives + potential fallback\n",
    "print(f\"   Total publications collected: {len(all_publication_data)}\")\n",
    "print(f\"   Unique publications after dedup: {len(sample_publications)}\")\n",
    "print(f\"   Rate limiting protection: 5-second delays between calls\")\n",
    "\n",
    "if len(sample_publications) > 0:\n",
    "    # Quick preview of available data for keyword analysis\n",
    "    if 'keywords' in publications_df_dedup.columns:\n",
    "        all_keywords = []\n",
    "        for keywords in publications_df_dedup['keywords'].dropna():\n",
    "            if isinstance(keywords, list):\n",
    "                all_keywords.extend(keywords)\n",
    "            elif isinstance(keywords, str):\n",
    "                all_keywords.extend([k.strip() for k in keywords.split(',') if k.strip()])\n",
    "        \n",
    "        if all_keywords:\n",
    "            keyword_counts = pd.Series(all_keywords).value_counts()\n",
    "            print(f\"\\nüîë Available Keywords Preview (top 5):\")\n",
    "            for keyword, count in keyword_counts.head(5).items():\n",
    "                print(f\"   '{keyword}': {count}\")\n",
    "        else:\n",
    "            print(f\"\\nüîë No structured keywords found, will use NLP extraction from titles/abstracts\")\n",
    "\n",
    "print(f\"\\nüìä Development Mode: Data ready in memory for analysis!\")\n",
    "print(f\"üìà Variables available:\")\n",
    "print(f\"   - publications_df_dedup: Deduplicated DataFrame ({len(publications_df_dedup)} rows)\")\n",
    "print(f\"   - sample_publications: List of publication dicts for analysis\")\n",
    "print(f\"   - all_publication_data: Raw data with duplicates ({len(all_publication_data)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168d290",
   "metadata": {},
   "source": [
    "#### Let's examine dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed488ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DETAILED DATAFRAME ANALYSIS\n",
      "==================================================\n",
      "\n",
      "üìä DataFrame Overview:\n",
      "   Shape: (1, 17)\n",
      "   Memory usage: 1.10 KB\n",
      "\n",
      "üìã Column Information:\n",
      "   doi: 0/1 non-null (object)\n",
      "   title: 1/1 non-null (object)\n",
      "   abstract: 1/1 non-null (object)\n",
      "   authors: 1/1 non-null (object)\n",
      "   publication_date: 1/1 non-null (object)\n",
      "   keywords: 1/1 non-null (object)\n",
      "   citation_count: 1/1 non-null (int64)\n",
      "   reference_count: 1/1 non-null (int64)\n",
      "   venue: 1/1 non-null (object)\n",
      "   publication_types: 1/1 non-null (object)\n",
      "   open_access_pdf: 0/1 non-null (object)\n",
      "   paper_id: 1/1 non-null (object)\n",
      "   source: 1/1 non-null (object)\n",
      "   approach_id: 1/1 non-null (object)\n",
      "   query_used: 1/1 non-null (object)\n",
      "   fetch_method: 1/1 non-null (object)\n",
      "   fetch_timestamp: 1/1 non-null (object)\n",
      "\n",
      "üìö Publication Details:\n",
      "   Title: Enhancing supply chain resilience with multi-agent systems and machine learning: a framework for adaptive decision-making\n",
      "   DOI: None\n",
      "   Authors: ['Md Zahidur Rahman Farazi']\n",
      "   Year: 2025\n",
      "   Venue: \n",
      "   Citation Count: 0\n",
      "   Paper ID: 806208c8d27347eab578ecb2faff64012a7d67dc\n",
      "   Abstract: \n",
      "   Keywords: []\n",
      "\n",
      "üîç Approach Tracking:\n",
      "   Approach ID: method_3a_simple\n",
      "   Query Used: agent scm\n",
      "   Fetch Method: direct_client_alternative\n",
      "   Fetch Timestamp: 2025-06-05T09:08:24.759901\n",
      "\n",
      "üîÑ Duplication Analysis:\n",
      "   Total records fetched: 200\n",
      "   Unique records after dedup: 1\n",
      "   Duplicate rate: 99.5%\n",
      "\n",
      "üîç Raw Data Analysis:\n",
      "   Raw DataFrame shape: (200, 17)\n",
      "   Duplicate analysis by field:\n",
      "     title: 183 unique out of 200 total (17 duplicates)\n",
      "     paper_id: 183 unique out of 200 total (17 duplicates)\n",
      "     doi: 0 unique out of 200 total (200 duplicates)\n",
      "\n",
      "   Raw data by approach:\n",
      "     method_3a_simple: 50 records\n",
      "     method_3c_autonomous: 50 records\n",
      "     method_3d_agent_based: 50 records\n",
      "     method_3e_multi_agent: 50 records\n",
      "\n",
      "   Raw data by query:\n",
      "     'agent scm': 50 records\n",
      "     'autonomous agent logistics': 50 records\n",
      "     'agent-based supply chain': 50 records\n",
      "     'multi-agent supply chain': 50 records\n",
      "\n",
      "ü§î Same Paper Analysis:\n",
      "   Unique titles found: 183\n",
      "   Unique paper IDs found: 183\n",
      "\n",
      "üí° Observations:\n",
      "   1. The high duplication rate (99.5%) suggests API issues\n",
      "   2. Different queries are returning the same paper\n",
      "   3. This could be due to:\n",
      "      - Very limited 2025 publications matching agent+SCM criteria\n",
      "      - API returning default/fallback results\n",
      "      - Year filtering not working properly\n",
      "      - Rate limiting affecting result diversity\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Try broader year range (e.g., 2024-2025)\n",
      "   2. Test without year filtering\n",
      "   3. Try completely different query terms\n",
      "   4. Check if API is working properly with smaller result sets\n",
      "\n",
      "üìù Available Content for Analysis:\n",
      "   Title length: 121 characters\n",
      "   Abstract length: 0 characters\n",
      "   Total text for NLP: 122 characters\n",
      "   ‚úÖ Sufficient text available for keyword extraction\n",
      "   Text preview: 'Enhancing supply chain resilience with multi-agent systems and machine learning: a framework for adaptive decision-making ...'\n",
      "\n",
      "üìä DataFrame is ready for analysis despite duplication issues!\n",
      "üöÄ Proceeding with keyword analysis on the available data...\n"
     ]
    }
   ],
   "source": [
    "# Let's examine our DataFrame in detail\n",
    "print(\"üîç DETAILED DATAFRAME ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä DataFrame Overview:\")\n",
    "print(f\"   Shape: {publications_df_dedup.shape}\")\n",
    "print(f\"   Memory usage: {publications_df_dedup.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\nüìã Column Information:\")\n",
    "for col in publications_df_dedup.columns:\n",
    "    non_null = publications_df_dedup[col].notna().sum()\n",
    "    data_type = publications_df_dedup[col].dtype\n",
    "    print(f\"   {col}: {non_null}/{len(publications_df_dedup)} non-null ({data_type})\")\n",
    "\n",
    "print(f\"\\nüìö Publication Details:\")\n",
    "if len(publications_df_dedup) > 0:\n",
    "    pub = publications_df_dedup.iloc[0]\n",
    "    print(f\"   Title: {pub.get('title', 'N/A')}\")\n",
    "    print(f\"   DOI: {pub.get('doi', 'N/A')}\")\n",
    "    print(f\"   Authors: {pub.get('authors', 'N/A')}\")\n",
    "    print(f\"   Year: {pub.get('publication_date', 'N/A')}\")\n",
    "    print(f\"   Venue: {pub.get('venue', 'N/A')}\")\n",
    "    print(f\"   Citation Count: {pub.get('citation_count', 'N/A')}\")\n",
    "    print(f\"   Paper ID: {pub.get('paper_id', 'N/A')}\")\n",
    "    print(f\"   Abstract: {pub.get('abstract', 'N/A')}\")\n",
    "    print(f\"   Keywords: {pub.get('keywords', 'N/A')}\")\n",
    "    \n",
    "    # Approach tracking info\n",
    "    print(f\"\\nüîç Approach Tracking:\")\n",
    "    print(f\"   Approach ID: {pub.get('approach_id', 'N/A')}\")\n",
    "    print(f\"   Query Used: {pub.get('query_used', 'N/A')}\")\n",
    "    print(f\"   Fetch Method: {pub.get('fetch_method', 'N/A')}\")\n",
    "    print(f\"   Fetch Timestamp: {pub.get('fetch_timestamp', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nüîÑ Duplication Analysis:\")\n",
    "print(f\"   Total records fetched: {len(all_publication_data)}\")\n",
    "print(f\"   Unique records after dedup: {len(publications_df_dedup)}\")\n",
    "print(f\"   Duplicate rate: {((len(all_publication_data) - len(publications_df_dedup)) / len(all_publication_data) * 100):.1f}%\")\n",
    "\n",
    "# Let's examine the raw data to understand the duplication\n",
    "print(f\"\\nüîç Raw Data Analysis:\")\n",
    "if len(all_publication_data) > 0:\n",
    "    # Convert all raw data to DataFrame to analyze duplicates\n",
    "    raw_df = pd.DataFrame(all_publication_data)\n",
    "    \n",
    "    print(f\"   Raw DataFrame shape: {raw_df.shape}\")\n",
    "    \n",
    "    # Check for duplicates by different fields\n",
    "    duplicate_analysis = {}\n",
    "    for field in ['title', 'paper_id', 'doi']:\n",
    "        if field in raw_df.columns:\n",
    "            unique_count = raw_df[field].nunique()\n",
    "            total_count = len(raw_df)\n",
    "            duplicate_analysis[field] = {\n",
    "                'unique': unique_count,\n",
    "                'total': total_count,\n",
    "                'duplicates': total_count - unique_count\n",
    "            }\n",
    "    \n",
    "    print(f\"   Duplicate analysis by field:\")\n",
    "    for field, stats in duplicate_analysis.items():\n",
    "        print(f\"     {field}: {stats['unique']} unique out of {stats['total']} total ({stats['duplicates']} duplicates)\")\n",
    "    \n",
    "    # Show approach distribution in raw data\n",
    "    if 'approach_id' in raw_df.columns:\n",
    "        approach_dist = raw_df['approach_id'].value_counts()\n",
    "        print(f\"\\n   Raw data by approach:\")\n",
    "        for approach, count in approach_dist.items():\n",
    "            print(f\"     {approach}: {count} records\")\n",
    "    \n",
    "    # Show query distribution in raw data\n",
    "    if 'query_used' in raw_df.columns:\n",
    "        query_dist = raw_df['query_used'].value_counts()\n",
    "        print(f\"\\n   Raw data by query:\")\n",
    "        for query, count in query_dist.items():\n",
    "            print(f\"     '{query}': {count} records\")\n",
    "\n",
    "# Check if this is the same paper returned for all queries\n",
    "print(f\"\\nü§î Same Paper Analysis:\")\n",
    "if len(all_publication_data) > 1:\n",
    "    # Check if all papers have the same title\n",
    "    titles = [pub.get('title', '') for pub in all_publication_data]\n",
    "    unique_titles = set(titles)\n",
    "    print(f\"   Unique titles found: {len(unique_titles)}\")\n",
    "    \n",
    "    if len(unique_titles) == 1:\n",
    "        print(f\"   ‚ö†Ô∏è All 200 records have the same title: '{list(unique_titles)[0]}'\")\n",
    "        print(f\"   This suggests the API is returning the same paper for all different queries\")\n",
    "    \n",
    "    # Check paper IDs\n",
    "    paper_ids = [pub.get('paper_id', '') for pub in all_publication_data]\n",
    "    unique_paper_ids = set(paper_ids)\n",
    "    print(f\"   Unique paper IDs found: {len(unique_paper_ids)}\")\n",
    "    \n",
    "    if len(unique_paper_ids) == 1:\n",
    "        print(f\"   ‚ö†Ô∏è All records have the same paper ID: '{list(unique_paper_ids)[0]}'\")\n",
    "\n",
    "print(f\"\\nüí° Observations:\")\n",
    "print(f\"   1. The high duplication rate (99.5%) suggests API issues\")\n",
    "print(f\"   2. Different queries are returning the same paper\")\n",
    "print(f\"   3. This could be due to:\")\n",
    "print(f\"      - Very limited 2025 publications matching agent+SCM criteria\")\n",
    "print(f\"      - API returning default/fallback results\")\n",
    "print(f\"      - Year filtering not working properly\")\n",
    "print(f\"      - Rate limiting affecting result diversity\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Try broader year range (e.g., 2024-2025)\")\n",
    "print(f\"   2. Test without year filtering\")\n",
    "print(f\"   3. Try completely different query terms\")\n",
    "print(f\"   4. Check if API is working properly with smaller result sets\")\n",
    "\n",
    "# Let's also check what we can extract from this single publication\n",
    "print(f\"\\nüìù Available Content for Analysis:\")\n",
    "if len(sample_publications) > 0:\n",
    "    pub = sample_publications[0]\n",
    "    title = pub.get('title', '')\n",
    "    abstract = pub.get('abstract', '')\n",
    "    \n",
    "    print(f\"   Title length: {len(title)} characters\")\n",
    "    print(f\"   Abstract length: {len(abstract)} characters\")\n",
    "    print(f\"   Total text for NLP: {len(title + ' ' + abstract)} characters\")\n",
    "    \n",
    "    if len(title + abstract) > 10:\n",
    "        print(f\"   ‚úÖ Sufficient text available for keyword extraction\")\n",
    "        text_preview = (title + ' ' + abstract)[:200]\n",
    "        print(f\"   Text preview: '{text_preview}...'\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Limited text available for keyword extraction\")\n",
    "\n",
    "print(f\"\\nüìä DataFrame is ready for analysis despite duplication issues!\")\n",
    "print(f\"üöÄ Proceeding with keyword analysis on the available data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5913fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1 entries, 0 to 0\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   doi                0 non-null      object\n",
      " 1   title              1 non-null      object\n",
      " 2   abstract           1 non-null      object\n",
      " 3   authors            1 non-null      object\n",
      " 4   publication_date   1 non-null      object\n",
      " 5   keywords           1 non-null      object\n",
      " 6   citation_count     1 non-null      int64 \n",
      " 7   reference_count    1 non-null      int64 \n",
      " 8   venue              1 non-null      object\n",
      " 9   publication_types  1 non-null      object\n",
      " 10  open_access_pdf    0 non-null      object\n",
      " 11  paper_id           1 non-null      object\n",
      " 12  source             1 non-null      object\n",
      " 13  approach_id        1 non-null      object\n",
      " 14  query_used         1 non-null      object\n",
      " 15  fetch_method       1 non-null      object\n",
      " 16  fetch_timestamp    1 non-null      object\n",
      "dtypes: int64(2), object(15)\n",
      "memory usage: 144.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "publications_df_dedup.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae47ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>keywords</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>reference_count</th>\n",
       "      <th>venue</th>\n",
       "      <th>publication_types</th>\n",
       "      <th>open_access_pdf</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>source</th>\n",
       "      <th>approach_id</th>\n",
       "      <th>query_used</th>\n",
       "      <th>fetch_method</th>\n",
       "      <th>fetch_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Enhancing supply chain resilience with multi-a...</td>\n",
       "      <td></td>\n",
       "      <td>[Md Zahidur Rahman Farazi]</td>\n",
       "      <td>2025</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>806208c8d27347eab578ecb2faff64012a7d67dc</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "      <td>method_3a_simple</td>\n",
       "      <td>agent scm</td>\n",
       "      <td>direct_client_alternative</td>\n",
       "      <td>2025-06-05T09:08:24.759901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doi                                              title abstract  \\\n",
       "0  None  Enhancing supply chain resilience with multi-a...            \n",
       "\n",
       "                      authors publication_date keywords  citation_count  \\\n",
       "0  [Md Zahidur Rahman Farazi]             2025       []               0   \n",
       "\n",
       "   reference_count venue publication_types open_access_pdf  \\\n",
       "0                0                      []            None   \n",
       "\n",
       "                                   paper_id            source  \\\n",
       "0  806208c8d27347eab578ecb2faff64012a7d67dc  Semantic Scholar   \n",
       "\n",
       "        approach_id query_used               fetch_method  \\\n",
       "0  method_3a_simple  agent scm  direct_client_alternative   \n",
       "\n",
       "              fetch_timestamp  \n",
       "0  2025-06-05T09:08:24.759901  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications_df_dedup.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6170d44",
   "metadata": {},
   "source": [
    "## 3. Keyword Extraction\n",
    "\n",
    "Now let's extract keywords using both API-based and NLP-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize keyword extractor\n",
    "keyword_extractor = KeywordExtractor(config)\n",
    "\n",
    "print(\"üîç Starting keyword extraction...\")\n",
    "\n",
    "# Extract keywords using API data\n",
    "print(\"\\n1. API-based keyword extraction:\")\n",
    "api_keywords = keyword_extractor.extract_from_api_data(sample_publications)\n",
    "print(f\"   - Extracted {len(api_keywords.get('all_keywords', []))} unique keywords\")\n",
    "print(f\"   - Top 10 by frequency: {list(api_keywords.get('keyword_frequencies', {}).keys())[:10]}\")\n",
    "\n",
    "# Extract keywords using NLP methods\n",
    "print(\"\\n2. NLP-based keyword extraction:\")\n",
    "text_corpus = [pub.get('title', '') + ' ' + pub.get('abstract', '') for pub in sample_publications]\n",
    "text_corpus = [text for text in text_corpus if text.strip()]  # Remove empty texts\n",
    "\n",
    "if text_corpus:\n",
    "    nlp_keywords = keyword_extractor.extract_from_text(text_corpus)\n",
    "    \n",
    "    for method in ['tfidf', 'rake', 'yake']:\n",
    "        if method in nlp_keywords:\n",
    "            method_keywords = nlp_keywords[method]\n",
    "            print(f\"   - {method.upper()}: {len(method_keywords)} keywords\")\n",
    "            print(f\"     Top 5: {list(method_keywords.keys())[:5]}\")\n",
    "else:\n",
    "    print(\"   - No text content available for NLP extraction\")\n",
    "    nlp_keywords = {}\n",
    "\n",
    "# Combine and analyze frequency distribution\n",
    "print(\"\\n3. Frequency analysis:\")\n",
    "all_keywords_combined = {}\n",
    "all_keywords_combined.update(api_keywords.get('keyword_frequencies', {}))\n",
    "\n",
    "for method_keywords in nlp_keywords.values():\n",
    "    for kw, freq in method_keywords.items():\n",
    "        all_keywords_combined[kw] = all_keywords_combined.get(kw, 0) + freq\n",
    "\n",
    "freq_stats = keyword_extractor.analyze_frequency_distribution(all_keywords_combined)\n",
    "print(f\"   - Total unique keywords: {freq_stats['total_keywords']}\")\n",
    "print(f\"   - Mean frequency: {freq_stats['mean_frequency']:.2f}\")\n",
    "print(f\"   - Frequency std: {freq_stats['frequency_std']:.2f}\")\n",
    "print(f\"   - High-frequency keywords (>mean): {len(freq_stats['high_frequency_keywords'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9a455",
   "metadata": {},
   "source": [
    "## 4. Semantic Analysis\n",
    "\n",
    "Let's perform semantic analysis using BGE-M3 embeddings and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize semantic analyzer\n",
    "semantic_analyzer = SemanticAnalyzer(config)\n",
    "\n",
    "print(\"üß† Starting semantic analysis...\")\n",
    "\n",
    "# Get top keywords for semantic analysis\n",
    "top_keywords = list(all_keywords_combined.keys())[:50]  # Limit for demo\n",
    "print(f\"Analyzing top {len(top_keywords)} keywords\")\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"\\n1. Generating BGE-M3 embeddings...\")\n",
    "embeddings = semantic_analyzer.generate_embeddings(top_keywords)\n",
    "print(f\"   - Generated embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Perform clustering\n",
    "print(\"\\n2. Performing clustering analysis...\")\n",
    "clustering_results = semantic_analyzer.perform_clustering(\n",
    "    keywords=top_keywords,\n",
    "    embeddings=embeddings,\n",
    "    algorithm='kmeans',\n",
    "    n_clusters=8\n",
    ")\n",
    "\n",
    "print(f\"   - Number of clusters: {clustering_results['cluster_stats']['n_clusters']}\")\n",
    "print(f\"   - Silhouette score: {clustering_results['cluster_stats']['silhouette_score']:.3f}\")\n",
    "print(f\"   - Largest cluster size: {max(clustering_results['cluster_stats']['cluster_sizes'])}\")\n",
    "\n",
    "# Show cluster examples\n",
    "print(\"\\n3. Cluster examples:\")\n",
    "for cluster_id, keywords_in_cluster in clustering_results['clusters'].items():\n",
    "    if len(keywords_in_cluster) > 1:  # Show clusters with multiple keywords\n",
    "        print(f\"   Cluster {cluster_id}: {', '.join(keywords_in_cluster[:5])}\")\n",
    "        if len(keywords_in_cluster) > 5:\n",
    "            print(f\"      ... and {len(keywords_in_cluster) - 5} more\")\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "print(\"\\n4. Dimensionality reduction...\")\n",
    "reduced_embeddings = semantic_analyzer.reduce_dimensions(\n",
    "    embeddings, \n",
    "    method='umap', \n",
    "    n_components=2\n",
    ")\n",
    "print(f\"   - Reduced to 2D: {reduced_embeddings.shape}\")\n",
    "\n",
    "# Store results for visualization\n",
    "semantic_results = {\n",
    "    'keywords': top_keywords,\n",
    "    'embeddings': embeddings,\n",
    "    'embeddings_2d': reduced_embeddings,\n",
    "    'cluster_labels': clustering_results['cluster_labels'],\n",
    "    'clusters': clustering_results['clusters'],\n",
    "    'cluster_stats': clustering_results['cluster_stats']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30eb8ef",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis\n",
    "\n",
    "Let's analyze temporal patterns and trends in keyword usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize temporal analyzer\n",
    "temporal_analyzer = TemporalAnalyzer(config)\n",
    "\n",
    "print(\"üìà Starting temporal analysis...\")\n",
    "\n",
    "# Prepare keyword data for temporal analysis\n",
    "combined_keywords = {\n",
    "    'all_keywords': list(all_keywords_combined.keys()),\n",
    "    'keyword_frequencies': all_keywords_combined\n",
    "}\n",
    "\n",
    "# 1. Analyze publication trends\n",
    "print(\"\\n1. Publication volume trends:\")\n",
    "pub_trends = temporal_analyzer.analyze_publication_trends(sample_publications)\n",
    "if 'volume_trends' in pub_trends:\n",
    "    volume_trends = pub_trends['volume_trends']\n",
    "    print(f\"   - Date range: {pub_trends['date_range']['start']} to {pub_trends['date_range']['end']}\")\n",
    "    print(f\"   - Total publications: {pub_trends['total_publications']}\")\n",
    "    print(f\"   - Peak year: {volume_trends.get('peak_year', 'N/A')} ({volume_trends.get('peak_count', 0)} publications)\")\n",
    "    print(f\"   - Average yearly growth: {volume_trends.get('average_yearly_growth', 0):.2%}\")\n",
    "\n",
    "# 2. Analyze keyword trends\n",
    "print(\"\\n2. Keyword temporal trends:\")\n",
    "keyword_trends = temporal_analyzer.analyze_keyword_trends(sample_publications, combined_keywords)\n",
    "if 'individual_trends' in keyword_trends:\n",
    "    trends = keyword_trends['individual_trends']\n",
    "    print(f\"   - Keywords analyzed: {len(trends)}\")\n",
    "    \n",
    "    # Show trending keywords\n",
    "    if 'top_growing_keywords' in keyword_trends:\n",
    "        growing = keyword_trends['top_growing_keywords']\n",
    "        print(f\"   - Growing keywords: {len(growing)}\")\n",
    "        for kw in growing[:3]:\n",
    "            print(f\"     ‚Ä¢ {kw['keyword']}: slope={kw['slope']:.3f}, R¬≤={kw['r_squared']:.3f}\")\n",
    "\n",
    "# 3. Detect temporal patterns\n",
    "print(\"\\n3. Pattern detection:\")\n",
    "patterns = temporal_analyzer.detect_temporal_patterns(sample_publications, combined_keywords)\n",
    "if 'pattern_summary' in patterns:\n",
    "    summary = patterns['pattern_summary']\n",
    "    print(f\"   - Keywords with seasonal patterns: {summary.get('seasonal_keywords', 0)}\")\n",
    "    print(f\"   - Keywords with cyclical patterns: {summary.get('cyclical_keywords', 0)}\")\n",
    "    print(f\"   - Keywords with trend changes: {summary.get('keywords_with_trend_changes', 0)}\")\n",
    "\n",
    "# 4. Lifecycle analysis\n",
    "print(\"\\n4. Keyword lifecycle analysis:\")\n",
    "lifecycle = temporal_analyzer.analyze_keyword_lifecycle(sample_publications, combined_keywords)\n",
    "if 'lifecycle_categories' in lifecycle:\n",
    "    categories = lifecycle['lifecycle_categories']\n",
    "    print(f\"   - Emerging keywords: {len(categories.get('emerging', []))}\")\n",
    "    print(f\"   - Growing keywords: {len(categories.get('growing', []))}\")\n",
    "    print(f\"   - Mature keywords: {len(categories.get('mature', []))}\")\n",
    "    print(f\"   - Declining keywords: {len(categories.get('declining', []))}\")\n",
    "\n",
    "# 5. Compare time periods\n",
    "print(\"\\n5. Time period comparison:\")\n",
    "comparison = temporal_analyzer.compare_time_periods(sample_publications, combined_keywords)\n",
    "if 'period_data' in comparison:\n",
    "    period_data = comparison['period_data']\n",
    "    for period, keywords in period_data.items():\n",
    "        print(f\"   - {period}: {len(keywords)} unique keywords, {sum(keywords.values())} total occurrences\")\n",
    "\n",
    "# Store temporal results\n",
    "temporal_results = {\n",
    "    'publication_trends': pub_trends,\n",
    "    'keyword_trends': keyword_trends,\n",
    "    'temporal_patterns': patterns,\n",
    "    'lifecycle_analysis': lifecycle,\n",
    "    'comparative_analysis': comparison\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30867462",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Now let's create comprehensive visualizations of our analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3471a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = Visualizer(config)\n",
    "\n",
    "print(\"üìä Creating visualizations...\")\n",
    "\n",
    "# Create output directory for visualizations\n",
    "viz_dir = '/workspaces/tsi-sota-ai/outputs/agent_research_analysis'\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "visualization_files = []\n",
    "\n",
    "# 1. Word cloud\n",
    "print(\"\\n1. Creating word cloud...\")\n",
    "try:\n",
    "    wordcloud_path = os.path.join(viz_dir, 'keyword_wordcloud.png')\n",
    "    visualizer.create_word_cloud(\n",
    "        keywords=all_keywords_combined,\n",
    "        title=\"Agent Research Dynamics - Keyword Analysis\",\n",
    "        output_path=wordcloud_path\n",
    "    )\n",
    "    visualization_files.append(wordcloud_path)\n",
    "    print(f\"   ‚úÖ Word cloud saved: {wordcloud_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating word cloud: {str(e)}\")\n",
    "\n",
    "# 2. Frequency plot\n",
    "print(\"\\n2. Creating frequency plot...\")\n",
    "try:\n",
    "    freq_path = os.path.join(viz_dir, 'keyword_frequencies.png')\n",
    "    visualizer.plot_keyword_frequencies(\n",
    "        keywords=all_keywords_combined,\n",
    "        top_n=20,\n",
    "        title=\"Top 20 Keywords by Frequency - Agent Research\",\n",
    "        output_path=freq_path\n",
    "    )\n",
    "    visualization_files.append(freq_path)\n",
    "    print(f\"   ‚úÖ Frequency plot saved: {freq_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating frequency plot: {str(e)}\")\n",
    "\n",
    "# 3. Semantic clusters\n",
    "print(\"\\n3. Creating semantic cluster plot...\")\n",
    "try:\n",
    "    cluster_path = os.path.join(viz_dir, 'semantic_clusters.png')\n",
    "    visualizer.plot_semantic_clusters(\n",
    "        cluster_data=semantic_results,\n",
    "        title=\"Semantic Keyword Clusters (BGE-M3 + UMAP) - Agent Research\",\n",
    "        output_path=cluster_path\n",
    "    )\n",
    "    visualization_files.append(cluster_path)\n",
    "    print(f\"   ‚úÖ Cluster plot saved: {cluster_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating cluster plot: {str(e)}\")\n",
    "\n",
    "# 4. Temporal trends\n",
    "print(\"\\n4. Creating temporal trends plot...\")\n",
    "try:\n",
    "    if 'keyword_trends' in temporal_results and temporal_results['keyword_trends']:\n",
    "        trends_path = os.path.join(viz_dir, 'temporal_trends.png')\n",
    "        visualizer.plot_temporal_trends(\n",
    "            trend_data=temporal_results['keyword_trends'],\n",
    "            top_keywords=10,\n",
    "            title=\"Agent Research Keyword Temporal Trends\",\n",
    "            output_path=trends_path\n",
    "        )\n",
    "        visualization_files.append(trends_path)\n",
    "        print(f\"   ‚úÖ Temporal trends plot saved: {trends_path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è No temporal trends data available\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating temporal trends plot: {str(e)}\")\n",
    "\n",
    "# 5. Lifecycle analysis\n",
    "print(\"\\n5. Creating lifecycle analysis plot...\")\n",
    "try:\n",
    "    if 'lifecycle_analysis' in temporal_results and temporal_results['lifecycle_analysis']:\n",
    "        lifecycle_path = os.path.join(viz_dir, 'keyword_lifecycle.png')\n",
    "        visualizer.plot_lifecycle_analysis(\n",
    "            lifecycle_data=temporal_results['lifecycle_analysis'],\n",
    "            title=\"Agent Research Keyword Lifecycle Analysis\",\n",
    "            output_path=lifecycle_path\n",
    "        )\n",
    "        visualization_files.append(lifecycle_path)\n",
    "        print(f\"   ‚úÖ Lifecycle plot saved: {lifecycle_path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è No lifecycle analysis data available\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating lifecycle plot: {str(e)}\")\n",
    "\n",
    "# 6. Time period comparison\n",
    "print(\"\\n6. Creating time period comparison plot...\")\n",
    "try:\n",
    "    if 'comparative_analysis' in temporal_results and temporal_results['comparative_analysis']:\n",
    "        comparison_path = os.path.join(viz_dir, 'time_period_comparison.png')\n",
    "        visualizer.plot_comparative_analysis(\n",
    "            comparative_data=temporal_results['comparative_analysis'],\n",
    "            title=\"Agent Research Time Period Comparison\",\n",
    "            output_path=comparison_path\n",
    "        )\n",
    "        visualization_files.append(comparison_path)\n",
    "        print(f\"   ‚úÖ Comparison plot saved: {comparison_path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è No comparative analysis data available\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating comparison plot: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüìÅ Total visualizations created: {len(visualization_files)}\")\n",
    "for path in visualization_files:\n",
    "    print(f\"   - {os.path.basename(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e07012",
   "metadata": {},
   "source": [
    "## 7. Interactive Dashboard\n",
    "\n",
    "Let's create an interactive dashboard combining all our analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéõÔ∏è Creating interactive dashboard...\")\n",
    "\n",
    "# Compile all analysis results\n",
    "complete_results = {\n",
    "    'keyword_frequencies': all_keywords_combined,\n",
    "    'semantic_analysis': semantic_results,\n",
    "    'temporal_analysis': temporal_results,\n",
    "    'publication_count': len(sample_publications),\n",
    "    'analysis_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Create interactive dashboard\n",
    "try:\n",
    "    dashboard_path = os.path.join(viz_dir, 'interactive_dashboard.html')\n",
    "    visualizer.create_dashboard(\n",
    "        analysis_results=complete_results,\n",
    "        output_path=dashboard_path\n",
    "    )\n",
    "    print(f\"‚úÖ Interactive dashboard created: {dashboard_path}\")\n",
    "    print(f\"üåê Open in browser: file://{dashboard_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating dashboard: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55df83",
   "metadata": {},
   "source": [
    "## 8. Export Results\n",
    "\n",
    "Let's export all our analysis results in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f064670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Exporting analysis results...\")\n",
    "\n",
    "# Export keyword extraction results\n",
    "print(\"\\n1. Exporting keyword extraction results:\")\n",
    "try:\n",
    "    keywords_export_path = os.path.join(viz_dir, 'keyword_extraction_results.json')\n",
    "    keyword_extractor.export_keywords(\n",
    "        keywords={'combined_keywords': all_keywords_combined},\n",
    "        output_path=keywords_export_path,\n",
    "        format='json'\n",
    "    )\n",
    "    print(f\"   ‚úÖ Keywords exported: {keywords_export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error exporting keywords: {str(e)}\")\n",
    "\n",
    "# Export semantic analysis results\n",
    "print(\"\\n2. Exporting semantic analysis results:\")\n",
    "try:\n",
    "    semantic_export_path = os.path.join(viz_dir, 'semantic_analysis_results.json')\n",
    "    semantic_analyzer.export_analysis_results(\n",
    "        results=semantic_results,\n",
    "        output_path=semantic_export_path,\n",
    "        format='json'\n",
    "    )\n",
    "    print(f\"   ‚úÖ Semantic analysis exported: {semantic_export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error exporting semantic analysis: {str(e)}\")\n",
    "\n",
    "# Export temporal analysis results\n",
    "print(\"\\n3. Exporting temporal analysis results:\")\n",
    "try:\n",
    "    temporal_export_path = os.path.join(viz_dir, 'temporal_analysis_results.json')\n",
    "    temporal_analyzer.export_temporal_analysis(\n",
    "        output_path=temporal_export_path,\n",
    "        format='json'\n",
    "    )\n",
    "    print(f\"   ‚úÖ Temporal analysis exported: {temporal_export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error exporting temporal analysis: {str(e)}\")\n",
    "\n",
    "# Export all visualizations\n",
    "print(\"\\n4. Exporting all visualizations:\")\n",
    "try:\n",
    "    all_viz_files = visualizer.export_all_visualizations(\n",
    "        analysis_results=complete_results,\n",
    "        output_dir=viz_dir\n",
    "    )\n",
    "    print(f\"   ‚úÖ Exported {len(all_viz_files)} visualization files\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error exporting visualizations: {str(e)}\")\n",
    "\n",
    "# Create summary report\n",
    "print(\"\\n5. Creating summary report:\")\n",
    "try:\n",
    "    summary_report = {\n",
    "        'analysis_summary': {\n",
    "            'total_publications': len(sample_publications),\n",
    "            'total_keywords': len(all_keywords_combined),\n",
    "            'semantic_clusters': semantic_results.get('cluster_stats', {}).get('n_clusters', 0),\n",
    "            'temporal_patterns': len(temporal_results.get('temporal_patterns', {}).get('keyword_patterns', {})),\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        },\n",
    "        'top_keywords': dict(list(all_keywords_combined.items())[:20]),\n",
    "        'configuration_used': config.get('keyword_analysis', {}),\n",
    "        'files_generated': {\n",
    "            'visualizations': len(visualization_files),\n",
    "            'exports': 3,  # JSON exports\n",
    "            'dashboard': 1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(viz_dir, 'analysis_summary.json')\n",
    "    import json\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"   ‚úÖ Summary report created: {summary_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating summary: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüéâ Analysis complete! All results saved to: {viz_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84afb77",
   "metadata": {},
   "source": [
    "## 9. Analysis Summary\n",
    "\n",
    "Let's display a comprehensive summary of our keyword analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã KEYWORD ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä DATA OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Publications analyzed: {len(sample_publications)}\")\n",
    "print(f\"   ‚Ä¢ Total unique keywords: {len(all_keywords_combined)}\")\n",
    "print(f\"   ‚Ä¢ Search queries used: {len(test_queries[:2])}\")\n",
    "\n",
    "print(f\"\\nüîç KEYWORD EXTRACTION:\")\n",
    "print(f\"   ‚Ä¢ API-based keywords: {len(api_keywords.get('all_keywords', []))}\")\n",
    "print(f\"   ‚Ä¢ NLP-based methods: {len(nlp_keywords)} (TF-IDF, RAKE, YAKE)\")\n",
    "print(f\"   ‚Ä¢ Combined keyword pool: {len(all_keywords_combined)}\")\n",
    "\n",
    "print(f\"\\nüß† SEMANTIC ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ BGE-M3 embeddings generated: {len(top_keywords)}\")\n",
    "print(f\"   ‚Ä¢ Semantic clusters found: {semantic_results.get('cluster_stats', {}).get('n_clusters', 0)}\")\n",
    "print(f\"   ‚Ä¢ Clustering quality (silhouette): {semantic_results.get('cluster_stats', {}).get('silhouette_score', 0):.3f}\")\n",
    "print(f\"   ‚Ä¢ Dimensionality reduction: UMAP to 2D\")\n",
    "\n",
    "print(f\"\\nüìà TEMPORAL ANALYSIS:\")\n",
    "if temporal_results.get('publication_trends'):\n",
    "    pub_trends = temporal_results['publication_trends']\n",
    "    print(f\"   ‚Ä¢ Publication date range: {pub_trends.get('date_range', {}).get('start', 'N/A')} - {pub_trends.get('date_range', {}).get('end', 'N/A')}\")\n",
    "    if 'volume_trends' in pub_trends:\n",
    "        volume = pub_trends['volume_trends']\n",
    "        print(f\"   ‚Ä¢ Peak publication year: {volume.get('peak_year', 'N/A')} ({volume.get('peak_count', 0)} papers)\")\n",
    "        print(f\"   ‚Ä¢ Average yearly growth: {volume.get('average_yearly_growth', 0):.2%}\")\n",
    "\n",
    "if temporal_results.get('keyword_trends'):\n",
    "    kw_trends = temporal_results['keyword_trends']\n",
    "    print(f\"   ‚Ä¢ Keywords with temporal trends: {len(kw_trends.get('individual_trends', {}))}\")\n",
    "    print(f\"   ‚Ä¢ Growing keywords: {len(kw_trends.get('top_growing_keywords', []))}\")\n",
    "    print(f\"   ‚Ä¢ Declining keywords: {len(kw_trends.get('declining_keywords', []))}\")\n",
    "\n",
    "if temporal_results.get('lifecycle_analysis'):\n",
    "    lifecycle = temporal_results['lifecycle_analysis']\n",
    "    if 'lifecycle_categories' in lifecycle:\n",
    "        cats = lifecycle['lifecycle_categories']\n",
    "        print(f\"   ‚Ä¢ Lifecycle stages:\")\n",
    "        print(f\"     - Emerging: {len(cats.get('emerging', []))} keywords\")\n",
    "        print(f\"     - Growing: {len(cats.get('growing', []))} keywords\")\n",
    "        print(f\"     - Mature: {len(cats.get('mature', []))} keywords\")\n",
    "        print(f\"     - Declining: {len(cats.get('declining', []))} keywords\")\n",
    "\n",
    "print(f\"\\nüìä VISUALIZATIONS CREATED:\")\n",
    "print(f\"   ‚Ä¢ Static plots: {len(visualization_files)}\")\n",
    "print(f\"   ‚Ä¢ Interactive dashboard: 1\")\n",
    "print(f\"   ‚Ä¢ Word cloud: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Frequency plots: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Semantic clusters: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Temporal trends: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Lifecycle analysis: ‚úÖ\")\n",
    "\n",
    "print(f\"\\nüíæ EXPORTS GENERATED:\")\n",
    "print(f\"   ‚Ä¢ JSON analysis results: 3 files\")\n",
    "print(f\"   ‚Ä¢ Visualization images: {len(visualization_files)} files\")\n",
    "print(f\"   ‚Ä¢ Interactive HTML dashboard: 1 file\")\n",
    "print(f\"   ‚Ä¢ Summary report: 1 file\")\n",
    "\n",
    "print(f\"\\nüéØ TOP INSIGHTS:\")\n",
    "if all_keywords_combined:\n",
    "    top_5_keywords = list(all_keywords_combined.keys())[:5]\n",
    "    print(f\"   ‚Ä¢ Most frequent keywords: {', '.join(top_5_keywords)}\")\n",
    "\n",
    "if semantic_results.get('clusters'):\n",
    "    largest_cluster = max(semantic_results['clusters'].items(), key=lambda x: len(x[1]))\n",
    "    print(f\"   ‚Ä¢ Largest semantic cluster: {len(largest_cluster[1])} keywords\")\n",
    "    print(f\"     Example terms: {', '.join(largest_cluster[1][:3])}\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {viz_dir}\")\n",
    "print(f\"üåê Open dashboard: file://{os.path.join(viz_dir, 'interactive_dashboard.html')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ KEYWORD ANALYSIS MODULE DEMONSTRATION COMPLETE!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42810124",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Integration\n",
    "\n",
    "This demonstration shows the complete capabilities of our Keyword Analysis Module. Here are suggested next steps:\n",
    "\n",
    "### Integration with Existing Workflows:\n",
    "1. **Data Pipeline Integration**: Connect with `DataAcquirer` for real-time analysis\n",
    "2. **Batch Processing**: Set up automated keyword analysis for large datasets\n",
    "3. **API Integration**: Expose keyword analysis through REST APIs\n",
    "\n",
    "### Advanced Analysis:\n",
    "1. **Cross-Database Analysis**: Compare keywords across multiple academic databases\n",
    "2. **Citation-Weighted Keywords**: Weight keywords by publication citation counts\n",
    "3. **Co-occurrence Networks**: Analyze keyword co-occurrence patterns\n",
    "\n",
    "### Performance Optimization:\n",
    "1. **Caching**: Implement embedding and analysis result caching\n",
    "2. **Parallel Processing**: Utilize multiprocessing for large datasets\n",
    "3. **Memory Optimization**: Optimize for large-scale keyword analysis\n",
    "\n",
    "### Enhanced Visualizations:\n",
    "1. **Interactive Networks**: Create interactive keyword co-occurrence networks\n",
    "2. **Time-series Animation**: Animate temporal keyword evolution\n",
    "3. **Comparative Dashboards**: Side-by-side comparison of different datasets\n",
    "\n",
    "The module is now ready for production use and can be easily integrated into the larger TSI-SOTA-AI research analytics platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
